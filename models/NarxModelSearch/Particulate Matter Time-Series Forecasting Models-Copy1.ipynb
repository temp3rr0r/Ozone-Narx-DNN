{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling of air-quality in Belgium for forecasting purposes using Deep Neural Networks\n",
    "\n",
    "#### Konstantinos Theodorakos  \n",
    "*Katholieke Universiteit Leuven*  \n",
    "*Student of the Master Artificial Intelligence: Engineering and Computer Science*  \n",
    "*Faculty of Engineering Science*  \n",
    "*Departments of Computer Science & Electrical Engineering, ESAT-STADIUS*  \n",
    "*Kasteelpark Arenberg 10*  \n",
    "*Leuven (Heverlee), Belgium*  \n",
    "*Email: Konstantinos.Theodorakos@student.kuleuven.be*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BACKGROUND"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Particulate Matter\n",
    "\n",
    "Particulate matter (see: https://en.wikipedia.org/wiki/Particulates ) or atmospheric aeorosol particles, is solid or liquid matter suspended in the Earth's atmosphere. Inhaling particulate matter may cause adverse effects on human and animal health: asthma, lung cancer, respiratory diseases, cardiovascular disease, premature delivery, birth defects, low birth weight, and premature death.\n",
    "\n",
    "![Alt Text](pics/Airborne-particulate-size-chart.svg.png)\n",
    "*Figure: \n",
    "Diameter-size distribution of airborne particles in micrometres (µm) (source: https://en.wikipedia.org/wiki/File:Airborne-particulate-size-chart.svg ).*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. METHODS\n",
    "## Data handling & time-series split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sensor data points that were flagged as invalid were omitted entirely. For the 1-station ozone prediction experiments, linear interpolation is filling the missing values. For all the other 4+ station experiments, missing data points are covered by: (1) data from the geographically closest stations and (2) linear interpolation (in case there are still missing values).\n",
    "\n",
    "For the model selection, a unique form of k-fold cross-validation was used: **time-series split** [3] see: https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.TimeSeriesSplit.html ). In each split, shuffling is omitted, in order to retain the sequence of the full time-series. In addition, the model is trained with data of increasing total time-step count. The mean validation Mean Squared Error (MSE) determines whether a model is optimal and generalizes well. The \"test\" data is held-out from the start, never used in model selection. For all the experiments, time-series split had had 10-folds.\n",
    "![title](pics/timeSeriesValidateSplit.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neuroevolution with \"Island Transpeciation\"\n",
    "\n",
    "Neuroevolution [21] is an algorithmic method that utilizes *Evolutionary Algorithms* *(EA)* [18] to generate Artificial Neural Network (ANN) [22] parameters, topologies [5, 6, 10, 11, 12] and rules (see: https://en.wikipedia.org/wiki/Neuroevolution ). In order to find an optimal deep learning model for the time-series forecasting [8, 9] task, architecture search is performed in conjunction with hyper-parameter tuning [23, 34]. Speciation is the evolutionary process by which populations evolve to become distinct species (see: https://en.wikipedia.org/wiki/Speciation ). With *island speciation*, neuroevolution can be parallelized: Each island [24, 33] can be a distinct thread, process or even a different computing node. Even though the evolution progresses distinctly in each island, populations of meta-learning agents migrate periodically between islands.\n",
    "\n",
    "Every island can have the same global optimization technique (with similar or different settings versus the other islands), a **species**. A global optimizer like Differential Evolution (DE) [17] (see: https://docs.scipy.org/doc/scipy-0.15.1/reference/generated/scipy.optimize.differential_evolution.html ), maintains a number of different agents internally, that are used in a parameter search. An agent has a different internal representation (*genotype*) depending on the meta-heuristic algorithm. The actual bounded parameter set, the *phenotype*, is a candidate Deep Neural Network (DNN) [6] model architecture that we try to optimize. Within the DE implementation, there exist 2-way operations that can perform genotype-to-phenotype representation conversions and vice-versa.\n",
    "\n",
    "Three types of global optimization EA islands were used:\n",
    "- **Random (Rand)** [19] island. Creates a random model, by sampling the bounded hyper-parameter values from a uniform pseudo-random number generator. In this case genotype = phenotype and the island population is 1.\n",
    "\n",
    "- **Differential Evolution** island. Genotype: energy (real values in [0,1]). Phenotype: the genotype values, scaled from [0, 1] into the bounds of each parameter.\n",
    "![Alt Text](pics/Ackley.gif)\n",
    "*Figure: Example of Differential Evolution optimizing the 2D Ackley function using a population of size 20 (source: https://en.wikipedia.org/wiki/Differential_evolution#/media/File:Ackley.gif ).*\n",
    "\n",
    "\n",
    "- **Particle Swarm Optimization (PSO)** [16] island (see: https://pythonhosted.org/pyswarm/ ). Genotype: position (multi-dimensional representation of the parameters as real values) and velocity (rate of change of the positions).\n",
    "![Alt Text](pics/ParticleSwarmArrowsAnimation.gif)\n",
    "*Figure: A particle swarm searching for the global minimum of a function (source: https://en.wikipedia.org/wiki/Particle_swarm_optimization#/media/File:ParticleSwarmArrowsAnimation.gif ).*\n",
    "\n",
    "\n",
    "In some cases, it is possible to convert an agent from one global optimization method to another. For example, to convert a DE to a PSO agent: the agent of the DE \"species\" is converted from the DE genotype to the common phenotype (by unscaling real values) and then is converted to the PSO genotype (real values = position & velocity). We define this conversion process as **island transpeciation** (see: http://www.websters1913.com/words/Transpeciate ). Loosely termed, transpeciation acts as a (lossy in some cases) *transformation function* between the agent genotypes of different global optimization functions. Transpeciation in conjunction with the island model, allow combination (via *migration*) and diversification (via unique evolution per island) of different model architecture search methods.\n",
    "\n",
    "Worth noting is that in order to keep the populations stable on all islands, the migrating agents replace a random agent at the receiving island. In practice, it was a matter of replacing an agent entry on the internal data structures of the EA algorithms. For the local experiments, the island population was fixed to 5 agents.\n",
    "\n",
    "In the future, there will be attempts to integrate additional meta-learning islands such as:\n",
    "- Genetic Algorithm (GA) [13, 14, 15].\n",
    "- (Multi-agent) Reinforcement Learning (RL) [4].\n",
    "- Basin Hopping (BH) [20].\n",
    "- Estimation of distribution algorithms (EDA) [26], or probabilistic model-building genetic algorithms (PMBGAs) [25].\n",
    "- Tree-structured Parzen Estimator (TPE) [34]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](pics/agentIslandTranspeciation.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture search genes\n",
    "\n",
    "The genes are expressed as bounded real or integer values. They can represent:\n",
    "- Cells/layers of a sequential DNN.\n",
    "- Various model hyper-parameters/rates.\n",
    "- Training optimizers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell Sequence search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The candidate models contain 4 base layers:\n",
    "- 3x Long-Short Term Memory (**LSTM**) [27].\n",
    "- 1x **Fully Connected** cell at the end.\n",
    "\n",
    "The **auxiliary/utility cells** are placed between the base layers. Auxiliary cells placements are determined purely by the evolutionary search process:\n",
    "\n",
    "- Batch Normalization [36].\n",
    "- Gaussian Noise [37].\n",
    "\n",
    "![title](pics/cells.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyper-Parameter search\n",
    "\n",
    "The following **bounded parameters** (ranges expressed in the parentheses) are searched:\n",
    "- Standard Deviation of the Gaussian noise.\n",
    "- Dropout/Recurrent-dropout [35] rate: Fraction of neurons to randomly ignore while training; reduces overfitting.\n",
    "- Cell/layer size.\n",
    "- Max training epochs.\n",
    "- Batch size per epoch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "![title](pics/hyperParameters.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Optimizers** are the algorithms that guide the weight training process of an ANN. The following optimizer options are possible (https://keras.io/optimizers/):\n",
    "\n",
    "- Adagrad (Adaptive Subgradient) [38].\n",
    "- Adadelta [39].\n",
    "- Adam, adamax [40].\n",
    "- Adam (amsgrad version) [41].\n",
    "- Nadam (adam with Nesterov momentum) [42].\n",
    "\n",
    "![title](pics/optimizers.png)\n",
    "\n",
    "It is a subset of the total optimizers available, the only valid options for Recurrent Neural Networks (RNN)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overfitting avoidance\n",
    "\n",
    "**Early Stopping** and **Learning-Rate reduction on plateau** (see: https://keras.io/callbacks/ ) use the *loss* and *validation_loss* values of Mean Squared Error (MSE) during training, to approach a finely-trained model and to avoid overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parallelism\n",
    "An asynchronous version of the **Master-Slave pattern** (see: http://charm.cs.uiuc.edu/research/masterSlave ) allowed for CPU and GPU parallelism of different island species. Agent migration is carried out by slaves (islands) sending candidates to the master (main CPU process). The master decides when and to which island to migrate an agent. **Message Passing Interface (MPI)** (see: https://www.mpi-forum.org/docs/ ) handles the communication between the islands and the master (see: https://mpi4py.readthedocs.io/en/stable/tutorial.html#point-to-point-communication ).\n",
    "\n",
    "The parallel implementation was tested on a local workstation (6 physical CPU cores, 2x Cuda capable GPUs), as well as on the Amazon Elastic Compute Cloud (EC2), on a p3.8xlarge Amazon Machine Image (AMI) (see: https://aws.amazon.com/ec2/instance-types/p3/ ) containing 4x Tesla V100 GPUs. *Note: unfortunately, for now, the models/weights trained on the cloud are incompatible with the local tensorflow installation. After the completion of the \"Python for HPC\" seminar, the GPU cluster of the VSC super-computer will be utilized (see: https://www.vscentrum.be/ )*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. MODELS & RESULTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Particulate Matter 10 micrometer (μm) particle diameter (PM10) MIMO NARX-1 for 16 stations (All Belgian background PM10 stations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training period was 2000 - 2011 and testing 2012, because most of the PM10 stations started operating after 2000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "def smape(a, b):\n",
    "    \"\"\"\n",
    "    Calculates sMAPE\n",
    "    :param a: actual values\n",
    "    :param b: predicted values\n",
    "    :return: sMAPE\n",
    "    \"\"\"\n",
    "    a = np.reshape(a, (-1,))\n",
    "    b = np.reshape(b, (-1,))\n",
    "    return 100.0 * np.mean(2.0 * np.abs(a - b) / (np.abs(a) + np.abs(b))).item()\n",
    "def mean_absolute_percentage_error(y_true, y_pred): \n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)    \n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100 # In %\n",
    "def index_of_agreement(validation, prediction):\n",
    "    return 1 - (np.sum((validation - prediction) ** 2)) / (np.sum((np.abs(prediction - \n",
    "      np.mean(validation)) + np.abs(validation - np.mean(validation))) ** 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Using scale: standardize\n",
      "r shape: (3285, 91)\n",
      "\n",
      "Start Array r:\n",
      " -0.2514263162518368\n",
      "Variables: 90\n",
      "TimeSteps: 3285\n",
      "x_data shape: (3285, 75)\n",
      "y_data shape: (3285, 16)\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot\n",
    "\n",
    "# Make plots inline\n",
    "%matplotlib inline\n",
    "\n",
    "# Make inline plots vector graphics instead of raster graphics\n",
    "from IPython.display import set_matplotlib_formats\n",
    "set_matplotlib_formats('pdf', 'svg')\n",
    "\n",
    "modelLabel = 'rand'\n",
    "\n",
    "dataManipulation = {\n",
    "    \"detrend\": False,\n",
    "    \"scale\": None,\n",
    "#     \"scale\": 'standardize',\n",
    "    # \"scale\": 'normalize',\n",
    "    \"swapEvery\": 50000,  # Do swap island agent every iterations\n",
    "    \"master\": 0,\n",
    "    \"folds\": 10,\n",
    "    \"iterations\": 800,\n",
    "    \"agents\": 20\n",
    "}\n",
    "\n",
    "dataManipulation[\"scale\"] = 'standardize'\n",
    "year = 2012\n",
    "lag = 1\n",
    "    \n",
    "print('Loading data...')\n",
    "print('Using scale: {}'.format(dataManipulation[\"scale\"]))\n",
    "\n",
    "if dataManipulation[\"scale\"] == 'standardize':\n",
    "    r = np.genfromtxt(\"data/PM10_BETN_calendar_1995To2019/PM10_BETN_ts_standardized.csv\", delimiter=',')\n",
    "elif dataManipulation[\"scale\"] == 'normalize':\n",
    "    r = np.genfromtxt(\"data/PM10_BETN_calendar_1995To2019/PM10_BETN_ts_normalized.csv\", delimiter=',')\n",
    "else:\n",
    "    r = np.genfromtxt(\"data/PM10_BETN/PM10_BETN_ts.csv\", delimiter=',')\n",
    "r = np.delete(r, [0], axis=1)  # Remove dates\n",
    "\n",
    "# TODO: greatly decrease r length for testing: 2000-2012 training, 2013 for testing\n",
    "row2000_01_01 = 5481 - 1\n",
    "r = r[row2000_01_01:-1, :]\n",
    "\n",
    "print('r shape:', r.shape)\n",
    "print(\"\\nStart Array r:\\n {}\".format(r[0, 0]))\n",
    "maxLen = r.shape[1] - 1\n",
    "print('Variables: {}'.format(maxLen))\n",
    "print('TimeSteps: {}'.format(r.shape[0]))\n",
    "\n",
    "mimoOutputs = 16\n",
    "x_data = r[:, mimoOutputs:maxLen + 1]\n",
    "y_data = r[:, 0:mimoOutputs]\n",
    "\n",
    "print('x_data shape:', x_data.shape)\n",
    "print(\"y_data shape:\", y_data.shape)\n",
    "\n",
    "y_data = np.array(y_data)\n",
    "x_data_3d = x_data.reshape(x_data.shape[0], 1, x_data.shape[1])  # reshape input to 3D[samples, timesteps, features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0725 06:10:52.438818  2828 deprecation_wrapper.py:119] From C:\\Users\\temp3rr0r\\Anaconda3\\envs\\tensorflowLast\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "('Keyword argument not understood:', 'time_major')",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-1a4e0ba48fbe>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mloaded_model_json\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjson_file\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0mjson_file\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[0mloaded_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel_from_json\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloaded_model_json\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[0mloaded_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"runs/18CellularAutomata3DGrid3x3x3_5AgentsPM10_1994-2018_16_stations_calendar/local/foundModels/bestModelWeights.h5\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflowLast\\lib\\site-packages\\keras\\engine\\saving.py\u001b[0m in \u001b[0;36mmodel_from_json\u001b[1;34m(json_string, custom_objects)\u001b[0m\n\u001b[0;32m    490\u001b[0m     \u001b[0mconfig\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjson_string\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    491\u001b[0m     \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdeserialize\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 492\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mdeserialize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    493\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    494\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflowLast\\lib\\site-packages\\keras\\layers\\__init__.py\u001b[0m in \u001b[0;36mdeserialize\u001b[1;34m(config, custom_objects)\u001b[0m\n\u001b[0;32m     53\u001b[0m                                     \u001b[0mmodule_objects\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mglobs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m                                     \u001b[0mcustom_objects\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 55\u001b[1;33m                                     printable_module_name='layer')\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflowLast\\lib\\site-packages\\keras\\utils\\generic_utils.py\u001b[0m in \u001b[0;36mdeserialize_keras_object\u001b[1;34m(identifier, module_objects, custom_objects, printable_module_name)\u001b[0m\n\u001b[0;32m    143\u001b[0m                     \u001b[0mconfig\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'config'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    144\u001b[0m                     custom_objects=dict(list(_GLOBAL_CUSTOM_OBJECTS.items()) +\n\u001b[1;32m--> 145\u001b[1;33m                                         list(custom_objects.items())))\n\u001b[0m\u001b[0;32m    146\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mCustomObjectScope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    147\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_config\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'config'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflowLast\\lib\\site-packages\\keras\\engine\\sequential.py\u001b[0m in \u001b[0;36mfrom_config\u001b[1;34m(cls, config, custom_objects)\u001b[0m\n\u001b[0;32m    298\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mconf\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlayer_configs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    299\u001b[0m             layer = layer_module.deserialize(conf,\n\u001b[1;32m--> 300\u001b[1;33m                                              custom_objects=custom_objects)\n\u001b[0m\u001b[0;32m    301\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    302\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minputs\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mbuild_input_shape\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflowLast\\lib\\site-packages\\keras\\layers\\__init__.py\u001b[0m in \u001b[0;36mdeserialize\u001b[1;34m(config, custom_objects)\u001b[0m\n\u001b[0;32m     53\u001b[0m                                     \u001b[0mmodule_objects\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mglobs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m                                     \u001b[0mcustom_objects\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 55\u001b[1;33m                                     printable_module_name='layer')\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflowLast\\lib\\site-packages\\keras\\utils\\generic_utils.py\u001b[0m in \u001b[0;36mdeserialize_keras_object\u001b[1;34m(identifier, module_objects, custom_objects, printable_module_name)\u001b[0m\n\u001b[0;32m    143\u001b[0m                     \u001b[0mconfig\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'config'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    144\u001b[0m                     custom_objects=dict(list(_GLOBAL_CUSTOM_OBJECTS.items()) +\n\u001b[1;32m--> 145\u001b[1;33m                                         list(custom_objects.items())))\n\u001b[0m\u001b[0;32m    146\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mCustomObjectScope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    147\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_config\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'config'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflowLast\\lib\\site-packages\\keras\\layers\\wrappers.py\u001b[0m in \u001b[0;36mfrom_config\u001b[1;34m(cls, config, custom_objects)\u001b[0m\n\u001b[0;32m    648\u001b[0m         \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdeserialize\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mdeserialize_layer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    649\u001b[0m         rnn_layer = deserialize_layer(config.pop('layer'),\n\u001b[1;32m--> 650\u001b[1;33m                                       custom_objects=custom_objects)\n\u001b[0m\u001b[0;32m    651\u001b[0m         \u001b[0mnum_constants\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'num_constants'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    652\u001b[0m         \u001b[0mlayer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrnn_layer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflowLast\\lib\\site-packages\\keras\\layers\\__init__.py\u001b[0m in \u001b[0;36mdeserialize\u001b[1;34m(config, custom_objects)\u001b[0m\n\u001b[0;32m     53\u001b[0m                                     \u001b[0mmodule_objects\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mglobs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m                                     \u001b[0mcustom_objects\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 55\u001b[1;33m                                     printable_module_name='layer')\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflowLast\\lib\\site-packages\\keras\\utils\\generic_utils.py\u001b[0m in \u001b[0;36mdeserialize_keras_object\u001b[1;34m(identifier, module_objects, custom_objects, printable_module_name)\u001b[0m\n\u001b[0;32m    145\u001b[0m                                         list(custom_objects.items())))\n\u001b[0;32m    146\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mCustomObjectScope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 147\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_config\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'config'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    148\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m             \u001b[1;31m# Then `cls` may be a function returning a class.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflowLast\\lib\\site-packages\\keras\\layers\\recurrent.py\u001b[0m in \u001b[0;36mfrom_config\u001b[1;34m(cls, config)\u001b[0m\n\u001b[0;32m   1751\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;34m'implementation'\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mconfig\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'implementation'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1752\u001b[0m             \u001b[0mconfig\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'implementation'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1753\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1754\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1755\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflowLast\\lib\\site-packages\\keras\\legacy\\interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[0;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[1;32m---> 91\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflowLast\\lib\\site-packages\\keras\\layers\\recurrent.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, units, activation, recurrent_activation, use_bias, kernel_initializer, recurrent_initializer, bias_initializer, kernel_regularizer, recurrent_regularizer, bias_regularizer, activity_regularizer, kernel_constraint, recurrent_constraint, bias_constraint, dropout, recurrent_dropout, implementation, return_sequences, return_state, go_backwards, stateful, unroll, reset_after, **kwargs)\u001b[0m\n\u001b[0;32m   1638\u001b[0m                                   \u001b[0mstateful\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstateful\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1639\u001b[0m                                   \u001b[0munroll\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0munroll\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1640\u001b[1;33m                                   **kwargs)\n\u001b[0m\u001b[0;32m   1641\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactivity_regularizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mregularizers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mactivity_regularizer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1642\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflowLast\\lib\\site-packages\\keras\\layers\\recurrent.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, cell, return_sequences, return_state, go_backwards, stateful, unroll, **kwargs)\u001b[0m\n\u001b[0;32m    406\u001b[0m                              \u001b[1;34m'(tuple of integers, '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    407\u001b[0m                              'one integer per RNN state).')\n\u001b[1;32m--> 408\u001b[1;33m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mRNN\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    409\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcell\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcell\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    410\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreturn_sequences\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreturn_sequences\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflowLast\\lib\\site-packages\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    126\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mkwarg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    127\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mkwarg\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mallowed_kwargs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 128\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Keyword argument not understood:'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwarg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    129\u001b[0m         \u001b[0mname\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'name'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: ('Keyword argument not understood:', 'time_major')"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "from keras.models import model_from_json\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"  # Use CPU only for inference\n",
    "\n",
    "json_file = open('runs/18CellularAutomata3DGrid3x3x3_5AgentsPM10_1994-2018_16_stations_calendar/local/foundModels/bestModelArchitecture.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "loaded_model.load_weights(\"runs/18CellularAutomata3DGrid3x3x3_5AgentsPM10_1994-2018_16_stations_calendar/local/foundModels/bestModelWeights.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data, x_data_holdout = x_data_3d[:-365], x_data_3d[-365:]\n",
    "y_data, y_data_holdout = y_data[:-365], y_data[-365:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: The MAPE metric naturally goes to infinity, because there are valid sensor zero values ( see: https://en.wikipedia.org/wiki/Mean_absolute_percentage_error#Issues ) in the PM10 dataset. SMAPE is adviced for comparisons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "holdout_prediction = loaded_model.predict(x_data_holdout)\n",
    "\n",
    "sensor_mean = pd.read_pickle(\"data/PM10_BETN/PM10_BETN_ts_mean.pkl\")\n",
    "sensor_std = pd.read_pickle(\"data/PM10_BETN/PM10_BETN_ts_std.pkl\")\n",
    "sensor_mean = np.array(sensor_mean)\n",
    "sensor_std = np.array(sensor_std)\n",
    "\n",
    "predictionAll = (holdout_prediction * sensor_std[0:y_data.shape[1]]) + sensor_mean[0:y_data.shape[1]]\n",
    "y_validationAll = (y_data_holdout * sensor_std[0:y_data.shape[1]]) + sensor_mean[0:y_data.shape[1]]\n",
    "   \n",
    "prediction = predictionAll\n",
    "y_validation = y_validationAll\n",
    "\n",
    "MSE = mean_squared_error(y_validation, prediction)\n",
    "RMSE = np.sqrt(MSE)\n",
    "MAE = mean_absolute_error(y_validation, prediction)\n",
    "MAPE = mean_absolute_percentage_error(y_validation, prediction)\n",
    "SMAPE = ( smape(y_validation[:,0], prediction[:,0]) + smape(y_validation[:,1], prediction[:,1]) + smape(y_validation[:,2], prediction[:,2]) + smape(y_validation[:,3], prediction[:,3])) / 4.0\n",
    "R2_score = r2_score(y_validation, prediction)\n",
    "IOA = index_of_agreement(y_validation, prediction)\n",
    "\n",
    "fig, ax = pyplot.subplots(nrows=1, ncols=1, figsize=(20, 12), sharex=True, sharey=True)\n",
    "ax.set_ylabel('Sensor value')\n",
    "ax.set_xlabel('Day')\n",
    "ax.set_title('PM10 Time-series (All {} Stations) {}: Island LSTM MIMO NARX-{} prediction (MSE: {}, RMSE: {}, MAE: {}, MAPE: {}%, SMAPE: {}%, R2: {}, IOA: {}%)'.format(mimoOutputs, year, lag, np.round(MSE,2), np.round(RMSE,2), np.round(MAE,2), np.round(MAPE,2), np.round(SMAPE,2), np.round(R2_score,2), np.round(IOA * 100,2)))\n",
    "pyplot.plot(y_validation, color='blue')\n",
    "ax.legend(['expected x {}'.format(mimoOutputs)])    \n",
    "\n",
    "fig, ax = pyplot.subplots(nrows=1, ncols=1, figsize=(20, 12), sharex=True, sharey=True)\n",
    "ax.set_ylabel('Sensor value')\n",
    "ax.set_xlabel('Day')\n",
    "ax.set_title('PM10 Time-series (All {} Stations) {}: Island LSTM MIMO NARX-{} prediction (MSE: {}, RMSE: {}, MAE: {}, MAPE: {}%, SMAPE: {}%, R2: {}, IOA: {}%)'.format(mimoOutputs, year, lag, np.round(MSE,2), np.round(RMSE,2), np.round(MAE,2), np.round(MAPE,2), np.round(SMAPE,2), np.round(R2_score,2), np.round(IOA * 100,2)))\n",
    "pyplot.plot(prediction, color='green')\n",
    "ax.legend(['predicted x {}'.format(mimoOutputs)])    \n",
    "  \n",
    "fig, ax = pyplot.subplots(nrows=1, ncols=1, figsize=(20, 12), sharex=True, sharey=True)\n",
    "ax.set_ylabel('Sensor value')\n",
    "ax.set_xlabel('Day')\n",
    "ax.set_title('PM10 Time-series (All {} Stations) {}: Island LSTM MIMO NARX-{} prediction (MSE: {}, RMSE: {}, MAE: {}, MAPE: {}%, SMAPE: {}%, R2: {}, IOA: {}%)'.format(mimoOutputs, year, lag, np.round(MSE,2), np.round(RMSE,2), np.round(MAE,2), np.round(MAPE,2), np.round(SMAPE,2), np.round(R2_score,2), np.round(IOA * 100,2)))\n",
    "ax.plot(y_validation[:,0], color='blue');\n",
    "ax.plot(prediction[:,0], color='red');\n",
    "ax.legend(['expected x {}'.format(mimoOutputs),'predicted x {}'.format(mimoOutputs)])\n",
    "ax.plot(y_validation[:,1:None], color='blue');\n",
    "ax.plot(prediction[:,1:None], color='red');\n",
    "pyplot.savefig(\"PM10all16.svg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stations = [\"BETN043\", \"BETN045\", \"BETN052\", \"BETN054\", \"BETN060\", \"BETN063\", \"BETN066\", \"BETN067\", \"BETN070\", \"BETN073\", \"BETN085\", \"BETN093\", \"BETN100\", \"BETN113\", \"BETN121\", \"BETN132\"]\n",
    "fig, ax = pyplot.subplots(nrows=len(stations), ncols=1, figsize=(20, 120), sharex=True, sharey=True)\n",
    "\n",
    "for i in range(len(stations)):\n",
    "    prediction = predictionAll[:,i]\n",
    "    y_validation = y_validationAll[:,i]\n",
    "\n",
    "    MSE = mean_squared_error(y_validation, prediction)\n",
    "    RMSE = np.sqrt(MSE)\n",
    "    MAE = mean_absolute_error(y_validation, prediction)\n",
    "    MAPE = mean_absolute_percentage_error(y_validation, prediction)\n",
    "    SMAPE = smape(y_validation, prediction)\n",
    "    R2_score = r2_score(y_validation, prediction)\n",
    "    IOA = index_of_agreement(y_validation, prediction)\n",
    "    ax[i].set_ylabel('Sensor value')\n",
    "    ax[i].set_title('PM10 Time-series (Station: {}) {}: Island LSTM MIMO NARX-{} prediction (MSE: {}, RMSE: {}, MAE: {}, MAPE: {}%, SMAPE: {}%, R2: {}, IOA: {}%)'.format(stations[i], year, lag, np.round(MSE,2), np.round(RMSE,2), np.round(MAE,2), np.round(MAPE,2), np.round(SMAPE,2), np.round(R2_score,2), np.round(IOA * 100,2)))\n",
    "    ax[i].plot(y_validationAll[:,i])\n",
    "    ax[i].plot(predictionAll[:,i])    \n",
    "    ax[i].legend(['expected', 'predicted'])    \n",
    "ax[len(stations)-1].set_xlabel('Day');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stations = [\"BETN043\", \"BETN045\", \"BETN052\", \"BETN054\", \"BETN060\", \"BETN063\", \"BETN066\", \"BETN067\", \"BETN070\", \"BETN073\", \"BETN085\", \"BETN093\", \"BETN100\", \"BETN113\", \"BETN121\", \"BETN132\"]\n",
    "fig, ax = pyplot.subplots(nrows=1, ncols=1, figsize=(20, 12), sharex=True, sharey=True)\n",
    "i = 0\n",
    "prediction = predictionAll[:,i]\n",
    "y_validation = y_validationAll[:,i]\n",
    "\n",
    "MSE = mean_squared_error(y_validation, prediction)\n",
    "RMSE = np.sqrt(MSE)\n",
    "MAE = mean_absolute_error(y_validation, prediction)\n",
    "MAPE = mean_absolute_percentage_error(y_validation, prediction)\n",
    "SMAPE = smape(y_validation, prediction)\n",
    "R2_score = r2_score(y_validation, prediction)\n",
    "IOA = index_of_agreement(y_validation, prediction)\n",
    "ax.set_ylabel('Sensor value')\n",
    "ax.set_title('PM10 Time-series (Station: {}) {}: Island LSTM MIMO NARX-{} prediction (MSE: {}, RMSE: {}, MAE: {}, MAPE: {}%, SMAPE: {}%, R2: {}, IOA: {}%)'.format(stations[i], year, lag, np.round(MSE,2), np.round(RMSE,2), np.round(MAE,2), np.round(MAPE,2), np.round(SMAPE,2), np.round(R2_score,2), np.round(IOA * 100,2)))\n",
    "ax.plot(y_validationAll[:,i])\n",
    "ax.plot(predictionAll[:,i])    \n",
    "ax.legend(['expected', 'predicted'])\n",
    "pyplot.savefig(\"pm10Brussels.svg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stations = [\"BETN043\", \"BETN045\", \"BETN052\", \"BETN054\", \"BETN060\", \"BETN063\", \"BETN066\", \"BETN067\", \"BETN070\", \"BETN073\", \"BETN085\", \"BETN093\", \"BETN100\", \"BETN113\", \"BETN121\", \"BETN132\"]\n",
    "fig, ax = pyplot.subplots(nrows=1, ncols=1, figsize=(20, 12), sharex=True, sharey=True)\n",
    "i = 10\n",
    "prediction = predictionAll[:,i]\n",
    "y_validation = y_validationAll[:,i]\n",
    "\n",
    "MSE = mean_squared_error(y_validation, prediction)\n",
    "RMSE = np.sqrt(MSE)\n",
    "MAE = mean_absolute_error(y_validation, prediction)\n",
    "MAPE = mean_absolute_percentage_error(y_validation, prediction)\n",
    "SMAPE = smape(y_validation, prediction)\n",
    "R2_score = r2_score(y_validation, prediction)\n",
    "IOA = index_of_agreement(y_validation, prediction)\n",
    "ax.set_ylabel('Sensor value')\n",
    "ax.set_title('PM10 Time-series (Station: {}) {}: Island LSTM MIMO NARX-{} prediction (MSE: {}, RMSE: {}, MAE: {}, MAPE: {}%, SMAPE: {}%, R2: {}, IOA: {}%)'.format(stations[i], year, lag, np.round(MSE,2), np.round(RMSE,2), np.round(MAE,2), np.round(MAPE,2), np.round(SMAPE,2), np.round(R2_score,2), np.round(IOA * 100,2)))\n",
    "ax.plot(y_validationAll[:,i])\n",
    "ax.plot(predictionAll[:,i])    \n",
    "ax.legend(['expected', 'predicted'])\n",
    "pyplot.savefig(\"pm10Best.svg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-step prediction:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Up to 7 days ahead, testing for 1 year:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_timeseries = 16\n",
    "total_days = 365  # 365\n",
    "max_days_ahead = 7  # 7\n",
    "days = total_days - max_days_ahead\n",
    "\n",
    "MSE_ahead = []\n",
    "RMSE_ahead = []\n",
    "MAE_ahead = []\n",
    "smape_ahead = []\n",
    "ioa_ahead = []\n",
    "R2_score_ahead = []\n",
    "y_ahead = []\n",
    "y_hat_ahead = []\n",
    "\n",
    "sensor_mean = pd.read_pickle(\"data/PM10_BETN/PM10_BETN_ts_mean.pkl\")\n",
    "sensor_std = pd.read_pickle(\"data/PM10_BETN/PM10_BETN_ts_std.pkl\")\n",
    "sensor_mean = np.array(sensor_mean)\n",
    "sensor_std = np.array(sensor_std)\n",
    "\n",
    "for days_ahead in range(1, max_days_ahead + 1):\n",
    "\n",
    "    MSE_array = []\n",
    "    RMSE_array = []\n",
    "    MAE_array = []\n",
    "    smape_array = []\n",
    "    ioa_array = []\n",
    "    R2_score_array = []\n",
    "    y_array = []\n",
    "    y_hat_array = []\n",
    "\n",
    "    for day in range(0, days):\n",
    "        x = []\n",
    "        y = []\n",
    "        y_hat = []\n",
    "        x = np.array([x_data_holdout[day]])\n",
    "        y = np.array(y_data_holdout[day + days_ahead - 1])\n",
    "        y_hat = np.array(loaded_model.predict(x))\n",
    "\n",
    "        for day_ahead in range(1, days_ahead):      \n",
    "            x_next = np.array([x_data_holdout[day + day_ahead]])\n",
    "            x_next[0, 0, -16:] = x[0, 0, :16]  # Current TS (first 16 columns) -> Previous TS (last 16 columns)\n",
    "            x_next[0, 0, :16] = y_hat  # Predicted TS -> Current TS (first 16 columns)\n",
    "            y_hat = np.array(loaded_model.predict(x_next))\n",
    "        y_array.append(y)\n",
    "        y_hat_array.append(y_hat[0])\n",
    "\n",
    "    predictionAll = (y_hat_array * sensor_std[0:y_data.shape[1]]) + sensor_mean[0:y_data.shape[1]]\n",
    "    y_validationAll = (y_array * sensor_std[0:y_data.shape[1]]) + sensor_mean[0:y_data.shape[1]]\n",
    "\n",
    "    MSE = mean_squared_error(y_validationAll, predictionAll)\n",
    "    RMSE = np.sqrt(MSE)\n",
    "    MAE = mean_absolute_error(y_validationAll, predictionAll)\n",
    "    smape_value = smape(y_validationAll, predictionAll)\n",
    "    ioa = index_of_agreement(y_validationAll, predictionAll)\n",
    "    R2_score = r2_score(y_validationAll, predictionAll)\n",
    "    \n",
    "    MSE_ahead.append(MSE)\n",
    "    RMSE_ahead.append(RMSE)\n",
    "    MAE_ahead.append(MAE)\n",
    "    smape_ahead.append(smape_value)\n",
    "    ioa_ahead.append(ioa)\n",
    "    R2_score_ahead.append(R2_score)\n",
    "    \n",
    "    y_ahead.append(y_validationAll)\n",
    "    y_hat_ahead.append(predictionAll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"MSE_ahead\", MSE_ahead)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "red_square = dict(markerfacecolor='g', marker='o')\n",
    "figure_size = (8, 6)\n",
    "\n",
    "metric_strings = [\"MSE\", \"RMSE\", \"MAE\", \"SMAPE\", \"IOA\", \"R2\"]\n",
    "metric_arrays = [MSE_ahead, RMSE_ahead, MAE_ahead, smape_ahead, ioa_ahead, R2_score_ahead]\n",
    "\n",
    "for i in range(len(metric_strings)):\n",
    "\n",
    "    fig, ax = plt.subplots(1, 1, figsize=figure_size)\n",
    "    ax.set_xlabel(\"Value\")\n",
    "    ax.set_ylabel(\"Days Ahead\")\n",
    "    ax.xaxis.grid(True)    \n",
    "    ax.set_title(\"Multi-day ahead prediction metric: {} (PM10: 1-day lag model, 16 stations, 2012 test)\".format(metric_strings[i]))\n",
    "    ax.barh(range(1, len(metric_arrays[i]) + 1), metric_arrays[i]);\n",
    "    plt.yticks(range(1, len(metric_arrays[i]) + 1), range(1, len(metric_arrays[i]) + 1));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plots:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(MSE_ahead)):\n",
    "    \n",
    "    prediction = y_hat_ahead[i]\n",
    "    y_validation = y_ahead[i]\n",
    "    \n",
    "    MSE = mean_squared_error(y_validation, prediction)\n",
    "    RMSE = np.sqrt(MSE)\n",
    "    MAE = mean_absolute_error(y_validation, prediction)\n",
    "    MAPE = mean_absolute_percentage_error(y_validation, prediction)\n",
    "    SMAPE = ( smape(y_validation[:,0], prediction[:,0]) + smape(y_validation[:,1], prediction[:,1]) + smape(y_validation[:,2], prediction[:,2]) + smape(y_validation[:,3], prediction[:,3])) / 4.0\n",
    "    R2_score = r2_score(y_validation, prediction)\n",
    "    IOA = index_of_agreement(y_validation, prediction)\n",
    "    \n",
    "    fig, ax = pyplot.subplots(nrows=1, ncols=1, figsize=(20, 12), sharex=True, sharey=True)\n",
    "    ax.set_ylabel('Sensor value')\n",
    "    ax.set_xlabel('Day')\n",
    "    ax.set_title('Days ahead {}: PM10 Time-series (All {} Stations) {}: Island LSTM MIMO NARX-{} prediction (MSE: {}, RMSE: {}, MAE: {}, MAPE: {}%, SMAPE: {}%, R2: {}, IOA: {}%)'.format(i + 1, mimoOutputs, year, lag, np.round(MSE,2), np.round(RMSE,2), np.round(MAE,2), np.round(MAPE,2), np.round(SMAPE,2), np.round(R2_score,2), np.round(IOA * 100,2)))\n",
    "    pyplot.plot(y_validation, color='blue')\n",
    "    ax.legend(['expected x {}'.format(mimoOutputs)])    \n",
    "\n",
    "    fig, ax = pyplot.subplots(nrows=1, ncols=1, figsize=(20, 12), sharex=True, sharey=True)\n",
    "    ax.set_ylabel('Sensor value')\n",
    "    ax.set_xlabel('Day')\n",
    "    ax.set_title('Days ahead {}: PM10 Time-series (All {} Stations) {}: Island LSTM MIMO NARX-{} prediction (MSE: {}, RMSE: {}, MAE: {}, MAPE: {}%, SMAPE: {}%, R2: {}, IOA: {}%)'.format(i + 1, mimoOutputs, year, lag, np.round(MSE,2), np.round(RMSE,2), np.round(MAE,2), np.round(MAPE,2), np.round(SMAPE,2), np.round(R2_score,2), np.round(IOA * 100,2)))\n",
    "    pyplot.plot(prediction, color='green')\n",
    "    ax.legend(['predicted x {}'.format(mimoOutputs)])    \n",
    "\n",
    "    fig, ax = pyplot.subplots(nrows=1, ncols=1, figsize=(20, 12), sharex=True, sharey=True)\n",
    "    ax.set_ylabel('Sensor value')\n",
    "    ax.set_xlabel('Day')\n",
    "    ax.set_title('Days ahead {}: PM10 Time-series (All {} Stations) {}: Island LSTM MIMO NARX-{} prediction (MSE: {}, RMSE: {}, MAE: {}, MAPE: {}%, SMAPE: {}%, R2: {}, IOA: {}%)'.format(i + 1, mimoOutputs, year, lag, np.round(MSE,2), np.round(RMSE,2), np.round(MAE,2), np.round(MAPE,2), np.round(SMAPE,2), np.round(R2_score,2), np.round(IOA * 100,2)))\n",
    "    ax.plot(y_validation[:,0], color='blue');\n",
    "    ax.plot(prediction[:,0], color='red');\n",
    "    ax.legend(['expected x {}'.format(mimoOutputs),'predicted x {}'.format(mimoOutputs)])\n",
    "    ax.plot(y_validation[:,1:None], color='blue');\n",
    "    ax.plot(prediction[:,1:None], color='red');\n",
    "    pyplot.savefig(\"PM10all16.svg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary (PM10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PM10 mean metrics for all the 16 background stations:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|Metric| Islands LSTM NARX-1 (All BE background 16 stations) |\n",
    "|------|------|\n",
    "|   **MSE**  | 116.01 |\n",
    "|   **MAE**  | 7.62 |\n",
    "|   **MAPE** | N/A |\n",
    "|   **SMAPE**  | 30.27% |\n",
    "|   **IoA**  | 83.68% |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References:\n",
    "[1] J. Orellana Alvear. Application of LS-SVMs to ozone forecasting in Belgium, 2015.  \n",
    "[2] Denis Dumoulin. Fixed-Size Multi-Output LSSVM for Nonlinear System Identification, 2017.  \n",
    "[3] Christoph Bergmeir, Rob J Hyndman, Bonsoo Koo (2018) A note on the validity of cross-validation for evaluating autoregressive time series prediction. Computational Statistics and Data Analysis, 120, 70-83.  \n",
    "[4] Neural Architecture Search with Reinforcement Learning, Barret Zoph, Quoc V. Le. International Conference on Learning Representations, 2017.  \n",
    "[5] Using Evolutionary AutoML to Discover Neural Network Architectures - https://ai.googleblog.com/2018/03/using-evolutionary-automl-to-discover.html  \n",
    "[6] Ian Goodfellow, Yoshua Bengio, and Aaron Courville. 2016. Deep Learning. The MIT Press.  \n",
    "[7] Esteban Real, Sherry Moore, Andrew Selle, Saurabh Saxena, Yutaka Leon Suematsu, Jie Tan, Quoc Le: \"Large-Scale Evolution of Image Classifiers\", 2017  \n",
    "[8] Lingxue Zhu: \"Deep and Confident Prediction for Time Series at Uber\", 2017, 2017 IEEE International Conference on Data Mining Workshops. DOI: 10.1109/ICDMW.2017.19  \n",
    "[9] Makridakis S, Spiliotis E, Assimakopoulos V (2018) Statistical and Machine Learning forecasting methods: Concerns and ways forward. PLOS ONE 13(3): e0194889. https://doi.org/10.1371/journal.pone.0194889  \n",
    "[10] Esteban Real, Alok Aggarwal, Yanping Huang: \"Regularized Evolution for Image Classifier Architecture Search\", 2018  \n",
    "[11] Xin Yao, \"Evolving artificial neural networks,\" in Proceedings of the IEEE, vol. 87, no. 9, pp. 1423-1447, Sept. 1999.\n",
    "doi: 10.1109/5.784219  \n",
    "[12] Risto Miikkulainen, Jason Liang, Elliot Meyerson, Aditya Rawal, Dan Fink, Olivier Francon, Bala Raju, Hormoz Shahrzad, Arshak Navruzyan, Nigel Duffy: \"Evolving Deep Neural Networks\", 2017  \n",
    "[13] Kenneth O. Stanley and Risto Miikkulainen: \"Evolving Neural Networks Through Augmenting Topologies\",  Evolutionary Computation  \n",
    "[14] Variable Length Genomes for Evolutionary Algorithms –C. -y. Lee — 2000 — In Proceedings of the Genetic and Evolutionary Computation Conference, 806. Las Vegas  \n",
    "[15] Tim Kovacs: Genetics-Based Machine Learning. Handbook of Natural Computing 2012: 937-986  \n",
    "[16] Kennedy, J.; Eberhart, R. (1995). \"Particle Swarm Optimization\". Proceedings of IEEE International Conference on Neural Networks. IV. pp. 1942–1948. doi:10.1109/ICNN.1995.488968.  \n",
    "[17] Storn, R., & Price, K. (1997). Differential Evolution – A Simple and Efficient Heuristic for global Optimization over Continuous Spaces. Journal of Global Optimization, 11(4), 341–359. https://doi.org/10.1023/A:1008202821328  \n",
    "[18] P. A. Vikhar, \"Evolutionary algorithms: A critical review and its future prospects,\" 2016 International Conference on Global Trends in Signal Processing, Information Computing and Communication (ICGTSPICC), Jalgaon, 2016, pp. 261-265.\n",
    "doi: 10.1109/ICGTSPICC.2016.7955308  \n",
    "[19] James Bergstra and Yoshua Bengio. 2012. Random search for hyper-parameter optimization. J. Mach. Learn. Res. 13 (February 2012), 281-305.  \n",
    "[20] Wales, David J.; Doye, Jonathan P. K. (1997-07-10). \"Global Optimization by Basin-Hopping and the Lowest Energy Structures of Lennard-Jones Clusters Containing up to 110 Atoms\". The Journal of Physical Chemistry A. 101 (28): 5111–5116. doi:10.1021/jp970984n.  \n",
    "[21] Stanley, Kenneth O. (2017-07-13). \"Neuroevolution: A different kind of deep learning\". O'Reilly Media. Retrieved 2017-09-04.  \n",
    "[22] McCulloch, Warren; Walter Pitts (1943). \"A Logical Calculus of Ideas Immanent in Nervous Activity\". Bulletin of Mathematical Biophysics. 5 (4): 115–133. doi:10.1007/BF02478259.  \n",
    "[23] Claesen, Marc, and Bart De Moor. \"Hyperparameter Search in Machine Learning.\" arXiv preprint arXiv:1502.02127 (2015)  \n",
    "[24] W.N. Martin, J. Lienig, and J.P. Cohoon. Island (migration) models: evolutionary algorithms based on punctuated equilibria. Evolutionary Computation 2: Advanced Algorithms and Operators. Institute of Physics Publishing, Bristol, 2000, chapter 15, pages 101–124.  \n",
    "[25] Pelikan M. () Probabilistic Model-Building Genetic Algorithms. In: Hierarchical Bayesian Optimization Algorithm. Studies in Fuzziness and Soft Computing, vol 170. Springer, Berlin, Heidelberg  \n",
    "[26] Pedro Larrañaga; Jose A. Lozano (2002). Estimation of Distribution Algorithms a New Tool for Evolutionary Computation. Boston, MA: Springer US. ISBN 978-1-4615-1539-5.  \n",
    "[27] Sepp Hochreiter; Jürgen Schmidhuber (1997). \"Long short-term memory\". Neural Computation. 9 (8): 1735–1780. doi:10.1162/neco.1997.9.8.1735.\n",
    "[28] Dee DP, Uppala SM, Simmons AJ, Berrisford P, Poli P, Kobayashi S, Andrae U, Balmaseda MA,Balsamo G, Bauer P, Bechtold P, Beljaars ACM, van deBerg L, Bidlot J, Bormann N, Delsol C, Dragani R,Fuentes  M,  Geer  AJ,  Haimberger  L,  Healy  SB,  Hersbach  H,  H ́olm  EV,  Isaksen  L,  K ̊allberg  P,  K ̈ohler  M,Matricardi M, McNally AP, Monge-Sanz BM, Morcrette J-J, Park B-K, Peubey C, de Rosnay P, Tavolato C,Th ́epaut  J-N,  Vitart  F.  2011.  The  ERA-Interim  reanalysis:  configuration  and  performance  of  the  dataassimilation system.Q. J. R. Meteorol. Soc.137: 553 – 597. DOI:10.1002/qj.828  \n",
    "[29] Armstrong, J. S. (1978). Long-range Forecasting: From Crystal Ball to Computer, Wiley.  \n",
    "[30] Tofallis, C (2015) \"A Better Measure of Relative Prediction Accuracy for Model Selection and Model Estimation\", Journal of the Operational Research Society, 66(8),1352-1362.  \n",
    "[31] Willmott, C. J., S. G. Ackleson, R. E. Davis, J. J. Feddema, K. M. Klink, D. R. Legates, J. O'Donnell, and C. M. Rowe (1985), Statistics for the evaluation and comparison of models, J. Geophys. Res., 90(C5), 8995–9005, doi:10.1029/JC090iC05p08995.  \n",
    "[32] Makridakis, Spyros, (1993), Accuracy measures: theoretical and practical concerns, International Journal of Forecasting, 9, issue 4, p. 527-529.  \n",
    "[33] Tomassini, M. (2005). Spatially Structured Evolutionary Algorithms. New York. Springer. https://doi.org/10.1007/3-540-29938-6  \n",
    "[34] Bergstra, J., Bardenet, R., Bengio, Y., & Kégl, B. (2011). Algorithms for Hyper-Parameter Optimization. In Proceedings of Neural Information Processing Systems (NIPS), 2011. https://doi.org/2012arXiv1206.2944S  \n",
    "[35] Srivastava, N., Hinton, G., Krizhevsky, A., & Salakhutdinov, R. (2014). Dropout: A Simple Way to Prevent Neural Networks from Overfitting. Journal of Machine Learning Research (Vol. 15). Retrieved from http://www.cs.toronto.edu/~rsalakhu/papers/srivastava14a.pdf  \n",
    "[36] Ioffe, S., & Szegedy, C. (2015). Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift. Retrieved from http://arxiv.org/abs/1502.03167  \n",
    "[37] Klambauer, G., Unterthiner, T., Mayr, A., & Hochreiter, S. (2017). Self-Normalizing Neural Networks. Retrieved from http://arxiv.org/abs/1706.02515  \n",
    "[38] Duchi JDUCHI, J., & Singer, Y. (2011). Adaptive Subgradient Methods for Online Learning and Stochastic Optimization * Elad Hazan. Journal of Machine Learning Research (Vol. 12).  \n",
    "[39] Zeiler, M. D. (2012). ADADELTA: An Adaptive Learning Rate Method.  \n",
    "[40] Kingma, D. P., & Ba, J. (2014). Adam: A Method for Stochastic Optimization.  \n",
    "[41] Reddi, S. J., Kale, S., & Kumar, S. (2018). On the Convergence of Adam and Beyond. In Proceedings of the International Conference on Learning Representations (ICLR).  \n",
    "[42] Sutskever, I., Martens, J., Dahl, G., & Hinton, G. (2013). On the importance of initialization and momentum in deep learning.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) 2018 Matthew J. Hergott\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this library\n",
    "# except in compliance with the License. You may obtain a copy of the License at\n",
    "#\n",
    "# www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software distributed under the\n",
    "# License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,\n",
    "# either express or implied. See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = keras.backend\n",
    "\n",
    "from tensorflow.python.keras.models import Model\n",
    "from tensorflow.python.keras.layers import RepeatVector, Concatenate, Activation\n",
    "from tensorflow.python.keras.layers import Reshape, Input, Dense, Dot, LSTM\n",
    "from tensorflow.python.keras import regularizers\n",
    "\n",
    "from tensorflow.python.keras.models import load_model as keras_load_model\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pylab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sometimes helpful to implement own softmax activation function to\n",
    "# better manage calculations along specific axes.\n",
    "def softmax_activation(x):\n",
    "    e = K.exp(x - K.max(x, axis=1, keepdims=True))\n",
    "    s = K.sum(e, axis=1, keepdims=True)\n",
    "    return e / s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionModel(object):\n",
    "\n",
    "    def __init__(self, x, y,\n",
    "                 layer_1_rnn_units,\n",
    "                 attn_dense_nodes=0,\n",
    "                 epochs=100,\n",
    "                 batch_size=128,\n",
    "                 shared_attention_layer=True,\n",
    "                 chg_yield=False,\n",
    "                 float_type='float32',\n",
    "                 regularization=(0.00001, '00001'),\n",
    "                 window=52,\n",
    "                 predict=1):\n",
    "        K.clear_session()\n",
    "        tf.reset_default_graph()\n",
    "        self.set_learning(True)\n",
    "\n",
    "        # Scientific computing uses 'float64' but\n",
    "        # machine learning works much faster with 'float32'.\n",
    "        self.float_type = float_type\n",
    "        K.set_floatx(self.float_type)\n",
    "\n",
    "        # Capture inputs to instance variables.\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.shared_attention_layer = shared_attention_layer\n",
    "\n",
    "        self.layer_1_rnn_units = layer_1_rnn_units\n",
    "        self.layer_2_rnn_units = self.layer_1_rnn_units\n",
    "        self.attn_dense_nodes = attn_dense_nodes\n",
    "\n",
    "        self.num_obs = self.x.shape[0]\n",
    "        self.input_len = self.x.shape[1]\n",
    "        self.input_dims = self.x.shape[2]\n",
    "        self.num_outputs = self.y.shape[1]\n",
    "\n",
    "        self.regularization = regularization[0]\n",
    "\n",
    "        assert self.x.shape[0] == self.y.shape[0]\n",
    "\n",
    "        # Set the directory structure.\n",
    "        self.model_dir = f'models//window_{window}_predict_{predict}//'\n",
    "\n",
    "        self.model_name = f'{\"yield_changes\" if chg_yield==True else \"yield_levels\"}//' \\\n",
    "                          f'model_{layer_1_rnn_units}_rnn_{attn_dense_nodes}_dense_attn_' \\\n",
    "                          f'{epochs}_epochs_' \\\n",
    "                          f'{batch_size}_batch_' \\\n",
    "                          f'{\"shared_attention\" if shared_attention_layer else\"\"}_' \\\n",
    "                          f'{\"change_yield\" if chg_yield else \"level_yield\"}_' \\\n",
    "                          f'{regularization[1]}_reg'\n",
    "\n",
    "        # Activation function for the attention mechanism dense layer(s).\n",
    "        self.attn_dense_activation = 'selu'\n",
    "        self.attn_dense_initializer = 'lecun_normal'\n",
    "        \n",
    "    def delete_model(self):\n",
    "        try:\n",
    "            os.remove(f'{self.model_dir}{self.model_name}.h5')\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "\n",
    "    def load_model(self):\n",
    "        try:\n",
    "            self.model = keras_load_model(f'{self.model_dir}{self.model_name}.h5',\n",
    "                                          custom_objects={'softmax_activation': softmax_activation})\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            return False\n",
    "        return True\n",
    "\n",
    "    def save_model(self):\n",
    "        try:\n",
    "            self.model.save(f'{self.model_dir}{self.model_name}.h5')\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            return False\n",
    "        return True\n",
    "\n",
    "    def set_learning(self, learning):\n",
    "        if learning:\n",
    "            self.is_learning_phase = 1\n",
    "            K.set_learning_phase(self.is_learning_phase)\n",
    "            tf.keras.backend.set_learning_phase(True)\n",
    "        else:\n",
    "            self.is_learning_phase = 0\n",
    "            K.set_learning_phase(self.is_learning_phase)\n",
    "            tf.keras.backend.set_learning_phase(False)\n",
    "\n",
    "    # Method that constructs shared layers. A shared layer means its learned parameters\n",
    "    # are the same no matter where the layer is used in the neural network.\n",
    "    #\n",
    "    def make_shared_layers(self):\n",
    "        if self.regularization > 0.:\n",
    "            self.kernel_reg = regularizers.l2(self.regularization)\n",
    "            self.bias_reg = regularizers.l2(self.regularization)\n",
    "            self.recurrent_reg = regularizers.l2(self.regularization)\n",
    "            self.recurrent_dropout = 0.1\n",
    "        else:\n",
    "            self.kernel_reg = self.bias_reg = self.recurrent_reg = None\n",
    "            self.recurrent_dropout = 0.0\n",
    "\n",
    "        if self.shared_attention_layer:\n",
    "\n",
    "            # This is an optional intermediate dense layer in the attention network.\n",
    "            # If it is not present, the attention mechanism goes straight from inputs to weights.\n",
    "            if self.attn_dense_nodes > 0:\n",
    "                self.attn_middle_dense_layer = Dense(self.attn_dense_nodes,\n",
    "                                                     kernel_regularizer=self.kernel_reg,\n",
    "                                                     bias_regularizer=self.bias_reg,\n",
    "                                                     activation=self.attn_dense_activation,\n",
    "                                                     kernel_initializer=self.attn_dense_initializer,\n",
    "                                                     name='attention_mid_dense_shared')\n",
    "\n",
    "            # This is the layer in the attention mechanism that gives the attention weights.\n",
    "            self.attention_final_dense_layer = Dense(1,\n",
    "                                                     kernel_regularizer=self.kernel_reg,\n",
    "                                                     bias_regularizer=self.bias_reg,\n",
    "                                                     activation=self.attn_dense_activation,\n",
    "                                                     kernel_initializer=self.attn_dense_initializer,\n",
    "                                                     name='attention_final_dense_shared')\n",
    "\n",
    "        # Output-level LSTM cell.\n",
    "        self.layer_2_LSTM_cell = LSTM(self.layer_2_rnn_units,\n",
    "                                      kernel_regularizer=self.kernel_reg,\n",
    "                                      recurrent_regularizer=self.recurrent_reg,\n",
    "                                      bias_regularizer=self.bias_reg,\n",
    "                                      recurrent_dropout=self.recurrent_dropout,\n",
    "                                      return_state=True,\n",
    "                                      name='layer_2_LSTM')\n",
    "\n",
    "        # Final output (i.e., the prediction).\n",
    "        self.dense_output = Dense(1,\n",
    "                                  kernel_regularizer=self.kernel_reg,\n",
    "                                  bias_regularizer=self.bias_reg,\n",
    "                                  activation='linear',\n",
    "                                  name='dense_output')\n",
    "\n",
    "    # Builds the neural network. An LSTM+attention model doesn't need this much code.\n",
    "    # This method is long because it sets lots of layer parameters and because\n",
    "    # it handles four contingencies: (1) whether the attention mechanism is\n",
    "    # always the same or is different for every prediction node, and (2) whether or\n",
    "    # not the attention mechanism has an intermediate dense layer.\n",
    "    #\n",
    "    def build_attention_rnn(self):\n",
    "        self.make_shared_layers()\n",
    "\n",
    "        inputs = Input(shape=(self.input_len, self.input_dims), dtype=self.float_type)\n",
    "\n",
    "        X = LSTM(self.layer_1_rnn_units,\n",
    "                 kernel_regularizer=self.kernel_reg,\n",
    "                 recurrent_regularizer=self.recurrent_reg,\n",
    "                 bias_regularizer=self.bias_reg,\n",
    "                 recurrent_dropout=self.recurrent_dropout,\n",
    "                 return_sequences=True)(inputs)\n",
    "\n",
    "        X = Reshape((self.input_len, self.layer_2_rnn_units))(X)\n",
    "\n",
    "        h_start = Input(shape=(self.layer_2_rnn_units,), name='h_start')\n",
    "        c_start = Input(shape=(self.layer_2_rnn_units,), name='c_start')\n",
    "        h_prev = h_start\n",
    "        c_prev = c_start\n",
    "\n",
    "        outputs = list()\n",
    "\n",
    "        # This section constructs the attention mechanism and the output-level LSTM\n",
    "        # layer that leads to the predictions.\n",
    "        #\n",
    "        # There is an extra LSTM cell that is not attached to any prediction but\n",
    "        # which begins the output-level RNN sequence. This avoids sending in a bunch\n",
    "        # of zero values to the first usage of the attention mechanism.\n",
    "        #\n",
    "        # One way to avoid this extra LSTM cell might be to set the LSTM intial state\n",
    "        # tensors \"h_start\" and \"c_start\" as trainable (instead of zeros).\n",
    "        #\n",
    "        for t in range(self.num_outputs + 1):\n",
    "            h_prev_repeat = RepeatVector(self.input_len)(h_prev)\n",
    "            joined = Concatenate(axis=-1)([X, h_prev_repeat])\n",
    "\n",
    "            if self.attn_dense_nodes > 0:\n",
    "                if self.shared_attention_layer:\n",
    "                    joined = self.attn_middle_dense_layer(joined)\n",
    "                else:\n",
    "                    joined = Dense(self.attn_dense_nodes,\n",
    "                                   kernel_regularizer=self.kernel_reg,\n",
    "                                   bias_regularizer=self.bias_reg,\n",
    "                                   activation=self.attn_dense_activation,\n",
    "                                   kernel_initializer=self.attn_dense_initializer,\n",
    "                                   name=f'attention_mid_dense_{t}')(joined)\n",
    "\n",
    "            if self.shared_attention_layer:\n",
    "                e_vals = self.attention_final_dense_layer(joined)\n",
    "            else:\n",
    "                e_vals = Dense(1,\n",
    "                               kernel_regularizer=self.kernel_reg,\n",
    "                               bias_regularizer=self.bias_reg,\n",
    "                               activation=self.attn_dense_activation,\n",
    "                               kernel_initializer=self.attn_dense_initializer,\n",
    "                               name=f'attention_final_dense_{t}')(joined)\n",
    "\n",
    "            alphas = Activation(softmax_activation, name=f'attention_softmax_{t}')(e_vals)\n",
    "            attentions = Dot(axes=1)([alphas, X])\n",
    "\n",
    "            h_prev, _, c_prev = self.layer_2_LSTM_cell(attentions, initial_state=[h_prev, c_prev])\n",
    "\n",
    "            if t > 0:\n",
    "                out = self.dense_output(h_prev)\n",
    "                outputs.append(out)\n",
    "\n",
    "        self.model = Model(inputs=[inputs, h_start, c_start], outputs=outputs)\n",
    "        self.model.compile(loss='mse', optimizer='adam', metrics=['mse'])\n",
    "\n",
    "        print(self.model.summary())\n",
    "\n",
    "    def fit_model(self):\n",
    "        self.set_learning(True)\n",
    "\n",
    "        h_start = np.zeros((self.num_obs, self.layer_2_rnn_units))\n",
    "        c_start = np.zeros((self.num_obs, self.layer_2_rnn_units))\n",
    "\n",
    "        y_split = np.split(self.y, indices_or_sections=self.num_outputs, axis=1)\n",
    "\n",
    "        self.model.fit([self.x, h_start, c_start],\n",
    "                       y_split,\n",
    "                       epochs=self.epochs,\n",
    "                       batch_size=self.batch_size,\n",
    "                       shuffle=True,\n",
    "                       verbose=2,\n",
    "                       validation_split=0.1)\n",
    "\n",
    "    def calculate_attentions(self, x_data):\n",
    "        self.set_learning(False)\n",
    "\n",
    "        softmax_layer_names = [f'attention_softmax_{t}' for t in range(self.num_outputs + 1)]\n",
    "        softmax_layers = list()\n",
    "\n",
    "        for i, layer_name in enumerate(softmax_layer_names):\n",
    "            if i == 0:\n",
    "                continue\n",
    "            intermediate_layer = Model(inputs=self.model.input,\n",
    "                                       outputs=self.model.get_layer(layer_name).output)\n",
    "            softmax_layers.append(intermediate_layer)\n",
    "\n",
    "        num_obs = x_data.shape[0]\n",
    "        attention_map = np.zeros((num_obs, self.num_outputs, self.input_len))\n",
    "\n",
    "        h_start = np.zeros((1, self.layer_2_rnn_units))\n",
    "        c_start = np.zeros((1, self.layer_2_rnn_units))\n",
    "\n",
    "        for t in range(num_obs):\n",
    "            print(t)\n",
    "            for l_num, layer in enumerate(softmax_layers):\n",
    "                softmax_results = layer.predict([np.expand_dims(x_data[t], axis=0),\n",
    "                                                 h_start,\n",
    "                                                 c_start])\n",
    "                softmax_results = softmax_results[0, :, 0]\n",
    "                attention_map[t, l_num, :] = softmax_results\n",
    "\n",
    "        return attention_map\n",
    "\n",
    "    def heatmap(self, data, title_supplement=None):\n",
    "        plt.rcParams['axes.labelweight'] = 'bold'\n",
    "        plt.rcParams['axes.labelsize'] = 22\n",
    "        plt.rcParams['axes.titlesize'] = 22\n",
    "        plt.rcParams['axes.titleweight'] = 'bold'\n",
    "        plt.rcParams['xtick.labelsize'] = 18\n",
    "        plt.rcParams['ytick.labelsize'] = 18\n",
    "        plt.rcParams['axes.titlepad'] = 12\n",
    "        plt.rcParams['axes.edgecolor'] = '#000000'  # '#FD5E0F'\n",
    "\n",
    "        # Other common color schemes: 'viridis'  'plasma'  'gnuplot'\n",
    "        color_map = 'inferno'\n",
    "        pylab.pcolor(data, cmap=color_map, vmin=0.)\n",
    "        pylab.colorbar()\n",
    "\n",
    "        num_predictions = data.shape[0]\n",
    "        num_timesteps = data.shape[1]\n",
    "\n",
    "        if num_predictions == 4:\n",
    "            pylab.yticks([0.5, 1.5, 2.5, 3.5], ['t+1', 't+2', 't+3', 't+4'])\n",
    "            pylab.ylabel('y: t+1 to t+4')\n",
    "\n",
    "            plt.axhline(y=1., xmin=0.0, xmax=51.0, linewidth=1, color='w')\n",
    "            plt.axhline(y=2., xmin=0.0, xmax=51.0, linewidth=1, color='w')\n",
    "            plt.axhline(y=3., xmin=0.0, xmax=51.0, linewidth=1, color='w')\n",
    "\n",
    "        elif num_predictions == 1:\n",
    "            pylab.yticks([0.5], ['t+1'])\n",
    "            pylab.ylabel('y: t+1')\n",
    "\n",
    "        assert num_timesteps == 52\n",
    "\n",
    "        pylab.xticks([1.5, 11.5, 21.5, 31.5, 41.5, 51.5],\n",
    "                     ['t-50', 't-40', 't-30', 't-20', 't-10', 't'])\n",
    "        pylab.xlabel('x: t-51 to t')\n",
    "\n",
    "        pylab.title(f'{self.model_name} {title_supplement}')\n",
    "\n",
    "        mng = plt.get_current_fig_manager()\n",
    "        mng.window.showMaximized()\n",
    "        pylab.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate dummy data\n",
    "import numpy as np\n",
    "x_train = np.random.random((1000, 20))\n",
    "y_train = keras.utils.to_categorical(np.random.randint(10, size=(1000, 1)), num_classes=10)\n",
    "x_test = np.random.random((100, 20))\n",
    "y_test = keras.utils.to_categorical(np.random.randint(10, size=(100, 1)), num_classes=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() missing 1 required positional argument: 'layer_1_rnn_units'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-bc555b6633d2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mattention_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mAttentionModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: __init__() missing 1 required positional argument: 'layer_1_rnn_units'"
     ]
    }
   ],
   "source": [
    "attention_model = AttentionModel(x_train, y_train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

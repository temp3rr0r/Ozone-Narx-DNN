{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EA run comparisons 2010"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read all experiment data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: '../models/NarxModelSearch/runs/500iters3years128units_RS1/'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-1-8abcb708bd32>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     40\u001B[0m \u001B[0mdirs_list\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     41\u001B[0m \u001B[1;32mfor\u001B[0m \u001B[0mrun_directory\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mrun_directories\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 42\u001B[1;33m     \u001B[0mdirs_list\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mappend\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mos\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mlistdir\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mrun_directory\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     43\u001B[0m \u001B[1;31m#     print(\"dirs:\", os.listdir(run_directory))\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     44\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mFileNotFoundError\u001B[0m: [WinError 3] The system cannot find the path specified: '../models/NarxModelSearch/runs/500iters3years128units_RS1/'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import glob\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "run_directory_prefix = \"../models/NarxModelSearch/runs/\"\n",
    "file_name_pattern = \"*Runs.csv\"\n",
    "columns = [\"datetime\", \"iteration\", \"island\", \"cvMseMean\", \"cvMseStd\", \"cvSmapeMean\", \"cvSmapeStd\", \"holdoutRmse\", \"holdoutSmape\", \"holdoutMape\", \"holdoutMse\", \"holdoutIoa\", \"full_parameters\"]\n",
    "\n",
    "#experiment_directories = [\"18CellularAutomata3DGrid3x3x3_5AgentsO3_2000-2010_1_station_lerp\"]\n",
    "# experiment_directories = [\"18CellularAutomata3DGrid3x3x3_5AgentsO3_2000-2010_1_station_lerp\"]\n",
    "\n",
    "# BO\n",
    "#experiment_directories = [\"500iters3years128units_BO2\", \"500iters3years128units_BO3\", \"500iters3years128units_BO4\", \"500iters3years128units_BO5\", \"500iters3years128units_BO1\"]\n",
    "# Island 18\n",
    "#experiment_directories = [\"500iters3years128units_Island18LS_1\", \"500iters3years128units_Island18LS_3\", \"500iters3years128units_Island18LS_4\", \"500iters3years128units_Island18LS_5\", \"500iters3years128units_Island18LS_6\"]\n",
    "#                           \"500iters3years128units_Island18LS_2\", # TODO: was problematic, too few global iters \n",
    "#                             \"500iters3years128units_Island18LS100_7\"] # TODO: 100 ls runs\n",
    "# Island 5\n",
    "#experiment_directories = [\"500iters3years128units_Island5_1\", \"500iters3years128units_Island5_3\",  \"500iters3years128units_Island5_5\", \"500iters3years128units_Island5_2\", \"500iters3years128units_Island5_4\"]\n",
    "# RS\n",
    "experiment_directories = [\"500iters3years128units_RS3\", \"500iters3years128units_RS2\", \"500iters3years128units_RS1\", \"500iters3years128units_RS4\", \"500iters3years128units_RS5\"]\n",
    "# Island almost 4000 global\n",
    "#experiment_directories = [\"500iters3years128units_Island18_4000+global\"]\n",
    "# Island 400 global + 1000 local\n",
    "#experiment_directories = [\"500iters3years128units_Island18LS_1\"]\n",
    "\n",
    "    \n",
    "run_directories = []\n",
    "for experiment_directory in experiment_directories:\n",
    "    new_run_directory = run_directory_prefix + experiment_directory + \"/\"\n",
    "#     print(\"new_run_directory:\", new_run_directory)\n",
    "    run_directories.append(new_run_directory)\n",
    "\n",
    "worker_directories = [\"local\", \"TX2\", \"VSC\", \"EC2\"] # \"EC@\"\n",
    "\n",
    "# print(run_directories)\n",
    "\n",
    "dirs_list = []\n",
    "for run_directory in run_directories:\n",
    "    dirs_list.append(os.listdir(run_directory))\n",
    "#     print(\"dirs:\", os.listdir(run_directory))\n",
    "    \n",
    "for dirs in dirs_list:\n",
    "    for item in dirs:\n",
    "        if item not in worker_directories:\n",
    "            sub_items = os.listdir(run_directories[0] + item)\n",
    "            for sub_item in sub_items:\n",
    "                sub_path = item +\"/\" + sub_item\n",
    "#                 print(\"sub_path:\", item +\"/\" + sub_item)\n",
    "                worker_directories.append(sub_path)\n",
    "            \n",
    "# print(\"worker_directories:\", worker_directories)\n",
    "\n",
    "paths = []\n",
    "for run_directory in run_directories:\n",
    "    for worker_directory in worker_directories:\n",
    "        experiment = \"\"\n",
    "        if run_directory.startswith(run_directory_prefix):\n",
    "            experiment = run_directory[len(run_directory_prefix):-1]\n",
    "#         print(\"experiment:\", experiment)\n",
    "        paths.append((run_directory + worker_directory + \"/logs/\", worker_directory, experiment))\n",
    "\n",
    "# print(len(paths))\n",
    "    \n",
    "frames = []\n",
    "for path in paths:\n",
    "    for csv_file_path in glob.glob(path[0] + file_name_pattern):\n",
    "#         print(\"csv_file_path:\", csv_file_path)\n",
    "#         print(\"csv_file     :\", csv_file)\n",
    "        df = pd.read_csv(csv_file_path, names=columns, engine=\"python\", index_col=\"datetime\", parse_dates=True)\n",
    "    \n",
    "        if \"EC2\" in path[1]:  # EC2 -> + 1 hour (Ireland)\n",
    "            df.index = df.index + 3600\n",
    "        if \"TX2\" in path[1]:  # TX2 -> + 2 hours (UTC)        \n",
    "            df.index = df.index + 3600 * 2\n",
    "    \n",
    "        df.sort_index(inplace=True)\n",
    "        df[\"optimizer\"] = str(re.search('(.{1,6})Runs.csv', os.path.basename(csv_file_path)).group(1))\n",
    "        df[\"worker\"] = path[1]\n",
    "        df[\"experiment\"] = path[2]\n",
    "        df[\"cvSmapeMeanAccuracy\"] = 100 - df[\"cvSmapeMean\"]\n",
    "        df[\"holdoutSmapeAccuracy\"] = 100 - df[\"holdoutSmape\"]        \n",
    "#         print(\"df.shape\", df.shape)\n",
    "        frames.append(df)\n",
    "#         break\n",
    "\n",
    "df = pd.concat(frames)\n",
    "df.sort_index(inplace=True)\n",
    "\n",
    "print(\"df.shape:\", df.shape)\n",
    "df.tail()        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best MAPE% and IOA%:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values(by=['holdoutMse'], ascending=True).head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values(by=['cvMseMean'], ascending=True).head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values(by=['holdoutIoa'], ascending=False).head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values(by=['holdoutSmape'], ascending=True).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First n iterations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_iters = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_best_model_line = True # Store minimizingMaximizing array(s) to CSV\n",
    "if store_best_model_line:    \n",
    "    df_best_model = pd.DataFrame()\n",
    "    df_best_model[\"x\"] = range(0, 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "variables = [\n",
    "#     \"cvSmapeMeanAccuracy\", \n",
    "#    \"holdoutMse\",\n",
    "     \"holdoutSmapeAccuracy\",\n",
    "#     \"holdoutIoa\"\n",
    "]\n",
    "figure_size = [8, 6]\n",
    "remove_outliers = False\n",
    "outlier_std = 3\n",
    "minimizing = False\n",
    "\n",
    "def getMinimizingMaximizingArray(k, minimizing=True):\n",
    "    minsArray = []\n",
    "    minValue = k[0]\n",
    "    for value in np.array(k):\n",
    "        if minimizing:\n",
    "            if value < minValue:\n",
    "                minValue = value\n",
    "        else:\n",
    "            if value > minValue:\n",
    "                minValue = value\n",
    "        minsArray.append(minValue)    \n",
    "    return np.array(minsArray)\n",
    "    \n",
    "def reject_outliers(data, m=2):\n",
    "    return data[abs(data - np.mean(data)) < m * np.std(data)]\n",
    "\n",
    "\n",
    "for current_experiment in experiment_directories:\n",
    "    \n",
    "    legends = []\n",
    "    for variable in variables:    \n",
    "       \n",
    "        df2 = df.loc[df[\"experiment\"] == current_experiment]        \n",
    "#         df2 = df2.loc[df2[\"optimizer\"] != \"ls\"]            \n",
    "        df2 = df2.drop_duplicates().head(show_iters)\n",
    "        df2.sort_index(inplace=True)\n",
    "    \n",
    "        if remove_outliers:\n",
    "            df2 = df2[~(np.abs(df2[variable] - df2[variable].mean()) > (outlier_std * df2[variable].std()))]\n",
    "    \n",
    "        dict_points = {}        \n",
    "        for optimizer in df2[\"optimizer\"].unique():\n",
    "            dict_points[optimizer + \"_y\"] = []\n",
    "            dict_points[optimizer + \"_x\"] = []\n",
    "        i = 0\n",
    "        for index, row in df2.iterrows():\n",
    "            optimizer = row[\"optimizer\"]\n",
    "            dict_points[optimizer + \"_y\"].append(row[variable])\n",
    "            dict_points[optimizer + \"_x\"].append(i)            \n",
    "            i += 1\n",
    "    \n",
    "        x = np.array(range(0, len(df2[variable])))\n",
    "        y = df2[variable].values \n",
    "        \n",
    "        fig, ax = plt.subplots(1, 1, figsize=figure_size)\n",
    "        ax.set_ylabel(\"100 - SMAPE%\")\n",
    "        ax.set_xlabel('# Fitness Evaluations')\n",
    "       \n",
    "#         z2 = np.polyfit(x, y, 2) \n",
    "#         trendpoly2d = np.poly1d(z2)\n",
    "#         ax.plot(x, trendpoly2d(x) , 'r--')\n",
    "#         legends.append(\"trend 2 degree polynomial\")\n",
    "        \n",
    "#         z12 = np.polyfit(x, y, 12) \n",
    "#         trendpoly12d = np.poly1d(z12)\n",
    "#         ax.plot(x, trendpoly12d(x) , 'c--')\n",
    "#         legends.append(\"trend 12 degree polynomial\")\n",
    "\n",
    "        # Scatter plot all data points based on method    \n",
    "        i = 0\n",
    "        styles = [\"x\", \".\", \"^\", \"+\", \"v\", \"1\", \"*\"]\n",
    "        for optimizer in df2[\"optimizer\"].unique():\n",
    "            ax.plot(dict_points[optimizer + \"_x\"], dict_points[optimizer + \"_y\"], styles[i]);\n",
    "#             legends.append(optimizer)\n",
    "            i += 1\n",
    "        legends = legends + [\"Random (uniform) search (Rand)\", \"Differential Evolution (DE)\", \"Particle Swarm Optimization (PSO)\", \"Genetic Algorithm (GA)\", \"Local Search (LS)\"]\n",
    "\n",
    "        # Linear regression trend\n",
    "        z = np.polyfit(x, y, 1) \n",
    "        trendpoly1d = np.poly1d(z)\n",
    "#         print(\"trendpoly1d: \", trendpoly1d)\n",
    "        ax.plot(x, trendpoly1d(x) , 'k--')\n",
    "        legends.append(\"trend: {:.3f}*x + {:.3f}\".format(z[0], z[1]))        \n",
    "       \n",
    "        best_method = df2[\"optimizer\"].loc[df2[variable].idxmin()].upper()\n",
    "#         ax.set_title('Island Transpeciation - PM10 test accuracy (median: {:.2f}% +/- {:.2f}%, worst: {:.2f}%, best: {:.2f}%)'\n",
    "#                      .format(df2[variable].median(), df2[variable].mad(), df2[variable].min(), df2[variable].max()))\n",
    "        ax.set_title('Island Transpeciation - $O_3$ test 2010 (median: {:.2f}%, worst: {:.2f}%, best: {:.2f}%)'\n",
    "             .format(df2[variable].median(), df2[variable].min(), df2[variable].max()))\n",
    "\n",
    "        best_model = getMinimizingMaximizingArray(df2[variable].values, minimizing)\n",
    "        #best_model = getMinimizingMaximizingArray(df2[variable].values, True)\n",
    "        ax.plot(x, best_model, \"b-\");\n",
    "#         legends.append(\"minimizing \" + variable)\n",
    "        legends.append(\"Best model\")\n",
    "        \n",
    "        if store_best_model_line: # Store minimizingMaximizing array(s) to CSV\n",
    "            if best_model.shape[0] < 500:\n",
    "                best_model2 = np.append(best_model, np.array([best_model.max()] * (500 - best_model.shape[0])))\n",
    "                best_model = best_model2\n",
    "            df_best_model[current_experiment] = best_model\n",
    "\n",
    "        ax.legend(legends)\n",
    "        \n",
    "        ax.grid(True)                         \n",
    "        fig.savefig(\"{}Test.svg\".format(current_experiment))\n",
    "        fig.savefig(\"{}Test.png\".format(current_experiment))\n",
    "#         break\n",
    "#     break\n",
    "\n",
    "if store_best_model_line:\n",
    "    df_best_model.to_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "variables = [\n",
    "    \"cvSmapeMeanAccuracy\", \n",
    "#     \"holdoutMse\",\n",
    "#     \"holdoutSmapeAccuracy\",\n",
    "#     \"holdoutIoa\"\n",
    "]\n",
    "figure_size = [8, 6]\n",
    "remove_outliers = False\n",
    "outlier_std = 3\n",
    "minimizing = False\n",
    "\n",
    "def getMinimizingMaximizingArray(k, minimizing=True):\n",
    "    minsArray = []\n",
    "    minValue = k[0]\n",
    "    for value in np.array(k):\n",
    "        if minimizing:\n",
    "            if value < minValue:\n",
    "                minValue = value\n",
    "        else:\n",
    "            if value > minValue:\n",
    "                minValue = value\n",
    "        minsArray.append(minValue)    \n",
    "    return np.array(minsArray)\n",
    "    \n",
    "def reject_outliers(data, m=2):\n",
    "    return data[abs(data - np.mean(data)) < m * np.std(data)]\n",
    "\n",
    "for current_experiment in experiment_directories:\n",
    "    \n",
    "    legends = []\n",
    "    for variable in variables:    \n",
    "       \n",
    "        df2 = df.loc[df[\"experiment\"] == current_experiment]\n",
    "#         df2 = df2.loc[df2[\"optimizer\"] != \"ls\"]\n",
    "        \n",
    "        df2 = df2.drop_duplicates().head(show_iters)\n",
    "        df2.sort_index(inplace=True)\n",
    "    \n",
    "        if remove_outliers:\n",
    "            df2 = df2[~(np.abs(df2[variable] - df2[variable].mean()) > (outlier_std * df2[variable].std()))]\n",
    "    \n",
    "        dict_points = {}        \n",
    "        for optimizer in df2[\"optimizer\"].unique():\n",
    "            dict_points[optimizer + \"_y\"] = []\n",
    "            dict_points[optimizer + \"_x\"] = []\n",
    "        i = 0\n",
    "        for index, row in df2.iterrows():\n",
    "            optimizer = row[\"optimizer\"]\n",
    "            dict_points[optimizer + \"_y\"].append(row[variable])\n",
    "            dict_points[optimizer + \"_x\"].append(i)            \n",
    "            i += 1\n",
    "    \n",
    "        x = np.array(range(0, len(df2[variable])))\n",
    "        y = df2[variable].values \n",
    "        \n",
    "        fig, ax = plt.subplots(1, 1, figsize=figure_size)\n",
    "        ax.set_ylabel(\"100 - SMAPE%\")\n",
    "        ax.set_xlabel('# Fitness Evaluations')\n",
    "       \n",
    "#         z2 = np.polyfit(x, y, 2) \n",
    "#         trendpoly2d = np.poly1d(z2)\n",
    "#         ax.plot(x, trendpoly2d(x) , 'r--')\n",
    "#         legends.append(\"trend 2 degree polynomial\")\n",
    "        \n",
    "#         z12 = np.polyfit(x, y, 12) \n",
    "#         trendpoly12d = np.poly1d(z12)\n",
    "#         ax.plot(x, trendpoly12d(x) , 'c--')\n",
    "#         legends.append(\"trend 12 degree polynomial\")\n",
    "\n",
    "        # Scatter plot all data points based on method    \n",
    "        i = 0\n",
    "        styles = [\"x\", \".\", \"^\", \"+\", \"v\", \"1\", \"*\"]\n",
    "        for optimizer in df2[\"optimizer\"].unique():\n",
    "            ax.plot(dict_points[optimizer + \"_x\"], dict_points[optimizer + \"_y\"], styles[i]);\n",
    "#             legends.append(optimizer)\n",
    "            i += 1\n",
    "        legends = legends + [\"Bayesian Optimization (BO)\", \"Random (uniform) search (Rand)\", \"Differential Evolution (DE)\", \"Particle Swarm Optimization (PSO)\", \"Genetic Algorithm (GA)\", \"Local Search (LS)\"]\n",
    "\n",
    "        # Linear regression trend\n",
    "        z = np.polyfit(x, y, 1) \n",
    "        trendpoly1d = np.poly1d(z)\n",
    "#         print(\"trendpoly1d: \", trendpoly1d)\n",
    "        ax.plot(x, trendpoly1d(x) , 'k--')\n",
    "        legends.append(\"trend: {:.3f}*x + {:.3f}\".format(z[0], z[1]))        \n",
    "       \n",
    "        best_method = df2[\"optimizer\"].loc[df2[variable].idxmin()].upper()\n",
    "#         ax.set_title('Island Transpeciation - PM10 test accuracy (median: {:.2f}% +/- {:.2f}%, worst: {:.2f}%, best: {:.2f}%)'\n",
    "#                      .format(df2[variable].median(), df2[variable].mad(), df2[variable].min(), df2[variable].max()))\n",
    "        ax.set_title('Island Transpeciation - $O_3$ train CV 2000-2009 (median: {:.2f}%, worst: {:.2f}%, best: {:.2f}%)'\n",
    "             .format(df2[variable].median(), df2[variable].min(), df2[variable].max()))\n",
    "\n",
    "        ax.plot(x, getMinimizingMaximizingArray(df2[variable].values, minimizing), \"b-\");\n",
    "#         legends.append(\"minimizing \" + variable)\n",
    "        legends.append(\"Best model\")\n",
    "\n",
    "        ax.legend(legends)\n",
    "        \n",
    "        ax.grid(True)                         \n",
    "        fig.savefig(\"{}Train.svg\".format(current_experiment))    \n",
    "        fig.savefig(\"{}Train.png\".format(current_experiment))\n",
    "#         break\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bo_mean = df_best_model[['500iters3years128units_BO2',\n",
    "       '500iters3years128units_BO3', '500iters3years128units_BO4',\n",
    "       '500iters3years128units_BO5', '500iters3years128units_BO1']].mean(axis=1)\n",
    "island_18_mean = df_best_model[['500iters3years128units_Island18LS_1',\n",
    "       '500iters3years128units_Island18LS_3',\n",
    "       '500iters3years128units_Island18LS_4',\n",
    "       '500iters3years128units_Island18LS_5',\n",
    "       '500iters3years128units_Island18LS_6']].mean(axis=1)\n",
    "island_5_mean = df_best_model[['500iters3years128units_Island5_1', '500iters3years128units_Island5_3',\n",
    "       '500iters3years128units_Island5_5', '500iters3years128units_Island5_2',\n",
    "       '500iters3years128units_Island5_4']].mean(axis=1)\n",
    "rs_mean = df_best_model[['500iters3years128units_RS3', '500iters3years128units_RS2',\n",
    "       '500iters3years128units_RS1', '500iters3years128units_RS4',\n",
    "       '500iters3years128units_RS5']].mean(axis=1)\n",
    "\n",
    "bo_median = df_best_model[['500iters3years128units_BO2',\n",
    "       '500iters3years128units_BO3', '500iters3years128units_BO4',\n",
    "       '500iters3years128units_BO5', '500iters3years128units_BO1']].median(axis=1)\n",
    "island_18_median = df_best_model[['500iters3years128units_Island18LS_1',\n",
    "       '500iters3years128units_Island18LS_3',\n",
    "       '500iters3years128units_Island18LS_4',\n",
    "       '500iters3years128units_Island18LS_5',\n",
    "       '500iters3years128units_Island18LS_6']].median(axis=1)\n",
    "island_5_median = df_best_model[['500iters3years128units_Island5_1', '500iters3years128units_Island5_3',\n",
    "       '500iters3years128units_Island5_5', '500iters3years128units_Island5_2',\n",
    "       '500iters3years128units_Island5_4']].median(axis=1)\n",
    "rs_median = df_best_model[['500iters3years128units_RS3', '500iters3years128units_RS2',\n",
    "       '500iters3years128units_RS1', '500iters3years128units_RS4',\n",
    "       '500iters3years128units_RS5']].median(axis=1)\n",
    "\n",
    "bo_max = df_best_model[['500iters3years128units_BO2',\n",
    "       '500iters3years128units_BO3', '500iters3years128units_BO4',\n",
    "       '500iters3years128units_BO5', '500iters3years128units_BO1']].max(axis=1)\n",
    "island_18_max = df_best_model[['500iters3years128units_Island18LS_1',\n",
    "       '500iters3years128units_Island18LS_3',\n",
    "       '500iters3years128units_Island18LS_4',\n",
    "       '500iters3years128units_Island18LS_5',\n",
    "       '500iters3years128units_Island18LS_6']].max(axis=1)\n",
    "island_5_max = df_best_model[['500iters3years128units_Island5_1', '500iters3years128units_Island5_3',\n",
    "       '500iters3years128units_Island5_5', '500iters3years128units_Island5_2',\n",
    "       '500iters3years128units_Island5_4']].max(axis=1)\n",
    "rs_max = df_best_model[['500iters3years128units_RS3', '500iters3years128units_RS2',\n",
    "       '500iters3years128units_RS1', '500iters3years128units_RS4',\n",
    "       '500iters3years128units_RS5']].max(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_best_model[\"B_O_mean\"] = bo_mean\n",
    "df_best_model[\"isl_and18_mean\"] = island_18_mean\n",
    "df_best_model[\"isl_and5_mean\"] = island_5_mean\n",
    "df_best_model[\"R_S_mean\"] = rs_mean\n",
    "\n",
    "df_best_model[\"B_O_median\"] = bo_median\n",
    "df_best_model[\"isl_and18_median\"] = island_18_median\n",
    "df_best_model[\"isl_and5_median\"] = island_5_median\n",
    "df_best_model[\"R_S_median\"] = rs_median\n",
    "\n",
    "df_best_model[\"B_O_max\"] = bo_max\n",
    "df_best_model[\"isl_and18_max\"] = island_18_max\n",
    "df_best_model[\"isl_and5_max\"] = island_5_max\n",
    "df_best_model[\"R_S_max\"] = rs_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_best_model[\"B_O_mean\"].median())\n",
    "print(df_best_model[\"isl_and18_mean\"].median())\n",
    "print(df_best_model[\"R_S_mean\"].median())\n",
    "print(df_best_model[\"isl_and5_mean\"].median())\n",
    "print(df_best_model[\"isl_and18_mean\"].max())\n",
    "print(df_best_model[\"isl_and18_max\"].max())\n",
    "print(df_best_model[\"isl_and18_median\"].max())\n",
    "print(df_best_model[\"isl_and5_mean\"].max())\n",
    "print(df_best_model[\"isl_and5_max\"].max())\n",
    "print(df_best_model[\"isl_and5_median\"].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "figure_size = (18,12)\n",
    "fig, ax = plt.subplots(1, 4, figsize=figure_size)\n",
    "\n",
    "metric_type = \"median\" # max, mean\n",
    "\n",
    "ax[0].set_ylabel(\"100 - SMAPE%\")\n",
    "titles = [\"Bayesian Optimization (Best: {}%)\".format(round(df_best_model[\"B_O_max\"].max(), 3), round(df_best_model[\"B_O_median\"].mad(), 2)) , \n",
    "          \"18 islands + LS (Best: {}%)\".format(round(df_best_model[\"isl_and18_max\"].max(), 3), round(df_best_model[\"isl_and18_median\"].mad(), 2)) , \n",
    "          \"5 islands (Best: {}%)\".format(round(df_best_model[\"isl_and5_max\"].max(), 3), round(df_best_model[\"isl_and5_median\"].mad(), 2)) , \n",
    "          \"Random Search (Best: {}%)\".format(round(df_best_model[\"R_S_max\"].max(), 3), round(df_best_model[\"R_S_median\"].mad(), 2))\n",
    "         ]\n",
    "for i in range(0, ax.size):\n",
    "    ax[i].set_xlabel('# Fitness Evaluations')\n",
    "    ax[i].grid(True)   \n",
    "    ax[i].set_ylim([0, 100])\n",
    "    ax[i].set_xlim([0, 510])\n",
    "    ax[i].set_title(titles[i])\n",
    "\n",
    "alpha_value = 0.4\n",
    "\n",
    "for column in df_best_model.columns:\n",
    "    if column != \"x\":\n",
    "        # TODO: change color per type\n",
    "        # TODO: trendline\n",
    "        line_type = \"b-\"\n",
    "        \n",
    "        if re.compile('B_O_{}'.format(metric_type)).search(column):\n",
    "            line_type = \"k-\"\n",
    "            ax[0].plot(df_best_model[\"x\"], df_best_model[column], line_type)        \n",
    "            ax[0].legend([\"BO ({}% +/- {}%)\".format(round(df_best_model[\"B_O_median\"].median(), 3), round(df_best_model[\"B_O_median\"].mad(), 3)) ])\n",
    "\n",
    "        if re.compile('isl_and18_{}'.format(metric_type)).search(column):\n",
    "            line_type = \"k-\"\n",
    "            ax[1].plot(df_best_model[\"x\"], df_best_model[column], line_type)\n",
    "            ax[1].legend([\"18 islands ({}% +/- {}%)\".format(round(df_best_model[\"isl_and18_median\"].median(), 3), round(df_best_model[\"isl_and18_median\"].mad(), 3)) ])                \n",
    "        \n",
    "        if re.compile('isl_and5_{}'.format(metric_type)).search(column):\n",
    "            line_type = \"k-\"\n",
    "            ax[2].plot(df_best_model[\"x\"], df_best_model[column], line_type)\n",
    "            ax[2].legend([\"5 islands ({}% +/- {}%)\".format(round(df_best_model[\"isl_and5_median\"].median(), 3), round(df_best_model[\"isl_and5_median\"].mad(), 3)) ])\n",
    "            \n",
    "        if re.compile('R_S_{}'.format(metric_type)).search(column):\n",
    "            line_type = \"k-\"\n",
    "            ax[3].plot(df_best_model[\"x\"], df_best_model[column], line_type)\n",
    "            ax[3].legend([\"RS ({}% +/- {}%)\".format(round(df_best_model[\"R_S_median\"].median(), 3), round(df_best_model[\"R_S_median\"].mad(), 3)) ])    \n",
    "\n",
    "# Plot max points            \n",
    "for column in df_best_model.columns:\n",
    "    if column != \"x\":\n",
    "        # TODO: change color per type\n",
    "        # TODO: trendline\n",
    "        line_type = \"b-\"\n",
    "        \n",
    "        if re.compile('B_O_max'.format(metric_type)).search(column):\n",
    "            line_type = \"go\"\n",
    "            ax[0].plot(df_best_model[\"x\"].max(), df_best_model[column].max(), line_type)\n",
    "\n",
    "        if re.compile('isl_and18_max'.format(metric_type)).search(column):\n",
    "            line_type = \"bo\"\n",
    "            ax[1].plot(df_best_model[\"x\"].max(), df_best_model[column].max(), line_type)\n",
    "        \n",
    "        if re.compile('isl_and5_max'.format(metric_type)).search(column):\n",
    "            line_type = \"bo\"\n",
    "            ax[2].plot(df_best_model[\"x\"].max(), df_best_model[column].max(), line_type)\n",
    "            \n",
    "        if re.compile('R_S_max'.format(metric_type)).search(column):\n",
    "            line_type = \"ro\"\n",
    "            ax[3].plot(df_best_model[\"x\"].max(), df_best_model[column].max(), line_type)\n",
    "                        \n",
    "            \n",
    "for column in df_best_model.columns:\n",
    "    if column != \"x\":\n",
    "        # TODO: change color per type\n",
    "        # TODO: trendline\n",
    "        line_type = \"b-\"\n",
    "        \n",
    "        if re.compile('BO').search(column):\n",
    "            line_type = \"g--\"\n",
    "            ax[0].plot(df_best_model[\"x\"], df_best_model[column], line_type, alpha=alpha_value)\n",
    "        \n",
    "        if re.compile('Island18').search(column):\n",
    "            line_type = \"b--\"\n",
    "            ax[1].plot(df_best_model[\"x\"], df_best_model[column], line_type, alpha=alpha_value)\n",
    "            \n",
    "        if re.compile('Island5').search(column):\n",
    "            line_type = \"b--\"\n",
    "            ax[2].plot(df_best_model[\"x\"], df_best_model[column], line_type, alpha=alpha_value)\n",
    "            \n",
    "        if re.compile('RS').search(column):\n",
    "            line_type = \"r--\"\n",
    "            ax[3].plot(df_best_model[\"x\"], df_best_model[column], line_type, alpha=alpha_value)\n",
    "            \n",
    "            \n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "figure_size = (18,12)\n",
    "fig, ax = plt.subplots(1, 4, figsize=figure_size)\n",
    "\n",
    "metric_type = \"median\" # max, mean\n",
    "\n",
    "ax[0].set_ylabel(\"100 - SMAPE%\")\n",
    "titles = [\"Bayesian Optimization (Best: {}%)\".format(round(df_best_model[\"B_O_max\"].max(), 3), round(df_best_model[\"B_O_median\"].mad(), 2)) , \n",
    "          \"18 islands + LS (Best: {}%)\".format(round(df_best_model[\"isl_and18_max\"].max(), 3), round(df_best_model[\"isl_and18_median\"].mad(), 2)) , \n",
    "          \"5 islands (Best: {}%)\".format(round(df_best_model[\"isl_and5_max\"].max(), 3), round(df_best_model[\"isl_and5_median\"].mad(), 2)) , \n",
    "          \"Random Search (Best: {}%)\".format(round(df_best_model[\"R_S_max\"].max(), 3), round(df_best_model[\"R_S_median\"].mad(), 2))\n",
    "         ]\n",
    "for i in range(0, ax.size):\n",
    "    ax[i].set_xlabel('# Fitness Evaluations')\n",
    "    ax[i].grid(True)   \n",
    "    ax[i].set_ylim([76, 76.35])\n",
    "    ax[i].set_xlim([0, 510])\n",
    "    ax[i].set_title(titles[i])\n",
    "\n",
    "alpha_value = 0.4\n",
    "\n",
    "for column in df_best_model.columns:\n",
    "    if column != \"x\":\n",
    "        # TODO: change color per type\n",
    "        # TODO: trendline\n",
    "        line_type = \"b-\"\n",
    "        \n",
    "        if re.compile('B_O_{}'.format(metric_type)).search(column):\n",
    "            line_type = \"k-\"\n",
    "            ax[0].plot(df_best_model[\"x\"], df_best_model[column], line_type)        \n",
    "            ax[0].legend([\"BO ({}% +/- {}%)\".format(round(df_best_model[\"B_O_median\"].median(), 3), round(df_best_model[\"B_O_median\"].mad(), 3)) ])\n",
    "\n",
    "        if re.compile('isl_and18_{}'.format(metric_type)).search(column):\n",
    "            line_type = \"k-\"\n",
    "            ax[1].plot(df_best_model[\"x\"], df_best_model[column], line_type)\n",
    "            ax[1].legend([\"18 islands ({}% +/- {}%)\".format(round(df_best_model[\"isl_and18_median\"].median(), 3), round(df_best_model[\"isl_and18_median\"].mad(), 3)) ])                \n",
    "        \n",
    "        if re.compile('isl_and5_{}'.format(metric_type)).search(column):\n",
    "            line_type = \"k-\"\n",
    "            ax[2].plot(df_best_model[\"x\"], df_best_model[column], line_type)\n",
    "            ax[2].legend([\"5 islands ({}% +/- {}%)\".format(round(df_best_model[\"isl_and5_median\"].median(), 3), round(df_best_model[\"isl_and5_median\"].mad(), 3)) ])\n",
    "            \n",
    "        if re.compile('R_S_{}'.format(metric_type)).search(column):\n",
    "            line_type = \"k-\"\n",
    "            ax[3].plot(df_best_model[\"x\"], df_best_model[column], line_type)\n",
    "            ax[3].legend([\"RS ({}% +/- {}%)\".format(round(df_best_model[\"R_S_median\"].median(), 3), round(df_best_model[\"R_S_median\"].mad(), 3)) ])    \n",
    "\n",
    "# Plot max points            \n",
    "for column in df_best_model.columns:\n",
    "    if column != \"x\":\n",
    "        # TODO: change color per type\n",
    "        # TODO: trendline\n",
    "        line_type = \"b-\"\n",
    "        \n",
    "        if re.compile('B_O_max'.format(metric_type)).search(column):\n",
    "            line_type = \"go\"\n",
    "            ax[0].plot(df_best_model[\"x\"].max(), df_best_model[column].max(), line_type)\n",
    "\n",
    "        if re.compile('isl_and18_max'.format(metric_type)).search(column):\n",
    "            line_type = \"bo\"\n",
    "            ax[1].plot(df_best_model[\"x\"].max(), df_best_model[column].max(), line_type)\n",
    "        \n",
    "        if re.compile('isl_and5_max'.format(metric_type)).search(column):\n",
    "            line_type = \"bo\"\n",
    "            ax[2].plot(df_best_model[\"x\"].max(), df_best_model[column].max(), line_type)\n",
    "            \n",
    "        if re.compile('R_S_max'.format(metric_type)).search(column):\n",
    "            line_type = \"ro\"\n",
    "            ax[3].plot(df_best_model[\"x\"].max(), df_best_model[column].max(), line_type)\n",
    "                        \n",
    "            \n",
    "for column in df_best_model.columns:\n",
    "    if column != \"x\":\n",
    "        # TODO: change color per type\n",
    "        # TODO: trendline\n",
    "        line_type = \"b-\"\n",
    "        \n",
    "        if re.compile('BO').search(column):\n",
    "            line_type = \"g--\"\n",
    "            ax[0].plot(df_best_model[\"x\"], df_best_model[column], line_type, alpha=alpha_value)\n",
    "        \n",
    "        if re.compile('Island18').search(column):\n",
    "            line_type = \"b--\"\n",
    "            ax[1].plot(df_best_model[\"x\"], df_best_model[column], line_type, alpha=alpha_value)\n",
    "            \n",
    "        if re.compile('Island5').search(column):\n",
    "            line_type = \"b--\"\n",
    "            ax[2].plot(df_best_model[\"x\"], df_best_model[column], line_type, alpha=alpha_value)\n",
    "            \n",
    "        if re.compile('RS').search(column):\n",
    "            line_type = \"r--\"\n",
    "            ax[3].plot(df_best_model[\"x\"], df_best_model[column], line_type, alpha=alpha_value)\n",
    "            \n",
    "            \n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "figure_size = (18,6)\n",
    "fig, ax = plt.subplots(1, 1, figsize=figure_size)\n",
    "ax.set_ylabel(\"100 - SMAPE%\")\n",
    "titles = [\"Bayesian Optimization (Best: {}%)\".format(round(df_best_model[\"B_O_max\"].max(), 3), round(df_best_model[\"B_O_median\"].mad(), 2)) , \n",
    "          \"18 islands + LS (Best: {}%)\".format(round(df_best_model[\"isl_and18_max\"].max(), 3), round(df_best_model[\"isl_and18_median\"].mad(), 2)) , \n",
    "          \"5 islands (Best: {}%)\".format(round(df_best_model[\"isl_and5_max\"].max(), 3), round(df_best_model[\"isl_and5_median\"].mad(), 2)) , \n",
    "          \"Random Search (Best: {}%)\".format(round(df_best_model[\"R_S_max\"].max(), 3), round(df_best_model[\"R_S_median\"].mad(), 2)) , \n",
    "         ]\n",
    "ax.set_title(\"Optimization progress (5 samples): median +/- Mean Absolute Deviation (MAD)\")\n",
    "\n",
    "# for column in ['B_O_mean', 'isl_and18_mean', 'R_S_mean', 'isl_and5_mean']:\n",
    "#     if column != \"x\":\n",
    "#         if re.compile('B_O_mean').search(column):\n",
    "#             line_type = \"g-\"\n",
    "#             ax.plot(df_best_model[\"x\"], df_best_model[column], line_type)        \n",
    "#         if re.compile('isl_and18_mean').search(column):\n",
    "#             line_type = \"b-\"\n",
    "#             ax.plot(df_best_model[\"x\"], df_best_model[column], line_type)\n",
    "#         if re.compile('R_S_mean').search(column):\n",
    "#             line_type = \"r-\"\n",
    "#             ax.plot(df_best_model[\"x\"], df_best_model[column], line_type)\n",
    "#         if re.compile('isl_and5_mean').search(column):\n",
    "#             line_type = \"k-\"\n",
    "#             ax.plot(df_best_model[\"x\"], df_best_model[column], line_type)\n",
    "\n",
    "for column in ['B_O_median', 'isl_and18_median', 'R_S_median', 'isl_and5_median']:\n",
    "    if column != \"x\":\n",
    "        if re.compile('B_O_median').search(column):\n",
    "            line_type = \"g-\"\n",
    "            ax.plot(df_best_model[\"x\"], df_best_model[column], line_type)        \n",
    "        if re.compile('isl_and18_median').search(column):\n",
    "            line_type = \"b-\"\n",
    "            ax.plot(df_best_model[\"x\"], df_best_model[column], line_type)\n",
    "        if re.compile('isl_and5_median').search(column):\n",
    "            line_type = \"k-\"\n",
    "            ax.plot(df_best_model[\"x\"], df_best_model[column], line_type)  \n",
    "        if re.compile('R_S_median').search(column):\n",
    "            line_type = \"r-\"\n",
    "            ax.plot(df_best_model[\"x\"], df_best_model[column], line_type)\n",
    "\n",
    "                        \n",
    "ax.set_xlabel('# Fitness Evaluations')\n",
    "ax.grid(True)   \n",
    "ax.set_ylim([0, 100])\n",
    "ax.set_xlim([0, 510])\n",
    "ax.legend(titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "figure_size = (18,6)\n",
    "fig, ax = plt.subplots(1, 1, figsize=figure_size)\n",
    "ax.set_ylabel(\"100 - SMAPE%\")\n",
    "titles = [\"Bayesian Optimization (BO) (median: {}% +/- {}%)\".format(round(df_best_model[\"B_O_median\"].median(), 3), round(df_best_model[\"B_O_median\"].mad(), 3)) , \n",
    "          \"18 islands + LS (median: {}% +/- {}%)\".format(round(df_best_model[\"isl_and18_median\"].median(), 3), round(df_best_model[\"isl_and18_median\"].mad(), 3)) , \n",
    "          \"5 islands (median: {}% +/- {}%)\".format(round(df_best_model[\"isl_and5_median\"].median(), 3), round(df_best_model[\"isl_and5_median\"].mad(), 3)) , \n",
    "          \"Random Search (RS) (median: {}% +/- {}%)\".format(round(df_best_model[\"R_S_median\"].median(), 3), round(df_best_model[\"R_S_median\"].mad(), 3)) , \n",
    "         ]\n",
    "ax.set_title(\"Optimization progress (5 samples): median +/- Mean Absolute Deviation (MAD)\")\n",
    "\n",
    "for column in ['B_O_median', 'isl_and18_median', 'R_S_median', 'isl_and5_median']:\n",
    "    if column != \"x\":\n",
    "        if re.compile('B_O_median').search(column):\n",
    "            line_type = \"g-\"\n",
    "            ax.plot(df_best_model[\"x\"], df_best_model[column], line_type)        \n",
    "        if re.compile('isl_and18_median').search(column):\n",
    "            line_type = \"b-\"\n",
    "            ax.plot(df_best_model[\"x\"], df_best_model[column], line_type)\n",
    "        if re.compile('isl_and5_median').search(column):\n",
    "            line_type = \"k-\"\n",
    "            ax.plot(df_best_model[\"x\"], df_best_model[column], line_type)  \n",
    "        if re.compile('R_S_median').search(column):\n",
    "            line_type = \"r-\"\n",
    "            ax.plot(df_best_model[\"x\"], df_best_model[column], line_type)\n",
    "     \n",
    "                        \n",
    "ax.set_xlabel('# Fitness Evaluations')\n",
    "ax.grid(True)   \n",
    "ax.set_ylim([76, 76.3])\n",
    "ax.set_xlim([0, 510])\n",
    "ax.legend(titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "variables = [\n",
    "#     \"cvSmapeMeanAccuracy\", \n",
    "#     \"holdoutMse\",\n",
    "    \"holdoutSmapeAccuracy\",\n",
    "#     \"holdoutIoa\"\n",
    "]\n",
    "figure_size = [10, 8]\n",
    "remove_outliers = False\n",
    "outlier_std = 2\n",
    "minimizing = False\n",
    "\n",
    "def getMinimizingMaximizingArray(k, minimizing=True):\n",
    "    minsArray = []\n",
    "    minValue = k[0]\n",
    "    for value in np.array(k):\n",
    "        if minimizing:\n",
    "            if value < minValue:\n",
    "                minValue = value\n",
    "        else:\n",
    "            if value > minValue:\n",
    "                minValue = value\n",
    "        minsArray.append(minValue)    \n",
    "    return np.array(minsArray)\n",
    "    \n",
    "def reject_outliers(data, m=2):\n",
    "    return data[abs(data - np.mean(data)) < m * np.std(data)]\n",
    "\n",
    "for current_experiment in experiment_directories:\n",
    "    \n",
    "    legends = []\n",
    "    for variable in variables:    \n",
    "       \n",
    "        df2 = df.loc[df[\"experiment\"] == current_experiment]\n",
    "        \n",
    "#         df2 = df2.loc[df2[\"optimizer\"] != \"ls\"]\n",
    "        \n",
    "        df2 = df2.drop_duplicates()\n",
    "        df2.sort_index(inplace=True)\n",
    "    \n",
    "        if remove_outliers:\n",
    "            df2 = df2[~(np.abs(df2[variable] - df2[variable].mean()) > (outlier_std * df2[variable].std()))]\n",
    "    \n",
    "        dict_points = {}        \n",
    "        for optimizer in df2[\"optimizer\"].unique():\n",
    "            dict_points[optimizer + \"_y\"] = []\n",
    "            dict_points[optimizer + \"_x\"] = []\n",
    "        i = 0\n",
    "        for index, row in df2.iterrows():\n",
    "            optimizer = row[\"optimizer\"]\n",
    "            dict_points[optimizer + \"_y\"].append(row[variable])\n",
    "            dict_points[optimizer + \"_x\"].append(i)            \n",
    "            i += 1\n",
    "    \n",
    "        x = np.array(range(0, len(df2[variable])))\n",
    "        y = df2[variable].values \n",
    "        \n",
    "        fig, ax = plt.subplots(1, 1, figsize=figure_size)\n",
    "        ax.set_ylabel(variable)\n",
    "        ax.set_xlabel('# Fitness Evaluations')\n",
    "\n",
    "        # Linear regression trend\n",
    "        z = np.polyfit(x, y, 1) \n",
    "        trendpoly1d = np.poly1d(z)\n",
    "#         print(\"trendpoly1d: \", trendpoly1d)\n",
    "        ax.plot(x, trendpoly1d(x) , 'k--')\n",
    "        legends.append(\"trend \" + variable + \": {:.3f}*x + {:.3f}\".format(z[0], z[1]))\n",
    "        \n",
    "        z2 = np.polyfit(x, y, 2) \n",
    "        trendpoly2d = np.poly1d(z2)\n",
    "        ax.plot(x, trendpoly2d(x) , 'r--')\n",
    "        legends.append(\"trend 2 degree polynomial\")\n",
    "        \n",
    "        z12 = np.polyfit(x, y, 12) \n",
    "        trendpoly12d = np.poly1d(z12)\n",
    "        ax.plot(x, trendpoly12d(x) , 'c--')\n",
    "        legends.append(\"trend 12 degree polynomial\")\n",
    "                \n",
    "        # Scatter plot all data points based on method    \n",
    "        i = 0\n",
    "        styles = [\"x\", \".\", \"^\", \"+\", \"v\", \"1\", \"*\"]\n",
    "        for optimizer in df2[\"optimizer\"].unique():\n",
    "            ax.plot(dict_points[optimizer + \"_x\"], dict_points[optimizer + \"_y\"], styles[i]);\n",
    "            legends.append(optimizer)\n",
    "            i += 1\n",
    "\n",
    "        ax.plot(x, getMinimizingMaximizingArray(df2[variable].values, minimizing), \"b-\");\n",
    "        legends.append(\"minimizing \" + variable)\n",
    "        \n",
    "        ax.legend(legends)\n",
    "        best_method = df2[\"optimizer\"].loc[df2[variable].idxmin()]\n",
    "        ax.set_title('{} (evals: {}, min: {:.2f}, max: {:.2f}, median: {:.2f} +/- {:.2f}, mean: {:.2f} +/- {:.2f}, best: {}) - Experiment: {}'\n",
    "                     .format(variable, len(df2[variable]), df2[variable].min(), df2[variable].max(), df2[variable].median(), df2[variable].mad(), \n",
    "                             df2[variable].mean(), df2[variable].std(), best_method, current_experiment))\n",
    "        ax.grid(True) \n",
    "    \n",
    "#         break\n",
    "#     break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Worker execution times: Bar chart with error bars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def flatten(seq, container=None):\n",
    "    if container is None:\n",
    "        container = []\n",
    "    for s in seq:\n",
    "        try:\n",
    "            iter(s)  # check if it's iterable\n",
    "        except TypeError:\n",
    "            container.append(s)\n",
    "        else:\n",
    "            flatten(s, container)\n",
    "    return container\n",
    "\n",
    "workers = df[\"worker\"].unique()\n",
    "\n",
    "means = []\n",
    "samples = 0\n",
    "outlier_std = 3\n",
    "for current_worker in workers:\n",
    "    experiment_means = []\n",
    "    for current_experiment in experiment_directories:\n",
    "        df_worker = df.loc[df[\"experiment\"] == current_experiment]\n",
    "        df_worker = df_worker.loc[df_worker[\"worker\"] == current_worker]\n",
    "        df_worker = df_worker.drop_duplicates()\n",
    "        df_worker = df_worker.dropna()\n",
    "        df_worker.sort_index(inplace=True)\n",
    "        df_worker['datetimestamp'] = df_worker.index                \n",
    "        df_worker = df_worker[~(np.abs(df_worker[\"datetimestamp\"] - df_worker[\"datetimestamp\"].mean()) > (outlier_std * df_worker[\"datetimestamp\"].std()))]        \n",
    "        diff = df_worker[\"datetimestamp\"].diff().dropna()\n",
    "        mean = diff.mean(skipna=True)\n",
    "        if \"P100\" in current_worker or \"4xV100\" in current_worker:\n",
    "            mean *= 4        \n",
    "        if mean is not None and not math.isnan(mean):\n",
    "            samples += diff.size\n",
    "            experiment_means.append(mean / 60.0)\n",
    "    means.append(experiment_means)\n",
    "\n",
    "# Plot\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(16, 14))\n",
    "workers = list(workers)\n",
    "#fourV100 = workers.index('EC2/4xV100a')\n",
    "#oneV100 = workers.index('EC2/1xV100')\n",
    "#means[fourV100].append(means[oneV100][0])\n",
    "#del means[oneV100]\n",
    "\n",
    "all_data = means\n",
    "labels = []\n",
    "for x, y in zip(['GTX 970 & 1070Ti', 'Jetson TX2', 'V100s + cascadeLake'], [np.mean(x) for x in all_data]):\n",
    "    labels.append(\"{}\\n({:.2f} mins/model)\".format(x, y))\n",
    "\n",
    "axes[0].violinplot(all_data, showmeans=False, showmedians=True)\n",
    "axes[0].set_title('Violin plot - RNN (4 layers) training times vs GPU ({} samples, 99.73% CI)'.format(samples))\n",
    "axes[1].boxplot(all_data)\n",
    "axes[1].set_title('Box plot - RNN (4 layers) training times vs GPU ({} samples, 99.73% CI)'.format(samples))\n",
    "for ax in axes:\n",
    "    ax.yaxis.grid(True)\n",
    "    ax.set_xticks([y+1 for y in range(len(all_data))])\n",
    "    ax.set_xlabel('GPU Workers')\n",
    "    ax.set_ylabel('Time (minutes)')\n",
    "plt.setp(axes, xticks=[y+1 for y in range(len(all_data))],\n",
    "         xticklabels=labels)\n",
    "plt.savefig(\"rnnTrainingTimes.svg\")\n",
    "plt.savefig(\"rnnTrainingTimes.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def flatten(seq, container=None):\n",
    "    if container is None:\n",
    "        container = []\n",
    "    for s in seq:\n",
    "        try:\n",
    "            iter(s)  # check if it's iterable\n",
    "        except TypeError:\n",
    "            container.append(s)\n",
    "        else:\n",
    "            flatten(s, container)\n",
    "    return container\n",
    "\n",
    "workers = df[\"worker\"].unique()\n",
    "\n",
    "means = []\n",
    "samples = 0\n",
    "outlier_std = 3\n",
    "\n",
    "for current_worker in workers:\n",
    "    experiment_means = []\n",
    "    for current_experiment in experiment_directories:\n",
    "        df_worker = df.loc[df[\"experiment\"] == current_experiment]\n",
    "        df_worker = df_worker.loc[df_worker[\"worker\"] == current_worker]\n",
    "        df_worker = df_worker.drop_duplicates()\n",
    "        df_worker = df_worker.dropna()\n",
    "        df_worker.sort_index(inplace=True)\n",
    "        df_worker['datetimestamp'] = df_worker.index                \n",
    "        df_worker = df_worker[~(np.abs(df_worker[\"datetimestamp\"] - df_worker[\"datetimestamp\"].mean()) > (outlier_std * df_worker[\"datetimestamp\"].std()))]        \n",
    "        diff = df_worker[\"datetimestamp\"].diff().dropna()\n",
    "        mean = diff.mean(skipna=True)\n",
    "        if \"P100\" in current_worker or \"4xV100\" in current_worker:\n",
    "            mean *= 4        \n",
    "        if mean is not None and not math.isnan(mean):\n",
    "            samples += diff.size\n",
    "            experiment_means.append(mean / 60.0)\n",
    "    means.append(experiment_means)\n",
    "\n",
    "# Plot\n",
    "\n",
    "\n",
    "figure_size = (6, 4)  # (16, 14)\n",
    "fig, axes = plt.subplots(nrows=1, ncols=1, figsize=figure_size)\n",
    "workers = list(workers)\n",
    "#fourV100 = workers.index('EC2/4xV100a')\n",
    "#oneV100 = workers.index('EC2/1xV100')\n",
    "#means[fourV100].append(means[oneV100][0])\n",
    "#del means[oneV100]\n",
    "\n",
    "all_data = means\n",
    "labels = []\n",
    "for x, y in zip(['GTX 970 & 1070Ti', 'Jetson TX2', 'V100s + cascadeLake'], [np.mean(x) for x in all_data]):\n",
    "    labels.append(\"{}\\n{:.2f}\\nmins/model\".format(x, y))\n",
    "\n",
    "axes.violinplot(all_data, showmeans=False, showmedians=True)\n",
    "axes.set_title('Island DNN training times vs GPU ({} samples, 99.73% CI)'.format(samples))\n",
    "axes.set_title('Training times vs GPU ({} samples, 99.73% CI)'.format(samples))\n",
    "# axes[1].boxplot(all_data)\n",
    "# axes[1].set_title('Box plot - RNN (4 layers) training times vs GPU ({} samples, 99.73% CI)'.format(samples))\n",
    "\n",
    "axes.yaxis.grid(True)\n",
    "axes.set_xticks([y+1 for y in range(len(all_data))])\n",
    "axes.set_xlabel('GPU Workers')\n",
    "axes.set_ylabel('Time (minutes)')\n",
    "plt.setp(axes, xticks=[y+1 for y in range(len(all_data))], xticklabels=labels)\n",
    "plt.savefig(\"rnnTrainingTimes.svg\")\n",
    "plt.savefig(\"rnnTrainingTimes.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.ticker import FuncFormatter\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "models_per_hour = []\n",
    "models_per_hour_std = []\n",
    "for gpu_worker in all_data:\n",
    "    models_per_hour.append(np.mean(60.0 / np.array(gpu_worker)))\n",
    "    models_per_hour_std.append(np.std(60.0 / np.array(gpu_worker)))\n",
    "prices = [5281, (136 + 369) / 2.0, 6865, 352]\n",
    "price_per_model_hour = np.array(prices) / np.array(models_per_hour)\n",
    "\n",
    "price_per_model_hour_std = price_per_model_hour * (np.array(models_per_hour_std) / np.array(models_per_hour))\n",
    "print(\"price_per_model_hour_std\", price_per_model_hour_std)\n",
    "\n",
    "gpu_workers = ['GTX 970 & 1070Ti', 'Jetson TX2']\n",
    "price_per_model_hour = [x for y,x in sorted(zip(gpu_workers, price_per_model_hour))]\n",
    "gpu_workers = [y for y,x in sorted(zip(gpu_workers, price_per_model_hour))]\n",
    "\n",
    "price_per_model_hour_std = [26.98099847, 64.96931466, 495.39481663,  110.15980803]  # TODO: hardcoded\n",
    "\n",
    "x = np.arange(4)\n",
    "def millions(x, pos):\n",
    "    return '{:.0f} '.format(x)\n",
    "\n",
    "formatter = FuncFormatter(millions)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 4))\n",
    "ax.xaxis.set_major_formatter(formatter)\n",
    "plt.barh(x, price_per_model_hour, xerr=price_per_model_hour_std)\n",
    "# plt.barh(x, price_per_model_hour)\n",
    "ax.set_title('Cost per model-hour - RNN (4 layers) vs GPU ({} samples 99.73% CI)'.format(samples))\n",
    "ax.set_ylabel('GPU Workers')\n",
    "ax.set_xlabel('Price ()')\n",
    "plt.yticks(x, gpu_workers)\n",
    "for i, v in enumerate(price_per_model_hour):\n",
    "    ax.text(v + 4, i + 0.1, str(round(v, 1)) + \" +/- {:.1f} per model-hour\".format(price_per_model_hour_std[i]) , color='blue', fontweight='bold')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All models structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import glob\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "run_directory_prefix = \"../models/NarxModelSearch/runs/\"\n",
    "file_name_pattern = \"*Runs.csv\"\n",
    "columns = [\"datetime\", \"iteration\", \"island\", \"cvMseMean\", \"cvMseStd\", \"cvSmapeMean\", \"cvSmapeStd\", \"holdoutRmse\", \"holdoutSmape\", \"holdoutMape\", \"holdoutMse\", \"holdoutIoa\", \"full_parameters\"]\n",
    "\n",
    "experiment_directories = [\"18CellularAutomata3DGrid3x3x3_5AgentsO3_2000-2010_1_station_lerp\"]\n",
    "\n",
    "run_directories = []\n",
    "for experiment_directory in experiment_directories:\n",
    "    new_run_directory = run_directory_prefix + experiment_directory + \"/\"\n",
    "#     print(\"new_run_directory:\", new_run_directory)\n",
    "    run_directories.append(new_run_directory)\n",
    "\n",
    "worker_directories = [\"local\", \"TX2\"]\n",
    "\n",
    "# print(run_directories)\n",
    "\n",
    "dirs_list = []\n",
    "for run_directory in run_directories:\n",
    "    dirs_list.append(os.listdir(run_directory))\n",
    "#     print(\"dirs:\", os.listdir(run_directory))\n",
    "    \n",
    "for dirs in dirs_list:\n",
    "    for item in dirs:\n",
    "        if item not in worker_directories:\n",
    "            sub_items = os.listdir(run_directories[0] + item)\n",
    "            for sub_item in sub_items:\n",
    "                sub_path = item +\"/\" + sub_item\n",
    "#                 print(\"sub_path:\", item +\"/\" + sub_item)\n",
    "                worker_directories.append(sub_path)\n",
    "            \n",
    "# print(\"worker_directories:\", worker_directories)\n",
    "\n",
    "paths = []\n",
    "for run_directory in run_directories:\n",
    "    for worker_directory in worker_directories:\n",
    "        experiment = \"\"\n",
    "        if run_directory.startswith(run_directory_prefix):\n",
    "            experiment = run_directory[len(run_directory_prefix):-1]\n",
    "#         print(\"experiment:\", experiment)\n",
    "        paths.append((run_directory + worker_directory + \"/logs/\", worker_directory, experiment))\n",
    "\n",
    "# print(len(paths))\n",
    "    \n",
    "frames = []\n",
    "for path in paths:\n",
    "    for csv_file_path in glob.glob(path[0] + file_name_pattern):\n",
    "#         print(\"csv_file_path:\", csv_file_path)\n",
    "#         print(\"csv_file     :\", csv_file)\n",
    "        df = pd.read_csv(csv_file_path, names=columns, engine=\"python\", index_col=\"datetime\", parse_dates=True)\n",
    "    \n",
    "        if \"EC2\" in path[1]:  # EC2 -> + 1 hour (Ireland)\n",
    "            df.index = df.index + 3600\n",
    "        if \"TX2\" in path[1]:  # TX2 -> + 2 hours (UTC)        \n",
    "            df.index = df.index + 3600 * 2\n",
    "    \n",
    "        df.sort_index(inplace=True)\n",
    "        df[\"optimizer\"] = str(re.search('(.{1,6})Runs.csv', os.path.basename(csv_file_path)).group(1))\n",
    "        df[\"worker\"] = path[1]\n",
    "        df[\"experiment\"] = path[2]\n",
    "        df[\"cvSmapeMeanAccuracy\"] = 100 - df[\"cvSmapeMean\"]\n",
    "        df[\"holdoutSmapeAccuracy\"] = 100 - df[\"holdoutSmape\"]\n",
    "        \n",
    "        full_parameters = []\n",
    "        for params in df[\"full_parameters\"].tolist():\n",
    "#             print(params)\n",
    "            full_parameters.append([float(z) for z in params.replace('[', '').replace(']', '').split(\", \")])\n",
    "\n",
    "        df[\"full_parameters\"] = full_parameters        \n",
    "        a = [\"batch_size\",\n",
    "            \"epoch_size\",\n",
    "            \"optimizer\",\n",
    "            \"units1\",\n",
    "            \"units2\",\n",
    "            \"units3\",\n",
    "            \"dropout1\",\n",
    "            \"dropout2\",\n",
    "            \"dropout3\",\n",
    "            \"recurrent_dropout1\",\n",
    "            \"recurrent_dropout2\",\n",
    "            \"recurrent_dropout3\",\n",
    "            \"gaussian_noise_std1\",\n",
    "            \"gaussian_noise_std2\",\n",
    "            \"gaussian_noise_std3\",\n",
    "            \"batch_normalization1\",\n",
    "            \"batch_normalization2\",\n",
    "            \"batch_normalization3\",\n",
    "            \"gaussian_noise1\",\n",
    "            \"gaussian_noise2\",\n",
    "            \"gaussian_noise3\",\n",
    "            \"layer_type1\",\n",
    "            \"layer_type2\",\n",
    "            \"layer_type3\",\n",
    "            \"layer_initializer1\",\n",
    "            \"layer_initializer2\",\n",
    "            \"layer_initializer3\"]\n",
    "            \n",
    "        c = 0\n",
    "        for asdf in zip(*full_parameters):\n",
    "            df[a[c]] = asdf\n",
    "            c +=1\n",
    "        \n",
    "        frames.append(df)\n",
    "#         break\n",
    "\n",
    "df = pd.concat(frames)\n",
    "df.sort_index(inplace=True)\n",
    "\n",
    "print(\"df.shape:\", df.shape)\n",
    "df.tail()        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import glob\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "run_directory_prefix = \"../models/NarxModelSearch/runs/\"\n",
    "file_name_pattern = \"*Runs.csv\"\n",
    "columns = [\"datetime\", \"iteration\", \"island\", \"cvMseMean\", \"cvMseStd\", \"cvSmapeMean\", \"cvSmapeStd\", \"holdoutRmse\", \"holdoutSmape\", \"holdoutMape\", \"holdoutMse\", \"holdoutIoa\", \"full_parameters\"]\n",
    "\n",
    "experiment_directories = [\"18CellularAutomata3DGrid3x3x3_5AgentsO3_2000-2010_1_station_lerp\"]\n",
    "\n",
    "run_directories = []\n",
    "for experiment_directory in experiment_directories:\n",
    "    new_run_directory = run_directory_prefix + experiment_directory + \"/\"\n",
    "#     print(\"new_run_directory:\", new_run_directory)\n",
    "    run_directories.append(new_run_directory)\n",
    "\n",
    "worker_directories = [\"local\", \"TX2\"]\n",
    "\n",
    "# print(run_directories)\n",
    "\n",
    "dirs_list = []\n",
    "for run_directory in run_directories:\n",
    "    dirs_list.append(os.listdir(run_directory))\n",
    "#     print(\"dirs:\", os.listdir(run_directory))\n",
    "    \n",
    "for dirs in dirs_list:\n",
    "    for item in dirs:\n",
    "        if item not in worker_directories:\n",
    "            sub_items = os.listdir(run_directories[0] + item)\n",
    "            for sub_item in sub_items:\n",
    "                sub_path = item +\"/\" + sub_item\n",
    "#                 print(\"sub_path:\", item +\"/\" + sub_item)\n",
    "                worker_directories.append(sub_path)\n",
    "            \n",
    "# print(\"worker_directories:\", worker_directories)\n",
    "\n",
    "paths = []\n",
    "for run_directory in run_directories:\n",
    "    for worker_directory in worker_directories:\n",
    "        experiment = \"\"\n",
    "        if run_directory.startswith(run_directory_prefix):\n",
    "            experiment = run_directory[len(run_directory_prefix):-1]\n",
    "#         print(\"experiment:\", experiment)\n",
    "        paths.append((run_directory + worker_directory + \"/logs/\", worker_directory, experiment))\n",
    "\n",
    "# print(len(paths))\n",
    "    \n",
    "frames = []\n",
    "for path in paths:\n",
    "    for csv_file_path in glob.glob(path[0] + file_name_pattern):\n",
    "#         print(\"csv_file_path:\", csv_file_path)\n",
    "#         print(\"csv_file     :\", csv_file)\n",
    "        df = pd.read_csv(csv_file_path, names=columns, engine=\"python\", index_col=\"datetime\", parse_dates=True)\n",
    "    \n",
    "        if \"EC2\" in path[1]:  # EC2 -> + 1 hour (Ireland)\n",
    "            df.index = df.index + 3600\n",
    "        if \"TX2\" in path[1]:  # TX2 -> + 2 hours (UTC)        \n",
    "            df.index = df.index + 3600 * 2\n",
    "    \n",
    "        df.sort_index(inplace=True)\n",
    "        df[\"optimizer\"] = str(re.search('(.{1,6})Runs.csv', os.path.basename(csv_file_path)).group(1))\n",
    "        df[\"worker\"] = path[1]\n",
    "        df[\"experiment\"] = path[2]\n",
    "        df[\"cvSmapeMeanAccuracy\"] = 100 - df[\"cvSmapeMean\"]\n",
    "        df[\"holdoutSmapeAccuracy\"] = 100 - df[\"holdoutSmape\"]\n",
    "        \n",
    "        \n",
    "        full_parameters = []\n",
    "        for params in df[\"full_parameters\"].tolist():\n",
    "#             print(params)\n",
    "            full_parameters.append([float(z) for z in params.replace('[', '').replace(']', '').split(\", \")])\n",
    "\n",
    "        df[\"full_parameters\"] = full_parameters        \n",
    "        a = [\"batch_size\",\n",
    "            \"epoch_size\",\n",
    "            \"optimizer\",\n",
    "            \"units1\",\n",
    "            \"units2\",\n",
    "            \"units3\",\n",
    "            \"dropout1\",\n",
    "            \"dropout2\",\n",
    "            \"dropout3\",\n",
    "            \"recurrent_dropout1\",\n",
    "            \"recurrent_dropout2\",\n",
    "            \"recurrent_dropout3\",\n",
    "            \"gaussian_noise_std1\",\n",
    "            \"gaussian_noise_std2\",\n",
    "            \"gaussian_noise_std3\",\n",
    "            \"batch_normalization1\",\n",
    "            \"batch_normalization2\",\n",
    "            \"batch_normalization3\",\n",
    "            \"gaussian_noise1\",\n",
    "            \"gaussian_noise2\",\n",
    "            \"gaussian_noise3\",\n",
    "            \"layer_type1\",\n",
    "            \"layer_type2\",\n",
    "            \"layer_type3\",\n",
    "            \"layer_initializer1\",\n",
    "            \"layer_initializer2\",\n",
    "            \"layer_initializer3\"]\n",
    "            \n",
    "        c = 0\n",
    "        for asdf in zip(*full_parameters):\n",
    "            df[a[c]] = asdf\n",
    "            c +=1\n",
    "        \n",
    "        df = df.nlargest(1, 'holdoutSmapeAccuracy') # TODO: top 1 champion models\n",
    "\n",
    "        frames.append(df)\n",
    "#         break\n",
    "\n",
    "df = pd.concat(frames)\n",
    "df.sort_index(inplace=True)\n",
    "\n",
    "# df = df.nlargest(20, 'holdoutSmapeAccuracy') # TODO: get top 10 champion models\n",
    "print(\"df.shape:\", df.shape)\n",
    "df.tail()        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "units1 = []\n",
    "units2 = []\n",
    "units3 = []\n",
    "layer_type1 = []\n",
    "layer_type2 = []\n",
    "layer_type3 = []\n",
    "\n",
    "experiment_directories = [\"18CellularAutomata3DGrid3x3x3_5AgentsO3_2000-2010_1_station_lerp\"]\n",
    "\n",
    "# selected_experiments = [experiment_directories[0, 1, 2, 3, 4]]\n",
    "\n",
    "# df_models = df.loc[df[\"experiment\"] == experiment_directories[0]]  # For 1 experiment\n",
    "df_models = df[df['experiment'].isin(experiment_directories[0:1])]\n",
    "# df_models = df[df['experiment'].isin(experiment_directories[4:7])]  # For many experiments\n",
    "df_models = df[df['experiment'].isin(experiment_directories)]  # all experiments\n",
    "first_iterations = 400\n",
    "for element in df_models[\"full_parameters\"].head(first_iterations):  # For the first EA iterations\n",
    "# for element in df_models[\"full_parameters\"]:    \n",
    "    units1.append(element[3])\n",
    "    units2.append(element[4])\n",
    "    units3.append(element[5])\n",
    "    layer_type1.append(np.around(element[21], decimals=0).astype(int))\n",
    "    layer_type2.append(np.around(element[22], decimals=0).astype(int))\n",
    "    layer_type3.append(np.around(element[23], decimals=0).astype(int))\n",
    "#     layer_type1.append(element[15])\n",
    "#     layer_type2.append(element[16])\n",
    "#     layer_type3.append(element[17])\n",
    "units = [units1, units2, units3]    \n",
    "layer_types = [layer_type1, layer_type2, layer_type3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_df_model = df_models.loc[df_models[\"holdoutSmapeAccuracy\"] == df_models[\"holdoutSmapeAccuracy\"].max()]\n",
    "best_pm10_params = best_df_model[\"full_parameters\"]\n",
    "layer_types_list = [\"LSTM\", \"BiLSTM\", \"GRU\", \"BiGRU\", \"RNN\", \"BiRNN\"]\n",
    "for element in best_pm10_params:\n",
    "    print(\"\\nBest model(units):\")\n",
    "    print(np.around(element[3], decimals=0).astype(int), end=\", \")\n",
    "    print(np.around(element[4], decimals=0).astype(int), end=\", \")\n",
    "    print(np.around(element[5], decimals=0).astype(int), end=\"\")\n",
    "    print(\"\\nBest model(layers):\")\n",
    "    print(layer_types_list[np.around(element[21], decimals=0).astype(int)], end=\", \")\n",
    "    print(layer_types_list[np.around(element[22], decimals=0).astype(int)], end=\", \")\n",
    "    print(layer_types_list[np.around(element[23], decimals=0).astype(int)], end=\", \")\n",
    "best_df_model\n",
    "list(best_pm10_params)\n",
    "np.around(list(best_pm10_params), decimals=0).astype(int)\n",
    "print(\"\\nMax holdoutSmapeAccuracy: {:.2f}%\".format(df_models[\"holdoutSmapeAccuracy\"].max()))\n",
    "print(\"\\nMin holdoutMse: {:.2f}\".format(df_models[\"holdoutMse\"].min()))\n",
    "print(\"\\nMin cvMseMean: {:.2f}\".format(df_models[\"cvMseMean\"].min()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Worst:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "worst_df_model = df_models.loc[df_models[\"holdoutSmapeAccuracy\"] == df_models[\"holdoutSmapeAccuracy\"].min()]\n",
    "worst_pm10_params = worst_df_model[\"full_parameters\"]\n",
    "layer_types_list = [\"LSTM\", \"BiLSTM\", \"GRU\", \"BiGRU\", \"RNN\", \"BiRNN\"]\n",
    "for element in worst_pm10_params:\n",
    "    print(\"\\nBest model(units):\")\n",
    "    print(np.around(element[3], decimals=0).astype(int), end=\", \")\n",
    "    print(np.around(element[4], decimals=0).astype(int), end=\", \")\n",
    "    print(np.around(element[5], decimals=0).astype(int), end=\"\")\n",
    "    print(\"\\nBest model(layers):\")\n",
    "    print(layer_types_list[np.around(element[21], decimals=0).astype(int)], end=\", \")\n",
    "    print(layer_types_list[np.around(element[22], decimals=0).astype(int)], end=\", \")\n",
    "    print(layer_types_list[np.around(element[23], decimals=0).astype(int)], end=\", \")\n",
    "\n",
    "worst_df_model    \n",
    "list(worst_pm10_params)\n",
    "np.around(list(worst_pm10_params), decimals=0).astype(int)\n",
    "print(\"\\nMax holdoutSmapeAccuracy: {:.2f}%\".format(df_models[\"holdoutSmapeAccuracy\"].min()))\n",
    "print(\"\\nMax holdoutMse: {:.2f}\".format(df_models[\"holdoutMse\"].max()))\n",
    "print(\"\\nMax cvMseMean: {:.2f}\".format(df_models[\"cvMseMean\"].max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(16, 14))\n",
    "samples = len(units[0])\n",
    "labels = [\"First Layer\", \"Second Layer\", \"Third Layer\"]\n",
    "all_data = units\n",
    "\n",
    "axes[0].violinplot(all_data, showmeans=False, showmedians=True)\n",
    "axes[0].set_title('Violin plot - RNN (4 layers) layer unit count ({} samples)'.format(samples))\n",
    "axes[1].boxplot(all_data)\n",
    "axes[1].set_title('Box plot - RNN (4 layers) layer unit count ({} samples)'.format(samples))\n",
    "for ax in axes:\n",
    "    ax.yaxis.grid(True)\n",
    "    ax.set_xticks([y+1 for y in range(len(all_data))])\n",
    "    ax.set_xlabel('Layers')\n",
    "    ax.set_ylabel('Unit count')\n",
    "plt.setp(axes, xticks=[y+1 for y in range(len(all_data))],\n",
    "         xticklabels=labels)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(16, 14))\n",
    "samples = len(layer_types[0])\n",
    "labels = [\"First Layer\", \"Second Layer\", \"Third Layer\"]\n",
    "all_data = layer_types\n",
    "layer_type_labels = ['LSTM', 'LSTM', 'BiLSTM', 'GRU', 'BiGRU', 'SimpleRNN', 'BiSimpleRNN']\n",
    "\n",
    "axes[0].violinplot(all_data, showmeans=False, showmedians=True)\n",
    "axes[0].set_title('Violin plot - RNN (4 layers) layer type ({} samples)'.format(samples))\n",
    "axes[1].boxplot(all_data)\n",
    "axes[1].set_title('Box plot - RNN (4 layers) layer type ({} samples)'.format(samples))\n",
    "for ax in axes:\n",
    "    ax.yaxis.grid(True)\n",
    "    ax.set_xticks([y+1 for y in range(len(all_data))])\n",
    "    ax.set_xlabel('Layers')\n",
    "    ax.set_ylabel('Layer type')\n",
    "plt.setp(axes, xticks=[y+1 for y in range(len(all_data))],\n",
    "         xticklabels=labels, yticklabels=layer_type_labels)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_models\n",
    "first_iterations = 400  # For the first EA iterations\n",
    "# df_models2 = df.loc[df[\"experiment\"] == current_experiment].head(first_iterations)  # For 1 experiment\n",
    "# df_models2 = df[df['experiment'].isin(experiment_directories[4:7])]  # PM10 For many experiments\n",
    "# df_models2 = df[df['experiment'].isin(experiment_directories[0:4])]  # O3\n",
    "df_models2 = df[df['experiment'].isin(experiment_directories)]  # All\n",
    "\n",
    "# df_models2 = df[df['experiment'].isin(experiment_directories[0:4])]  # For many experiments\n",
    "a.append(\"holdoutSmapeAccuracy\")\n",
    "df_models3 = df_models2[a]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#correlation matrix\n",
    "import seaborn as sns\n",
    "df_models3 = df_models3\n",
    "corrmat = df_models3.corr(method=\"pearson\")\n",
    "f, ax = plt.subplots(figsize=(12, 9))\n",
    "sns.heatmap(corrmat, vmax=.8, square=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saleprice correlation matrix\n",
    "k = 5 #number of variables for heatmap\n",
    "cols = corrmat.nlargest(k, 'holdoutSmapeAccuracy')['holdoutSmapeAccuracy'].index\n",
    "cm = np.corrcoef(df_models3[cols].values.T)\n",
    "sns.set(font_scale=1.5)\n",
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "col_labels = cols.values\n",
    "col_labels[0] = \"100 - SMAPE%\"\n",
    "xticklabels = col_labels\n",
    "yticklabels = col_labels\n",
    "\n",
    "# ax.set_title(\"Neuroevolution correlation matrix (best 5): PM10 (4 layer DNN, {} models)\".format((df_models3[cols].shape[0]))\n",
    "ax.set_title(\"Island DNN correlation matrix: 2018 ({} models)\".format(df_models3.shape[0]))\n",
    "hm = sns.heatmap(cm, ax=ax, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 20}, yticklabels=col_labels, xticklabels=col_labels)\n",
    "hm.set_xticklabels(labels=col_labels, rotation=30)\n",
    "plt.show()\n",
    "fig.savefig(\"pm10correlationMatrixTop.svg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saleprice correlation matrix\n",
    "\n",
    "k = 6 #number of variables for heatmap\n",
    "cols = corrmat.nlargest(k, 'holdoutSmapeAccuracy')['holdoutSmapeAccuracy'].index\n",
    "# cm = np.corrcoef(df_models3[cols].values.T)\n",
    "# cm = df_models3[cols].corr(method='pearson')\n",
    "# cm = df_models3[cols].corr(method='kendall')\n",
    "cor_method = \"spearman\"  # pearson: standard, kendall: tau, spearman: ranked\n",
    "cm = df_models3[cols].corr(method='spearman')\n",
    "\n",
    "# Generate a mask for the upper triangle\n",
    "mask = np.zeros_like(cm, dtype=np.bool)\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "\n",
    "\n",
    "sns.set(font_scale=1.5)\n",
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "col_labels = cols.values\n",
    "col_labels[0] = \"100 - SMAPE%\"\n",
    "xticklabels = col_labels\n",
    "yticklabels = col_labels\n",
    "\n",
    "# ax.set_title(\"Neuroevolution correlation matrix (best 5): PM10 (4 layer DNN, {} models)\".format((df_models3[cols].shape[0]))\n",
    "ax.set_title(\"Island DNN correlation matrix ({}): 2018 (top {} models)\".format(cor_method, df_models3.shape[0]))\n",
    "hm = sns.heatmap(cm, ax=ax, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 20}, yticklabels=col_labels, xticklabels=col_labels, mask=mask)\n",
    "sns.set(style=\"whitegrid\")\n",
    "hm.set_xticklabels(labels=col_labels, rotation=30)\n",
    "plt.show()\n",
    "fig.savefig(\"correlationMatrixRankedTop.svg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## saleprice correlation matrix\n",
    "k = 4 #number of variables for heatmap\n",
    "cols = corrmat.nlargest(k, 'holdoutSmapeAccuracy')['holdoutSmapeAccuracy'].index\n",
    "cm = np.corrcoef(df_models3[cols].values.T)\n",
    "sns.set(font_scale=1.8)\n",
    "figure_size = (8, 6)\n",
    "fig, ax = plt.subplots(figsize=figure_size)\n",
    "col_labels = cols.values\n",
    "col_labels[0] = \"100 - SMAPE%\"\n",
    "xticklabels = col_labels\n",
    "yticklabels = col_labels\n",
    "\n",
    "# ax.set_title(\"Neuroevolution correlation matrix (best 5): PM10 (4 layer DNN, {} models)\".format((df_models3[cols].shape[0]))\n",
    "# ax.set_title(\"DNN (4 layers) correlation matrix: PM10 2018 ({} models)\".format(df_models3.shape[0]))\n",
    "# hm = sns.heatmap(cm, ax=ax, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 20}, yticklabels=col_labels, xticklabels=col_labels)\n",
    "hm = sns.heatmap(cm, ax=ax, cmap=\"Blues\", cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 20}, yticklabels=col_labels, xticklabels=[])\n",
    "hm.set_xticklabels(labels=col_labels, rotation=25)\n",
    "# hm.set_yticklabels(labels=col_labels, rotation=45)\n",
    "ax.set_title('Correlations: Island DNN (top {} models)'.format(df_models3.shape[0]))\n",
    "plt.show()\n",
    "fig.savefig(\"dnnCorrelationMatrixTop.svg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saleprice correlation matrix\n",
    "k = 5 #number of variables for heatmap\n",
    "cols = corrmat.nlargest(k, 'holdoutSmapeAccuracy')['holdoutSmapeAccuracy'].index\n",
    "cm = np.corrcoef(df_models3[cols].values.T)\n",
    "sns.set(font_scale=1.25)\n",
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "col_labels = cols.values\n",
    "col_labels[0] = \"100 - SMAPE%\"\n",
    "xticklabels = col_labels\n",
    "yticklabels = col_labels\n",
    "\n",
    "# ax.set_title(\"Neuroevolution correlation matrix (best 5): PM10 (4 layer DNN, {} models)\".format((df_models3[cols].shape[0]))\n",
    "ax.set_title(\"DNN (4 layers) correlation matrix: PM10 2018 ({} models)\".format(df_models3.shape[0]))\n",
    "hm = sns.heatmap(cm, ax=ax, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 10}, yticklabels=col_labels, xticklabels=col_labels)\n",
    "hm.set_xticklabels(labels=col_labels, rotation=30)\n",
    "plt.show()\n",
    "fig.savefig(\"pm10correlationMatrixTop.svg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.set(style=\"whitegrid\", palette=\"Blues\")\n",
    "sns.set(font_scale=2.0)\n",
    "\n",
    "figure_size = (8, 6)\n",
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=figure_size)\n",
    "labels = [\"First\", \"Second\", \"Third\"]\n",
    "\n",
    "# ax.boxplot(all_data)\n",
    "# ax.boxplot(df_models3[\"units1\"])\n",
    "# ax.boxplot([df_models3[\"units1\"], df_models3[\"units2\"], df_models3[\"units3\"]])\n",
    "# ax.violinplot([df_models3[\"units1\"], df_models3[\"units2\"], df_models3[\"units3\"]], showmeans=False, showmedians=False)\n",
    "\n",
    "c = \"blue\"\n",
    "\n",
    "# ax.boxplot([df_models3[\"units1\"], df_models3[\"units2\"], df_models3[\"units3\"]], showfliers=True)\n",
    "\n",
    "ax.boxplot([df_models3[\"units1\"], df_models3[\"units2\"], df_models3[\"units3\"]], showfliers=True, notch=False, patch_artist=True,\n",
    "#             boxprops=dict(facecolor=c, color=c),\n",
    "            capprops=dict(color=c),\n",
    "            whiskerprops=dict(color=c),\n",
    "            flierprops=dict(color=c, markeredgecolor=c),\n",
    "            medianprops=dict(color=c))\n",
    "\n",
    "# axes.scatter(all_data)\n",
    "ax.set_title('Layer size: Island DNN (top {} models)'.format(df_models3.shape[0]))\n",
    "\n",
    "ax.yaxis.grid(True)\n",
    "ax.set_xticks([y+1 for y in range(len(labels))])\n",
    "ax.set_xlabel('RNN Layers')\n",
    "ax.set_ylabel('Unit count')\n",
    "plt.setp(ax, xticks=[y+1 for y in range(len(labels))], xticklabels=labels)\n",
    "plt.show()\n",
    "\n",
    "fig.savefig(\"layerSizeBoxplotTop.svg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_models3.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.set(style=\"whitegrid\", palette=\"Blues\")\n",
    "sns.set(font_scale=2.0)\n",
    "\n",
    "figure_size = (8, 6)\n",
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=figure_size)\n",
    "labels = [\"First\", \"Second\", \"Third\"]\n",
    "\n",
    "c = \"blue\"\n",
    "# ax.boxplot([df_models3[\"dropout1\"], df_models3[\"dropout2\"], df_models3[\"dropout3\"]], showfliers=True, notch=False, patch_artist=True,\n",
    "#             capprops=dict(color=c),\n",
    "#             whiskerprops=dict(color=c),\n",
    "#             flierprops=dict(color=c, markeredgecolor=c),\n",
    "#             medianprops=dict(color=c))\n",
    "\n",
    "ax.violinplot([df_models3[\"dropout1\"], df_models3[\"dropout2\"], df_models3[\"dropout3\"]], showmeans=False, showmedians=True)\n",
    "\n",
    "ax.set_title('Dropout ratio: Island DNN (top {} models)'.format(df_models3.shape[0]))\n",
    "ax.yaxis.grid(True)\n",
    "ax.set_xticks([y+1 for y in range(len(labels))])\n",
    "ax.set_xlabel('RNN Layers')\n",
    "ax.set_ylabel('Dropout ratio (%)')\n",
    "plt.setp(ax, xticks=[y+1 for y in range(len(labels))], xticklabels=labels)\n",
    "plt.show()\n",
    "\n",
    "fig.savefig(\"dropoutRatioViolinplotTop.svg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.set(style=\"whitegrid\", palette=\"Blues\")\n",
    "sns.set(font_scale=2.0)\n",
    "\n",
    "figure_size = (8, 6)\n",
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=figure_size)\n",
    "labels = [\"First\", \"Second\", \"Third\"]\n",
    "\n",
    "c = \"blue\"\n",
    "# ax.boxplot([df_models3[\"recurrent_dropout1\"], df_models3[\"recurrent_dropout2\"], df_models3[\"recurrent_dropout3\"]], showfliers=True, notch=False, patch_artist=True,\n",
    "#             capprops=dict(color=c),\n",
    "#             whiskerprops=dict(color=c),\n",
    "#             flierprops=dict(color=c, markeredgecolor=c),\n",
    "#             medianprops=dict(color=c))\n",
    "\n",
    "ax.violinplot([df_models3[\"recurrent_dropout1\"], df_models3[\"recurrent_dropout2\"], df_models3[\"recurrent_dropout3\"]], showmeans=False, showmedians=True)\n",
    "\n",
    "ax.set_title('Recurrent dropout ratio: Island DNN (top {} models)'.format(df_models3.shape[0]))\n",
    "ax.yaxis.grid(True)\n",
    "ax.set_xticks([y+1 for y in range(len(labels))])\n",
    "ax.set_xlabel('RNN Layers')\n",
    "ax.set_ylabel('Recurrent dropout ratio (%)')\n",
    "plt.setp(ax, xticks=[y+1 for y in range(len(labels))], xticklabels=labels)\n",
    "plt.show()\n",
    "\n",
    "fig.savefig(\"recurrentDropoutRatioViolinplotTop.svg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.set(style=\"whitegrid\", palette=\"Blues\")\n",
    "sns.set(font_scale=2.0)\n",
    "\n",
    "figure_size = (8, 6)\n",
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=figure_size)\n",
    "labels = [\"First\", \"Second\", \"Third\"]\n",
    "\n",
    "c = \"blue\"\n",
    "# ax.boxplot([df_models3[\"recurrent_dropout1\"], df_models3[\"recurrent_dropout2\"], df_models3[\"recurrent_dropout3\"]], showfliers=True, notch=False, patch_artist=True,\n",
    "#             capprops=dict(color=c),\n",
    "#             whiskerprops=dict(color=c),\n",
    "#             flierprops=dict(color=c, markeredgecolor=c),\n",
    "#             medianprops=dict(color=c))\n",
    "\n",
    "ax.violinplot([df_models3[\"gaussian_noise_std1\"], df_models3[\"gaussian_noise_std2\"], df_models3[\"gaussian_noise_std3\"]], showmeans=False, showmedians=True)\n",
    "\n",
    "ax.set_title('Gaussian noise STD: Island DNN (top {} models)'.format(df_models3.shape[0]))\n",
    "ax.yaxis.grid(True)\n",
    "ax.set_xticks([y+1 for y in range(len(labels))])\n",
    "ax.set_xlabel('RNN Layers')\n",
    "ax.set_ylabel('STD')\n",
    "plt.setp(ax, xticks=[y+1 for y in range(len(labels))], xticklabels=labels)\n",
    "plt.show()\n",
    "\n",
    "fig.savefig(\"gaussianNoiseStdViolinplotTop.svg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.set(style=\"whitegrid\", palette=\"Blues\")\n",
    "sns.set(font_scale=2.0)\n",
    "\n",
    "figure_size = (8, 6)\n",
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=figure_size)\n",
    "labels = [\"First\", \"Second\", \"Third\"]\n",
    "\n",
    "c = \"blue\"\n",
    "ax.boxplot([df_models3[\"recurrent_dropout1\"], df_models3[\"recurrent_dropout2\"], df_models3[\"recurrent_dropout3\"]], showfliers=True, notch=False, patch_artist=True,\n",
    "            capprops=dict(color=c),\n",
    "            whiskerprops=dict(color=c),\n",
    "            flierprops=dict(color=c, markeredgecolor=c),\n",
    "            medianprops=dict(color=c))\n",
    "\n",
    "ax.set_title('Recurrent dropout ratio: Island DNN (top {} models)'.format(df_models3.shape[0]))\n",
    "ax.yaxis.grid(True)\n",
    "ax.set_xticks([y+1 for y in range(len(labels))])\n",
    "ax.set_xlabel('RNN Layers')\n",
    "ax.set_ylabel('Recurrent dropout ratio (%)')\n",
    "plt.setp(ax, xticks=[y+1 for y in range(len(labels))], xticklabels=labels)\n",
    "plt.show()\n",
    "\n",
    "# fig.savefig(\"layerSizeBoxplot.svg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style=\"whitegrid\")\n",
    "sns.set(font_scale=2.0)\n",
    "figure_size = (8, 6)\n",
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=figure_size)\n",
    "layer_type_labels = ['LSTM', 'BiLSTM', 'GRU', 'BiGRU', 'SimpleRNN', 'BiSimpleRNN']\n",
    "\n",
    "layer_type_counts = df_models3[\"layer_type3\"].round().astype(int).value_counts()\n",
    "bp_layers = sns.barplot(x=[0, 1, 2, 3, 4, 5], y=layer_type_counts)\n",
    "ax.set_title('Layer types: DNNs with 3 x RNN layers (top {} models)'.format(df_models3.shape[0]))\n",
    "\n",
    "ax.yaxis.grid(True)\n",
    "# ax.set_xticks([y+1 for y in range(len(labels))])\n",
    "ax.set_ylabel('Count')\n",
    "ax.set_xlabel('Layer type')\n",
    "plt.setp(ax, xticklabels=layer_type_labels)\n",
    "bp_layers.set_xticklabels(labels=layer_type_labels, rotation=20)\n",
    "plt.show()\n",
    "fig.savefig(\"layerTypeBarplotTop.svg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# x(['batch_size', 'epoch_size', 'optimizer', 'units1', 'units2', 'units3',\n",
    "#        'dropout1', 'dropout2', 'dropout3', 'recurrent_dropout1',\n",
    "#        'recurrent_dropout2', 'recurrent_dropout3', 'gaussian_noise_std1',\n",
    "#        'gaussian_noise_std2', 'gaussian_noise_std3', 'batch_normalization1',\n",
    "#        'batch_normalization2', 'batch_normalization3', 'gaussian_noise1',\n",
    "#        'gaussian_noise2', 'gaussian_noise3', 'layer_type1', 'layer_type2',\n",
    "#        'layer_type3', 'layer_initializer1', 'layer_initializer2',\n",
    "#        'layer_initializer3', 'holdoutSmapeAccuracy'],\n",
    "\n",
    "\n",
    "sns.set(style=\"whitegrid\")\n",
    "sns.set(font_scale=2.0)\n",
    "figure_size = (8, 6)\n",
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=figure_size)\n",
    "# layer_type_labels = ['LSTM', 'BiLSTM', 'GRU', 'BiGRU', 'SimpleRNN', 'BiSimpleRNN']\n",
    "#     optimizers = ['nadam', 'amsgrad', 'adagrad', 'adadelta', 'adam',\n",
    "#                   'nadam']  # Avoid loss NaNs, by removing rmsprop, sgd, adamax. TODO: ftrl: needs lr param (for future)\n",
    "layer_type_labels = ['nadam', 'amsgrad', 'adagrad', 'adadelta', 'adam', 'nadam']\n",
    "\n",
    "layer_type_counts = df_models3[\"optimizer\"].round().astype(int).value_counts()\n",
    "bp_layers = sns.barplot(x=[0, 1, 2, 3, 4], y=layer_type_counts)\n",
    "ax.set_title('Optimizer: DNNs with 3 x RNN layers (top {} models)'.format(df_models3.shape[0]))\n",
    "\n",
    "ax.yaxis.grid(True)\n",
    "# ax.set_xticks([y+1 for y in range(len(labels))])\n",
    "ax.set_ylabel('Count')\n",
    "ax.set_xlabel('Optimizer')\n",
    "plt.setp(ax, xticklabels=layer_type_labels)\n",
    "bp_layers.set_xticklabels(labels=layer_type_labels, rotation=20)\n",
    "plt.show()\n",
    "fig.savefig(\"optimizerBarplotTop.svg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_type_counts.unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# x(['batch_size', 'epoch_size', 'optimizer', 'units1', 'units2', 'units3',\n",
    "#        'dropout1', 'dropout2', 'dropout3', 'recurrent_dropout1',\n",
    "#        'recurrent_dropout2', 'recurrent_dropout3', 'gaussian_noise_std1',\n",
    "#        'gaussian_noise_std2', 'gaussian_noise_std3', 'batch_normalization1',\n",
    "#        'batch_normalization2', 'batch_normalization3', 'gaussian_noise1',\n",
    "#        'gaussian_noise2', 'gaussian_noise3', 'layer_type1', 'layer_type2',\n",
    "#        'layer_type3', 'layer_initializer1', 'layer_initializer2',\n",
    "#        'layer_initializer3', 'holdoutSmapeAccuracy'],\n",
    "\n",
    "\n",
    "sns.set(style=\"whitegrid\")\n",
    "sns.set(font_scale=2.0)\n",
    "figure_size = (8, 6)\n",
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=figure_size)\n",
    "# layer_type_labels = ['LSTM', 'BiLSTM', 'GRU', 'BiGRU', 'SimpleRNN', 'BiSimpleRNN']\n",
    "#     optimizers = ['nadam', 'amsgrad', 'adagrad', 'adadelta', 'adam',\n",
    "#                   'nadam']  # Avoid loss NaNs, by removing rmsprop, sgd, adamax. TODO: ftrl: needs lr param (for future)\n",
    "layer_type_labels = ['nadam', 'amsgrad', 'adagrad', 'adadelta', 'adam', 'nadam', 'nadam', 'amsgrad', 'adagrad', 'adadelta', 'adam', 'nadam']\n",
    "\n",
    "layer_type_counts = df_models3[\"layer_initializer1\"].round().astype(int).value_counts()\n",
    "bp_layers = sns.barplot(x=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10], y=layer_type_counts)\n",
    "ax.set_title('Optimizer: DNNs with 3 x RNN layers (all {} models)'.format(df_models3.shape[0]))\n",
    "\n",
    "ax.yaxis.grid(True)\n",
    "# ax.set_xticks([y+1 for y in range(len(labels))])\n",
    "ax.set_ylabel('Count')\n",
    "ax.set_xlabel('Optimizer')\n",
    "plt.setp(ax, xticklabels=layer_type_labels)\n",
    "bp_layers.set_xticklabels(labels=layer_type_labels, rotation=20)\n",
    "plt.show()\n",
    "# fig.savefig(\"optimizerBarplot.svg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BETN073"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import glob\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "run_directory_prefix = \"../models/NarxModelSearch/runs/\"\n",
    "file_name_pattern = \"*Runs.csv\"\n",
    "columns = [\"datetime\", \"iteration\", \"island\", \"cvMseMean\", \"cvMseStd\", \"cvSmapeMean\", \"cvSmapeStd\", \"holdoutRmse\", \"holdoutSmape\", \"holdoutMape\", \"holdoutMse\", \"holdoutIoa\", \"full_parameters\"]\n",
    "\n",
    "experiment_directories = [\n",
    "#     \"18CellularAutomata3DGrid3x3x3_5AgentsO3_1994-2018_background_rural_stations\",\n",
    "    \"18CellularAutomata3DGrid3x3x3_5AgentsO3_1994-2018_1_station_calendar\"\n",
    "#     \"18CellularAutomata3DGrid3x3x3_5AgentsO3_1994-2018_16_stations_calendar\", \n",
    "#                           \"18CellularAutomata3DGrid3x3x3_20AgentsO3_1994-2018_16_stations_calendar\", \n",
    "#                           \"18Islands20AgentsO3_1994-2018_16_stations_calendar\", \n",
    "#                           \"18Islands5AgentsO3_1994-2018_16_stations_calendar\",\n",
    "#                           \"18CellularAutomata3DGrid3x3x3_5AgentsPM10_1994-2018_16_stations_calendar\",\n",
    "#                           \"18Islands20AgentsPM10_1994-2018_16_stations_calendar\",\n",
    "#                           \"18Islands5AgentsPM10_1994-2018_16_stations_calendar\"\n",
    "                         ]\n",
    "\n",
    "run_directories = []\n",
    "for experiment_directory in experiment_directories:\n",
    "    new_run_directory = run_directory_prefix + experiment_directory + \"/\"\n",
    "#     print(\"new_run_directory:\", new_run_directory)\n",
    "    run_directories.append(new_run_directory)\n",
    "\n",
    "worker_directories = [\"local\", \"TX2\"]\n",
    "\n",
    "# print(run_directories)\n",
    "\n",
    "dirs_list = []\n",
    "for run_directory in run_directories:\n",
    "    dirs_list.append(os.listdir(run_directory))\n",
    "#     print(\"dirs:\", os.listdir(run_directory))\n",
    "    \n",
    "for dirs in dirs_list:\n",
    "    for item in dirs:\n",
    "        if item not in worker_directories:\n",
    "            sub_items = os.listdir(run_directories[0] + item)\n",
    "            for sub_item in sub_items:\n",
    "                sub_path = item +\"/\" + sub_item\n",
    "#                 print(\"sub_path:\", item +\"/\" + sub_item)\n",
    "                worker_directories.append(sub_path)\n",
    "            \n",
    "# print(\"worker_directories:\", worker_directories)\n",
    "\n",
    "paths = []\n",
    "for run_directory in run_directories:\n",
    "    for worker_directory in worker_directories:\n",
    "        experiment = \"\"\n",
    "        if run_directory.startswith(run_directory_prefix):\n",
    "            experiment = run_directory[len(run_directory_prefix):-1]\n",
    "#         print(\"experiment:\", experiment)\n",
    "        paths.append((run_directory + worker_directory + \"/logs/\", worker_directory, experiment))\n",
    "\n",
    "# print(len(paths))\n",
    "    \n",
    "frames = []\n",
    "for path in paths:\n",
    "    for csv_file_path in glob.glob(path[0] + file_name_pattern):\n",
    "#         print(\"csv_file_path:\", csv_file_path)\n",
    "#         print(\"csv_file     :\", csv_file)\n",
    "        df = pd.read_csv(csv_file_path, names=columns, engine=\"python\", index_col=\"datetime\", parse_dates=True)\n",
    "    \n",
    "        if \"EC2\" in path[1]:  # EC2 -> + 1 hour (Ireland)\n",
    "            df.index = df.index + 3600\n",
    "        if \"TX2\" in path[1]:  # TX2 -> + 2 hours (UTC)        \n",
    "            df.index = df.index + 3600 * 2\n",
    "    \n",
    "        df.sort_index(inplace=True)\n",
    "        df[\"optimizer\"] = str(re.search('(.{1,6})Runs.csv', os.path.basename(csv_file_path)).group(1))\n",
    "        df[\"worker\"] = path[1]\n",
    "        df[\"experiment\"] = path[2]\n",
    "#         df[\"cvSmapeMean\"] = 100 * df[\"cvSmapeMean\"]\n",
    "#         df[\"holdoutSmape\"] = 100 * df[\"holdoutSmape\"]  \n",
    "#         df[\"holdoutMape\"] = 100 * df[\"holdoutMape\"]        \n",
    "#         df[\"cvSmapeMeanAccuracy\"] = 100 - df[\"cvSmapeMean\"]\n",
    "#         df[\"holdoutSmapeAccuracy\"] = 100 - df[\"holdoutSmape\"]\n",
    "#         print(\"df.shape\", df.shape)\n",
    "        frames.append(df)\n",
    "#         break\n",
    "\n",
    "df = pd.concat(frames)\n",
    "df.sort_index(inplace=True)\n",
    "\n",
    "print(\"df.shape:\", df.shape)\n",
    "df.tail()        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"holdoutMse\"].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"holdoutSmape\"].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"holdoutMape\"].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"holdoutIoa\"].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"holdoutSmape\"].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"cvMseMean\"].min()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
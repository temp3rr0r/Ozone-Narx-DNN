{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import sys\n",
    "import time\n",
    "import numpy as np\n",
    "from math import sqrt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from matplotlib import pyplot\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import gc\n",
    "from sklearn.model_selection import TimeSeriesSplit, train_test_split\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_model(model):\n",
    "    \"\"\"\n",
    "    Clear a tensorflow model from memory & garbage collector.\n",
    "    :param model: Tensorflow model to remove.\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # Memory handling\n",
    "    del model  # Manually delete model\n",
    "    tf.reset_default_graph()\n",
    "    tf.keras.backend.clear_session()\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_model():\n",
    "    return [random.randint(lb[0], ub[0]),  # batch_size\n",
    "             random.randint(lb[1], ub[1]), random.randint(lb[2], ub[2]),  # epoch_size, optimizer\n",
    "             random.randint(lb[3], ub[3]), random.randint(lb[4], ub[4]), random.randint(lb[5], ub[5]),  # units\n",
    "             random.uniform(lb[6], ub[6]), random.uniform(lb[7], ub[7]), random.uniform(lb[8], ub[8]),  # dropout\n",
    "             random.uniform(lb[9], ub[9]), random.uniform(lb[10], ub[10]), random.uniform(lb[11], ub[11]),  # recurrent_dropout\n",
    "             random.uniform(lb[12], ub[12]), random.uniform(lb[13], ub[13]), random.uniform(lb[14], ub[14]),  # gaussian noise std\n",
    "             random.randint(lb[15], ub[15]), random.randint(lb[16], ub[16]), random.randint(lb[17], ub[17]),  # gaussian_noise\n",
    "             random.randint(lb[18], ub[18]), random.randint(lb[19], ub[19]), random.randint(lb[20], ub[20]),  # batch normalization\n",
    "             random.randint(lb[21], ub[21]), random.randint(lb[22], ub[22]), random.randint(lb[23], ub[23]),  # base layer types\n",
    "             random.randint(lb[24], ub[24]), random.randint(lb[25], ub[25]), random.randint(lb[26], ub[26])]  # layer initializers, normal/uniform he/lecun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bounds = [(7, 1 * 31),  # batch_size (~ #days: week, month, year)\n",
    "          (350, 600),  # epoch_size\n",
    "          (0, 4),  # optimizer\n",
    "          (64, 512),  # units\n",
    "          (64, 512),\n",
    "          (64, 512),\n",
    "          (0.01, 0.25),  # dropout\n",
    "          (0.01, 0.25),\n",
    "          (0.01, 0.25),\n",
    "          (0.01, 0.25),  # recurrent_dropout\n",
    "          (0.01, 0.25),\n",
    "          (0.01, 0.25),\n",
    "          (0.1, 0.5),  # gaussian noise std\n",
    "          (0.1, 0.5),\n",
    "          (0.1, 0.5),\n",
    "          (0, 1),  # batch normalization layers\n",
    "          (0, 1),\n",
    "          (0, 1),\n",
    "          (0, 1),  # gaussian noise layer layers\n",
    "          (0, 1),\n",
    "          (0, 1),\n",
    "          (0, 5),  # base layer types (plain/bidirectional: LSTM, GRU, Simple RNN)\n",
    "          (0, 5),\n",
    "          (0, 5),\n",
    "          (0, 9),  # layer initializers, normal/uniform he/lecun,...\n",
    "          (0, 9),\n",
    "          (0, 9)]\n",
    "\n",
    "# Model Search Space bounds\n",
    "lb, ub = zip(*bounds)\n",
    "lb = list(lb)  # Lower Bounds\n",
    "ub = list(ub)  # Upper Bounds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model1(x, *args):\n",
    "#     train_model.counter += 1\n",
    "#     modelLabel = train_model.label\n",
    "#     modelFolds = train_model.folds\n",
    "#     data_manipulation = train_model.data_manipulation\n",
    "#     rank = data_manipulation[\"rank\"]\n",
    "#     master = data_manipulation[\"master\"]\n",
    "#     directory = data_manipulation[\"directory\"]\n",
    "#     filePrefix = data_manipulation[\"filePrefix\"]\n",
    "#     island = data_manipulation[\"island\"]\n",
    "#     verbosity = data_manipulation[\"verbose\"]\n",
    "#     multi_gpu = data_manipulation[\"multi_gpu\"]\n",
    "#     store_plots = data_manipulation[\"storePlots\"]\n",
    "\n",
    "#     x_data, y_data = args\n",
    "\n",
    "    # if island == \"bh\" or island == \"sg\":  # TODO: un-normalize data\n",
    "    #     print(\"bounds \", data_manipulation[\"bounds\"])\n",
    "    #     print(\"x \", x)\n",
    "    #     for i in range(len(x)):\n",
    "    #         x[i] = x[i] * (data_manipulation[\"bounds\"][i][1] - data_manipulation[\"bounds\"][i][0]) \\\n",
    "    #                + data_manipulation[\"bounds\"][i][0]\n",
    "    #     x = np.array(x)\n",
    "    #     print(\"un-normalized x \", x)\n",
    "\n",
    "    # x = [32.269684115953126, 478.4579158867764, 2.4914987273745344, 291.55476719406147, 32.0, 512.0, 0.0812481431483004,\n",
    "    #      0.01, 0.1445004524623349, 0.22335740221774894, 0.03443050512961357, 0.05488258021289669, 1.0,\n",
    "    #      0.620275664519184, 0.34191582396595566, 0.9436131979280933, 0.4991752935129543, 0.4678261851228459, 0.0,\n",
    "    #      0.355287972380982, 0.0]  # TODO: Temp set the same model to benchmark a specific DNN\n",
    "\n",
    "    full_model_parameters = np.array(x.copy())\n",
    "#     if data_manipulation[\"fp16\"]:\n",
    "#         full_model_parameters.astype(np.float32, casting='unsafe')  # TODO: temp test speed of keras with fp16\n",
    "\n",
    "#     print(\"\\n=============\\n\")\n",
    "#     print(\"--- Rank {}: {} iteration {} using: {}\".format(rank, modelLabel, train_model.counter, x[6:15]))\n",
    "\n",
    "    dropout1 = x[6]\n",
    "    dropout2 = x[7]\n",
    "    dropout3 = x[8]\n",
    "    recurrent_dropout1 = x[9]\n",
    "    recurrent_dropout2 = x[10]\n",
    "    recurrent_dropout3 = x[11]\n",
    "\n",
    "    # Gaussian noise\n",
    "    noise_stddev1 = x[12]\n",
    "    noise_stddev2 = x[13]\n",
    "    noise_stddev3 = x[14]\n",
    "\n",
    "    x = np.rint(x).astype(np.int32)\n",
    "    optimizers = ['adadelta', 'adagrad', 'nadam', 'adamax',\n",
    "                  'adam', 'amsgrad']  # , 'rmsprop', 'sgd'] # Avoid loss NaNs, by removing rmsprop & sgd\n",
    "    batch_size = x[0]\n",
    "    epoch_size = x[1]\n",
    "    optimizer = optimizers[x[2]]\n",
    "    units1 = x[3]\n",
    "    units2 = x[4]\n",
    "    units3 = x[5]\n",
    "\n",
    "    # Batch normalization\n",
    "    use_batch_normalization1 = x[15]\n",
    "    use_batch_normalization2 = x[16]\n",
    "    use_batch_normalization3 = x[17]\n",
    "    use_gaussian_noise1 = x[18]\n",
    "    use_gaussian_noise2 = x[19]\n",
    "    use_gaussian_noise3 = x[20]\n",
    "\n",
    "#     print(\"--- Rank {}: batch_size: {}, epoch_size: {} Optimizer: {}, LSTM Unit sizes: {} \"\n",
    "#           \"Batch Normalization/Gaussian Noise: {}\"\n",
    "#           .format(rank, x[0], x[1], optimizers[x[2]], x[3:6], x[15:21]))\n",
    "\n",
    "#     x_data, x_data_holdout = x_data[:-365], x_data[-365:]\n",
    "#     y_data, y_data_holdout = y_data[:-365], y_data[-365:]\n",
    "\n",
    "#     totalFolds = modelFolds\n",
    "#     timeSeriesCrossValidation = TimeSeriesSplit(n_splits=totalFolds)\n",
    "    # timeSeriesCrossValidation = KFold(n_splits=totalFolds)\n",
    "\n",
    "    smape_scores = []\n",
    "    mse_scores = []\n",
    "    train_mse_scores = []\n",
    "    # dev_mse_scores = []\n",
    "    current_fold = 0\n",
    "\n",
    "    # TODO: (Baldwin) phenotypic plasticity, using random uniform.\n",
    "    min_regularizer = 0.0\n",
    "    max_regularizer = 0.01\n",
    "    regularizer_chance = 0.1\n",
    "    regularizer_chance_randoms = np.random.rand(9)\n",
    "#     core_layers_randoms = np.random.randint(4, size=5)  # TODO: Dense, LSTM, BiLSTM, GRU, BiGRU\n",
    "    core_layers_randoms = train_model.z\n",
    "    layer_initializer_genes = train_model.m\n",
    "    layer_initializers = ['he_normal', 'lecun_normal', 'glorot_normal', 'random_normal', 'truncated_normal',\n",
    "                      'he_uniform', 'lecun_uniform', 'random_uniform',\n",
    "                      'zeros', 'ones']\n",
    "\n",
    "    l1_l2_randoms = np.random.uniform(low=min_regularizer, high=max_regularizer, size=(9, 2))\n",
    "    \n",
    "    \n",
    "    model = tf.keras.models.Sequential()\n",
    "    lstm_kwargs = {'units': units1, 'dropout': dropout1, 'recurrent_dropout': recurrent_dropout1,\n",
    "                   'return_sequences': True,\n",
    "                   'implementation': 2,\n",
    "                   # 'kernel_regularizer': l2(0.01),\n",
    "                   # 'activity_regularizer': l2(0.01),\n",
    "                   # 'bias_regularizer': l2(0.01)    # TODO: test with kernel, activity, bias regularizers\n",
    "                   }\n",
    "    # Local mutation\n",
    "    if regularizer_chance_randoms[0] < regularizer_chance:\n",
    "        lstm_kwargs['activity_regularizer'] = tf.keras.regularizers.l1_l2(\n",
    "            l1_l2_randoms[0, 0], l1_l2_randoms[0, 1])\n",
    "    if regularizer_chance_randoms[1] < regularizer_chance:\n",
    "        lstm_kwargs['bias_regularizer'] = tf.keras.regularizers.l1_l2(\n",
    "            l1_l2_randoms[1, 0], l1_l2_randoms[2, 1])\n",
    "    if regularizer_chance_randoms[2] < regularizer_chance:\n",
    "        lstm_kwargs['kernel_regularizer'] = tf.keras.regularizers.l1_l2(\n",
    "            l1_l2_randoms[2, 0], l1_l2_randoms[0, 1])\n",
    "\n",
    "    # 1st base layer\n",
    "    # model.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(**lstm_kwargs), input_shape=(x_data.shape[1], x_data.shape[2])))  # input_shape: rows: n, timestep: 1, features: m\n",
    "    lstm_kwargs['kernel_initializer'] = layer_initializers[layer_initializer_genes[0]]  # TODO: layer initializer\n",
    "    if core_layers_randoms[0] == 0:\n",
    "        model.add(tf.keras.layers.LSTM(**lstm_kwargs))\n",
    "    elif core_layers_randoms[0] == 1:\n",
    "        model.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(**lstm_kwargs)))\n",
    "    elif core_layers_randoms[0] == 2:\n",
    "        model.add(tf.keras.layers.GRU(**lstm_kwargs))\n",
    "    elif core_layers_randoms[0] == 3:\n",
    "        model.add(tf.keras.layers.Bidirectional(tf.keras.layers.GRU(**lstm_kwargs)))\n",
    "    else:\n",
    "#         model.add(tf.keras.layers.Dense(units1))\n",
    "        model.add(tf.keras.layers.Dense(units1,\n",
    "                                        activity_regularizer=tf.keras.regularizers.l1_l2(l1_l2_randoms[3, 0],\n",
    "                                                                                         l1_l2_randoms[3, 1]),\n",
    "                                        bias_regularizer=tf.keras.regularizers.l1_l2(l1_l2_randoms[4, 0],\n",
    "                                                                                     l1_l2_randoms[4, 1]),\n",
    "                                        kernel_regularizer=tf.keras.regularizers.l1_l2(l1_l2_randoms[5, 0],\n",
    "                                                                                       l1_l2_randoms[5, 1])))\n",
    "\n",
    "    # 2nd base layer\n",
    "    if use_gaussian_noise1 < 0.5:\n",
    "        model.add(tf.keras.layers.GaussianNoise(noise_stddev1))\n",
    "    if use_batch_normalization1 < 0.5:\n",
    "        model.add(tf.keras.layers.BatchNormalization())\n",
    "\n",
    "    lstm_kwargs['kernel_initializer'] = layer_initializers[layer_initializer_genes[1]]  # TODO: layer initializer\n",
    "    lstm_kwargs['units'] = units2\n",
    "    lstm_kwargs['dropout'] = dropout2\n",
    "    lstm_kwargs['recurrent_dropout'] = recurrent_dropout2\n",
    "    # TODO: Local mutation\n",
    "    if regularizer_chance_randoms[3] < regularizer_chance:\n",
    "        lstm_kwargs['activity_regularizer'] = tf.keras.regularizers.l1_l2(\n",
    "            l1_l2_randoms[3, 0], l1_l2_randoms[3, 1])\n",
    "    if regularizer_chance_randoms[4] < regularizer_chance:\n",
    "        lstm_kwargs['bias_regularizer'] = tf.keras.regularizers.l1_l2(\n",
    "            l1_l2_randoms[4, 0], l1_l2_randoms[4, 1])\n",
    "    if regularizer_chance_randoms[5] < regularizer_chance:\n",
    "        lstm_kwargs['kernel_regularizer'] = tf.keras.regularizers.l1_l2(\n",
    "            l1_l2_randoms[5, 0], l1_l2_randoms[5, 1])\n",
    "    # 2nd base layer\n",
    "    if core_layers_randoms[1] == 0:\n",
    "        model.add(tf.keras.layers.LSTM(**lstm_kwargs))\n",
    "    elif core_layers_randoms[1] == 1:\n",
    "        model.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(**lstm_kwargs)))\n",
    "    elif core_layers_randoms[1] == 2:\n",
    "        model.add(tf.keras.layers.GRU(**lstm_kwargs))\n",
    "    elif core_layers_randoms[1] == 3:\n",
    "        model.add(tf.keras.layers.Bidirectional(tf.keras.layers.GRU(**lstm_kwargs)))\n",
    "    else:\n",
    "#         model.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(**lstm_kwargs)))\n",
    "        model.add(tf.keras.layers.Dense(units2,\n",
    "                                        activity_regularizer=tf.keras.regularizers.l1_l2(l1_l2_randoms[3, 0],\n",
    "                                                                                         l1_l2_randoms[3, 1]),\n",
    "                                        bias_regularizer=tf.keras.regularizers.l1_l2(l1_l2_randoms[4, 0],\n",
    "                                                                                     l1_l2_randoms[4, 1]),\n",
    "                                        kernel_regularizer=tf.keras.regularizers.l1_l2(l1_l2_randoms[5, 0],\n",
    "                                                                                       l1_l2_randoms[5, 1])))\n",
    "\n",
    "    if use_gaussian_noise2 < 0.5:\n",
    "        model.add(tf.keras.layers.GaussianNoise(noise_stddev2))\n",
    "    if use_batch_normalization2 < 0.5:\n",
    "        model.add(tf.keras.layers.BatchNormalization())\n",
    "\n",
    "    # 3rd base layer\n",
    "    lstm_kwargs['kernel_initializer'] = layer_initializers[layer_initializer_genes[2]]  # TODO: layer initializer\n",
    "    lstm_kwargs['units'] = units3\n",
    "    lstm_kwargs['dropout'] = dropout3\n",
    "    lstm_kwargs['recurrent_dropout'] = recurrent_dropout3\n",
    "    lstm_kwargs['return_sequences'] = False  # Last layer should return sequences\n",
    "    # TODO: Local mutation\n",
    "    if regularizer_chance_randoms[6] < regularizer_chance:\n",
    "        lstm_kwargs['activity_regularizer'] = tf.keras.regularizers.l1_l2(\n",
    "            l1_l2_randoms[6, 0], l1_l2_randoms[6, 1])\n",
    "    if regularizer_chance_randoms[7] < regularizer_chance:\n",
    "        lstm_kwargs['bias_regularizer'] = tf.keras.regularizers.l1_l2(\n",
    "            l1_l2_randoms[7, 0], l1_l2_randoms[7, 1])\n",
    "    if regularizer_chance_randoms[8] < regularizer_chance:\n",
    "        lstm_kwargs['kernel_regularizer'] = tf.keras.regularizers.l1_l2(\n",
    "            l1_l2_randoms[8, 0], l1_l2_randoms[8, 1])\n",
    "#     model.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(**lstm_kwargs)))\n",
    "    \n",
    "#     model.add(tf.keras.layers.Dense(units3))\n",
    "    \n",
    "    if core_layers_randoms[2] == 0:\n",
    "        model.add(tf.keras.layers.LSTM(**lstm_kwargs))\n",
    "    elif core_layers_randoms[2] == 1:\n",
    "        model.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(**lstm_kwargs)))\n",
    "    elif core_layers_randoms[2] == 2:\n",
    "        model.add(tf.keras.layers.GRU(**lstm_kwargs))\n",
    "    elif core_layers_randoms[2] == 3:\n",
    "        model.add(tf.keras.layers.Bidirectional(tf.keras.layers.GRU(**lstm_kwargs)))\n",
    "    else:\n",
    "#         model.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(**lstm_kwargs)))\n",
    "        model.add(tf.keras.layers.Dense(units3,\n",
    "                                        activity_regularizer=tf.keras.regularizers.l1_l2(l1_l2_randoms[3, 0],\n",
    "                                                                                         l1_l2_randoms[3, 1]),\n",
    "                                        bias_regularizer=tf.keras.regularizers.l1_l2(l1_l2_randoms[4, 0],\n",
    "                                                                                     l1_l2_randoms[4, 1]),\n",
    "                                        kernel_regularizer=tf.keras.regularizers.l1_l2(l1_l2_randoms[5, 0],\n",
    "                                                                                       l1_l2_randoms[5, 1])))\n",
    "    if use_gaussian_noise3 < 0.5:\n",
    "        model.add(tf.keras.layers.GaussianNoise(noise_stddev3))\n",
    "    if use_batch_normalization3 < 0.5:\n",
    "        model.add(tf.keras.layers.BatchNormalization())\n",
    "\n",
    "    # model.add(tf.keras.layers.Dense(y_data.shape[1], activation=random.choice(\n",
    "    #     [\"tanh\", \"softmax\", \"elu\", \"selu\", \"softplus\", \"relu\", \"softsign\", \"hard_sigmoid\",\n",
    "    #      \"linear\"])))  # TODO: test with 2 extra dense layers\n",
    "#     model.add(tf.keras.layers.Dense(y_data.shape[1]))  # TODO: shape\n",
    "#     if multi_gpu:\n",
    "#         model = tf.keras.utils.multi_gpu_model(model, gpus=2)\n",
    "\n",
    "    if optimizer == 'amsgrad':  # Adam variant: amsgrad (boolean), \"On the Convergence of Adam and Beyond\".\n",
    "        model.compile(loss='mean_squared_error', optimizer=tf.keras.optimizers.Adam(amsgrad=True))\n",
    "    else:\n",
    "        model.compile(loss='mean_squared_error', optimizer=optimizer)\n",
    "\n",
    "    current_fold += 1  # TODO: train, trainValidation, validation\n",
    "#     print(\"--- Rank {}: Current Fold: {}/{}\".format(rank, current_fold, totalFolds))\n",
    "\n",
    "    early_stop = [\n",
    "        tf.keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=10, verbose=1, mode='auto'),\n",
    "        tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, mode='auto',\n",
    "                                             cooldown=1, verbose=1),\n",
    "        tf.keras.callbacks.TerminateOnNaN()\n",
    "    ]\n",
    "\n",
    "#     try:\n",
    "#         history = model.fit(x_data[train], y_data[train],\n",
    "#                             verbose=verbosity,\n",
    "#                             batch_size=batch_size,\n",
    "#                             epochs=epoch_size,\n",
    "#                             validation_data=(x_data[validation], y_data[validation]),\n",
    "#                             callbacks=early_stop)\n",
    "#     except ValueError:\n",
    "# #         print(\"--- Rank {}: Value Error exception: Model fit exception. Trying again...\".format(rank))\n",
    "#         history = model.fit(x_data[train], y_data[train],\n",
    "#                             verbose=verbosity,\n",
    "#                             batch_size=batch_size,\n",
    "#                             epochs=epoch_size,\n",
    "#                             validation_data=(x_data[validation], y_data[validation]),\n",
    "#                             callbacks=early_stop)\n",
    "#     except:\n",
    "# #         print(\"--- Rank {}: Exception: Returning max float value for this iteration.\".format(rank))\n",
    "#         print(\"--- Exception: Returning max float value for this iteration.\")\n",
    "#         delete_model(model)\n",
    "\n",
    "#         return sys.float_info.max\n",
    "\n",
    "    print('.', end='')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(x, *args):\n",
    "    \"\"\"\n",
    "    Train a deep learning model.\n",
    "    :param x: Model phenotype.\n",
    "    :param args: Data (inputs and expected).\n",
    "    :return: Average validation Mean Squared Error.\n",
    "    \"\"\"\n",
    "\n",
    "    # TODO: Used for rapid distributed island testing\n",
    "    # import random\n",
    "    # rand_number = random.uniform(150, 500)\n",
    "    # if np.isnan(np.array(x)).any() or np.array(x).size == 0:\n",
    "    #     train_model.one_nan = True\n",
    "    # if not train_model.one_nan:\n",
    "    #     print(\"x: \", x)\n",
    "    # else:\n",
    "    #     print(\"\\n================\\nNans detected\\n================\\n\")\n",
    "    # return rand_number\n",
    "\n",
    "#     startTime = time.time()  # training time per model\n",
    "\n",
    "#     train_model.counter += 1\n",
    "#     modelLabel = train_model.label\n",
    "#     modelFolds = train_model.folds\n",
    "#     data_manipulation = train_model.data_manipulation\n",
    "#     rank = data_manipulation[\"rank\"]\n",
    "#     master = data_manipulation[\"master\"]\n",
    "#     directory = data_manipulation[\"directory\"]\n",
    "#     filePrefix = data_manipulation[\"filePrefix\"]\n",
    "#     island = data_manipulation[\"island\"]\n",
    "#     verbosity = data_manipulation[\"verbose\"]\n",
    "#     multi_gpu = data_manipulation[\"multi_gpu\"]\n",
    "#     store_plots = data_manipulation[\"storePlots\"]\n",
    "\n",
    "#     x_data, y_data = args\n",
    "\n",
    "#     print(\"=== TODO: Test network blocks (LSTM only for now) ===\")\n",
    "    # x2 = np.array([31.0, 402.80111162405194, 1.9058202160101727, 487.6506286543307, 124.26215489827942, 512.0, 0.241744517820298,\n",
    "    #  0.25, 0.12677851439487847, 0.23147568997273035, 0.01, 0.19396586046669612, 1.0, 0.6535668275388125,\n",
    "    #  0.16500668136007904, 0.999225537577359, 0.0, 0.20307441174041735, 1.0, 1.0, 0.0, 0.0, 0.5635281795259502,\n",
    "    #  1.4141248802054807, 4.763734792829404, 3.0683379620449647, 5.267796469977627])  # TODO: Temp set the same model to benchmark a specific DNN\n",
    "    # x[12:15] = x2[12:15]  # TODO: Tested: All ~(12:19). With adamax (index: 2) -> Fail. With gaussNoise & batchNorm -> Fail\n",
    "    # TODO: Test: da x3\n",
    "    # x[3:6] = np.array([8, 8, 8])  # TODO: test small for da\n",
    "\n",
    "    full_model_parameters = np.array(x.copy())\n",
    "#     if data_manipulation[\"fp16\"]:\n",
    "#         full_model_parameters.astype(np.float32, casting='unsafe')  # TODO: temp test speed of keras with fp16\n",
    "\n",
    "#     print(\"\\n=============\\n\")\n",
    "#     print(\"--- Rank {}: {} iteration {} using: {}\".format(rank, modelLabel, train_model.counter, x[6:15]))\n",
    "\n",
    "    dropout1 = x[6]\n",
    "    dropout2 = x[7]\n",
    "    dropout3 = x[8]\n",
    "    recurrent_dropout1 = x[9]\n",
    "    recurrent_dropout2 = x[10]\n",
    "    recurrent_dropout3 = x[11]\n",
    "\n",
    "    # Gaussian noise std\n",
    "    noise_stddev1 = x[12]\n",
    "    noise_stddev2 = x[13]\n",
    "    noise_stddev3 = x[14]\n",
    "\n",
    "    x = np.rint(x).astype(np.int32)\n",
    "    optimizers = ['nadam', 'amsgrad', 'adagrad', 'adadelta', 'adam',\n",
    "                  'nadam']  # Avoid loss NaNs, by removing rmsprop, sgd, adamax. TODO: ftrl: needs lr param\n",
    "\n",
    "    batch_size = x[0]\n",
    "    epoch_size = x[1]\n",
    "    optimizer = optimizers[x[2]]\n",
    "    units1 = x[3]\n",
    "    units2 = x[4]\n",
    "    units3 = x[5]\n",
    "\n",
    "    # Use Batch normalization?\n",
    "    use_batch_normalization1 = x[15]\n",
    "    use_batch_normalization2 = x[16]\n",
    "    use_batch_normalization3 = x[17]\n",
    "\n",
    "    # Use gaussian noise?\n",
    "    use_gaussian_noise1 = x[18]\n",
    "    use_gaussian_noise2 = x[19]\n",
    "    use_gaussian_noise3 = x[20]\n",
    "\n",
    "#     core_layers_genes = np.around(x[21:24], decimals=0).astype(int)  # TODO: plain/bidirectional: LSTM, GRU, SimpleRNN\n",
    "    core_layers_genes = train_model.z\n",
    "    layer_initializer_genes = train_model.m\n",
    "\n",
    "    layer_types = ['LSTM', 'BiLSTM', 'GRU', 'BiGRU', 'SimpleRNN', 'BiSimpleRNN']\n",
    "#     print(\"--- Rank {}: Layer Types: {}->{}->{}\"\n",
    "#           .format(rank, layer_types[core_layers_genes[0]], layer_types[core_layers_genes[1]],\n",
    "#                   layer_types[core_layers_genes[2]]))\n",
    "\n",
    "#     print(\"--- Rank {}: batch_size: {}, epoch_size: {} Optimizer: {}, Unit sizes: {} \"\n",
    "#           \"Batch Normalization/Gaussian Noise: {}\"\n",
    "#           .format(rank, x[0], x[1], optimizers[x[2]], x[3:6], x[15:21]))\n",
    "\n",
    "#     layer_initializer_genes = np.around(x[24:27], decimals=0).astype(int)  # layer initializers, normal/uniform he/lecun #  TODO: layer initializers\n",
    "    layer_initializers = ['he_normal', 'lecun_normal', 'glorot_normal', 'random_normal', 'truncated_normal',\n",
    "                          'he_uniform', 'lecun_uniform', 'random_uniform', 'zeros', 'ones']\n",
    "#     print(\"--- Rank {}: Layer initializers: {}->{}->{}\"\n",
    "#           .format(rank, layer_initializers[layer_initializer_genes[0]], layer_initializers[layer_initializer_genes[1]],\n",
    "#                   layer_initializers[layer_initializer_genes[2]]))\n",
    "\n",
    "#     x_data, x_data_holdout = x_data[:-365], x_data[-365:]\n",
    "#     y_data, y_data_holdout = y_data[:-365], y_data[-365:]\n",
    "\n",
    "#     totalFolds = modelFolds\n",
    "#     timeSeriesCrossValidation = TimeSeriesSplit(n_splits=totalFolds)\n",
    "    # timeSeriesCrossValidation = KFold(n_splits=totalFolds)\n",
    "\n",
    "    smape_scores = []\n",
    "    mse_scores = []\n",
    "    train_mse_scores = []\n",
    "    # dev_mse_scores = []\n",
    "    current_fold = 0\n",
    "\n",
    "    # TODO: (Baldwin) phenotypic plasticity, using random uniform.\n",
    "    min_regularizer = 0.0\n",
    "    max_regularizer = 0.01\n",
    "    regularizer_chance = 0.1\n",
    "    regularizer_chance_randoms = np.random.rand(9)\n",
    "    \n",
    "    l1_l2_randoms = np.random.uniform(low=min_regularizer, high=max_regularizer, size=(9, 2))\n",
    "\n",
    "#     for train, validation in timeSeriesCrossValidation.split(x_data, y_data):  # TODO: test train/dev/validation\n",
    "# for train, validation_full in timeSeriesCrossValidation.split(x_data, y_data):  # TODO: Nested CV?\n",
    "\n",
    "#     train, validation = reduce_time_series_validation_fold_size(train, validation)\n",
    "\n",
    "    # dev, validation = train_test_split(validation_full, test_size=0.1, shuffle=False)  # TODO: 50-50 for dev/val\n",
    "\n",
    "    # create model\n",
    "    model = tf.keras.models.Sequential()\n",
    "    lstm_kwargs = {'units': units1, 'dropout': dropout1, 'recurrent_dropout': recurrent_dropout1,\n",
    "                   'return_sequences': True,\n",
    "                   'implementation': 2,\n",
    "                   # 'kernel_regularizer': l2(0.01),\n",
    "                   # 'activity_regularizer': l2(0.01),\n",
    "                   # 'bias_regularizer': l2(0.01)\n",
    "                   }\n",
    "    # Local mutation\n",
    "    if regularizer_chance_randoms[0] < regularizer_chance:\n",
    "        lstm_kwargs['activity_regularizer'] = tf.keras.regularizers.l1_l2(\n",
    "            l1_l2_randoms[0, 0], l1_l2_randoms[0, 1])\n",
    "    if regularizer_chance_randoms[1] < regularizer_chance:\n",
    "        lstm_kwargs['bias_regularizer'] = tf.keras.regularizers.l1_l2(\n",
    "            l1_l2_randoms[1, 0], l1_l2_randoms[2, 1])\n",
    "    if regularizer_chance_randoms[2] < regularizer_chance:\n",
    "        lstm_kwargs['kernel_regularizer'] = tf.keras.regularizers.l1_l2(\n",
    "            l1_l2_randoms[2, 0], l1_l2_randoms[0, 1])\n",
    "\n",
    "    # 1st base layer\n",
    "    lstm_kwargs['kernel_initializer'] = layer_initializers[layer_initializer_genes[0]]  # TODO: layer initializer\n",
    "\n",
    "#     model.add(tf.keras.layers.LSTM(**lstm_kwargs))\n",
    "    \n",
    "    n_timesteps,n_features = 2, 64\n",
    "    #ValueError: Negative dimension size caused by subtracting 3\n",
    "    #from 1 for 'conv1d/conv1d' (op: 'Conv2D') with input shapes: [?,1,1,16], [1,3,16,64].  \n",
    "#     n_timesteps, n_features, n_outputs = trainX.shape[1], trainX.shape[2], trainy.shape[1]\n",
    "    \n",
    "    model.add(tf.keras.layers.Conv1D(\n",
    "        filters=64, kernel_size=3,\n",
    "        activation='relu', input_shape=(n_timesteps,n_features)))\n",
    "#     model.add(tf.keras.layers.Conv1D(filters=64, kernel_size=3, activation='relu'))\n",
    "    model.add(tf.keras.layers.Dropout(0.5))\n",
    "    model.add(tf.keras.layers.MaxPooling1D(pool_size=2))\n",
    "    model.add(tf.keras.layers.Flatten())\n",
    "    \n",
    "    if core_layers_genes[2] == 0:\n",
    "        model.add(tf.keras.layers.LSTM(**lstm_kwargs))\n",
    "    elif core_layers_genes[2] == 1:\n",
    "        model.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(**lstm_kwargs)))\n",
    "    elif core_layers_genes[2] == 2:\n",
    "        model.add(tf.keras.layers.GRU(**lstm_kwargs))\n",
    "    elif core_layers_genes[2] == 3:\n",
    "        model.add(tf.keras.layers.Bidirectional(tf.keras.layers.GRU(**lstm_kwargs)))\n",
    "    elif core_layers_genes[2] == 4:\n",
    "        model.add(tf.keras.layers.SimpleRNN(**lstm_kwargs))\n",
    "    else:\n",
    "        model.add(tf.keras.layers.Bidirectional(tf.keras.layers.SimpleRNN(**lstm_kwargs)))\n",
    "    if use_gaussian_noise1 < 0.5:\n",
    "        model.add(tf.keras.layers.GaussianNoise(noise_stddev1))\n",
    "    if use_batch_normalization1 < 0.5:\n",
    "        model.add(tf.keras.layers.BatchNormalization())\n",
    "\n",
    "    # 2nd base layer\n",
    "    lstm_kwargs['kernel_initializer'] = layer_initializers[layer_initializer_genes[1]]  # TODO: layer initializer\n",
    "    lstm_kwargs['units'] = units2\n",
    "    lstm_kwargs['dropout'] = dropout2\n",
    "    lstm_kwargs['recurrent_dropout'] = recurrent_dropout2\n",
    "    # TODO: Local mutation\n",
    "    if regularizer_chance_randoms[3] < regularizer_chance:\n",
    "        lstm_kwargs['activity_regularizer'] = tf.keras.regularizers.l1_l2(\n",
    "            l1_l2_randoms[3, 0], l1_l2_randoms[3, 1])\n",
    "    if regularizer_chance_randoms[4] < regularizer_chance:\n",
    "        lstm_kwargs['bias_regularizer'] = tf.keras.regularizers.l1_l2(\n",
    "            l1_l2_randoms[4, 0], l1_l2_randoms[4, 1])\n",
    "    if regularizer_chance_randoms[5] < regularizer_chance:\n",
    "        lstm_kwargs['kernel_regularizer'] = tf.keras.regularizers.l1_l2(\n",
    "            l1_l2_randoms[5, 0], l1_l2_randoms[5, 1])\n",
    "    # 2nd base layer\n",
    "    if core_layers_genes[2] == 0:\n",
    "        model.add(tf.keras.layers.LSTM(**lstm_kwargs))\n",
    "    elif core_layers_genes[2] == 1:\n",
    "        model.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(**lstm_kwargs)))\n",
    "    elif core_layers_genes[2] == 2:\n",
    "        model.add(tf.keras.layers.GRU(**lstm_kwargs))\n",
    "    elif core_layers_genes[2] == 3:\n",
    "        model.add(tf.keras.layers.Bidirectional(tf.keras.layers.GRU(**lstm_kwargs)))\n",
    "    elif core_layers_genes[2] == 4:\n",
    "        model.add(tf.keras.layers.SimpleRNN(**lstm_kwargs))\n",
    "    else:\n",
    "        model.add(tf.keras.layers.Bidirectional(tf.keras.layers.SimpleRNN(**lstm_kwargs)))\n",
    "    if use_gaussian_noise2 < 0.5:\n",
    "        model.add(tf.keras.layers.GaussianNoise(noise_stddev2))\n",
    "    if use_batch_normalization2 < 0.5:\n",
    "        model.add(tf.keras.layers.BatchNormalization())\n",
    "\n",
    "    # 3rd base layer\n",
    "    lstm_kwargs['kernel_initializer'] = layer_initializers[layer_initializer_genes[2]]  # TODO: layer initializer\n",
    "    lstm_kwargs['units'] = units3\n",
    "    lstm_kwargs['dropout'] = dropout3\n",
    "    lstm_kwargs['recurrent_dropout'] = recurrent_dropout3\n",
    "    lstm_kwargs['return_sequences'] = False  # Last layer should return sequences\n",
    "    # TODO: Local mutation\n",
    "    if regularizer_chance_randoms[6] < regularizer_chance:\n",
    "        lstm_kwargs['activity_regularizer'] = tf.keras.regularizers.l1_l2(\n",
    "            l1_l2_randoms[6, 0], l1_l2_randoms[6, 1])\n",
    "    if regularizer_chance_randoms[7] < regularizer_chance:\n",
    "        lstm_kwargs['bias_regularizer'] = tf.keras.regularizers.l1_l2(\n",
    "            l1_l2_randoms[7, 0], l1_l2_randoms[7, 1])\n",
    "    if regularizer_chance_randoms[8] < regularizer_chance:\n",
    "        lstm_kwargs['kernel_regularizer'] = tf.keras.regularizers.l1_l2(\n",
    "            l1_l2_randoms[8, 0], l1_l2_randoms[8, 1])\n",
    "    if core_layers_genes[2] == 0:\n",
    "        model.add(tf.keras.layers.LSTM(**lstm_kwargs))\n",
    "    elif core_layers_genes[2] == 1:\n",
    "        model.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(**lstm_kwargs)))\n",
    "    elif core_layers_genes[2] == 2:\n",
    "        model.add(tf.keras.layers.GRU(**lstm_kwargs))\n",
    "    elif core_layers_genes[2] == 3:\n",
    "        model.add(tf.keras.layers.Bidirectional(tf.keras.layers.GRU(**lstm_kwargs)))\n",
    "    elif core_layers_genes[2] == 4:\n",
    "        model.add(tf.keras.layers.SimpleRNN(**lstm_kwargs))\n",
    "    else:\n",
    "        model.add(tf.keras.layers.Bidirectional(tf.keras.layers.SimpleRNN(**lstm_kwargs)))\n",
    "\n",
    "    if use_gaussian_noise3 < 0.5:\n",
    "        model.add(tf.keras.layers.GaussianNoise(noise_stddev3))\n",
    "    if use_batch_normalization3 < 0.5:\n",
    "        model.add(tf.keras.layers.BatchNormalization())\n",
    "\n",
    "    # model.add(tf.keras.layers.Dense(y_data.shape[1], activation=random.choice(\n",
    "    #     [\"tanh\", \"softmax\", \"elu\", \"selu\", \"softplus\", \"relu\", \"softsign\", \"hard_sigmoid\",\n",
    "    #      \"linear\"])))  # TODO: test with 2 extra dense layers\n",
    "    #TODO: test dense\n",
    "    denseCount = 16\n",
    "#     model.add(tf.keras.layers.Dense(y_data.shape[1]))\n",
    "    model.add(tf.keras.layers.Dense(denseCount))\n",
    "#     if multi_gpu:\n",
    "#         model = tf.keras.utils.multi_gpu_model(model, gpus=2)\n",
    "\n",
    "    if optimizer == 'amsgrad':  # Adam variant: amsgrad (boolean), \"On the Convergence of Adam and Beyond\".\n",
    "        model.compile(loss='mean_squared_error', optimizer=tf.keras.optimizers.Adam(amsgrad=True))\n",
    "    else:\n",
    "        model.compile(loss='mean_squared_error', optimizer=optimizer)\n",
    "\n",
    "    current_fold += 1  # TODO: train, trainValidation, validation\n",
    "#     print(\"--- Rank {}: Current Fold: {}/{}\".format(rank, current_fold, totalFolds))\n",
    "\n",
    "    early_stop = [\n",
    "        tf.keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=10, verbose=1, mode='auto'),\n",
    "        tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, mode='auto',\n",
    "                                             cooldown=1, verbose=1),\n",
    "        tf.keras.callbacks.TerminateOnNaN()\n",
    "    ]\n",
    "\n",
    "    # try:  # TODO: Use dev set\n",
    "    #     history = model.fit(x_data[train], y_data[train],\n",
    "    #                         verbose=verbosity,\n",
    "    #                         batch_size=batch_size,\n",
    "    #                         epochs=epoch_size,\n",
    "    #                         validation_data=(x_data[dev], y_data[dev]),\n",
    "    #                         callbacks=early_stop)\n",
    "    # except ValueError:\n",
    "    #     print(\"--- Rank {}: Value Error exception: Model fit exception. Trying again...\".format(rank))\n",
    "    #     history = model.fit(x_data[train], y_data[train],\n",
    "    #                         verbose=verbosity,\n",
    "    #                         batch_size=batch_size,\n",
    "    #                         epochs=epoch_size,\n",
    "    #                         validation_data=(x_data[dev], y_data[dev]),\n",
    "    #                         callbacks=early_stop)\n",
    "#     try:\n",
    "#         history = model.fit(x_data[train], y_data[train],\n",
    "#                             verbose=verbosity,\n",
    "#                             batch_size=batch_size,\n",
    "#                             epochs=epoch_size,\n",
    "#                             validation_data=(x_data[validation], y_data[validation]),\n",
    "#                             callbacks=early_stop)\n",
    "#     except ValueError:\n",
    "#         print(\"--- Rank {}: Value Error exception: Model fit exception. Trying again...\".format(rank))\n",
    "#         history = model.fit(x_data[train], y_data[train],\n",
    "#                             verbose=verbosity,\n",
    "#                             batch_size=batch_size,\n",
    "#                             epochs=epoch_size,\n",
    "#                             validation_data=(x_data[validation], y_data[validation]),\n",
    "#                             callbacks=early_stop)\n",
    "#     except:\n",
    "#         print(\"--- Rank {}: Exception: Returning max float value for this iteration.\".format(rank))\n",
    "#         delete_model(model)\n",
    "\n",
    "#         return sys.float_info.max\n",
    "\n",
    "    print('=', end='')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(x, *args):  \n",
    "    full_model_parameters = np.array(x.copy())\n",
    "    dropout1 = x[6]\n",
    "    dropout2 = x[7]\n",
    "    dropout3 = x[8]\n",
    "    recurrent_dropout1 = x[9]\n",
    "    recurrent_dropout2 = x[10]\n",
    "    recurrent_dropout3 = x[11]\n",
    "    noise_stddev1 = x[12]\n",
    "    noise_stddev2 = x[13]\n",
    "    noise_stddev3 = x[14]\n",
    "    x = np.rint(x).astype(np.int32)\n",
    "    optimizers = ['nadam', 'amsgrad', 'adagrad', 'adadelta', 'adam',\n",
    "                  'nadam']  # Avoid loss NaNs, by removing rmsprop, sgd, adamax. TODO: ftrl: needs lr param\n",
    "    batch_size = x[0]\n",
    "    epoch_size = x[1]\n",
    "    optimizer = optimizers[x[2]]\n",
    "    units1 = x[3]\n",
    "    units2 = x[4]\n",
    "    units3 = x[5]\n",
    "    use_batch_normalization1 = x[15]\n",
    "    use_batch_normalization2 = x[16]\n",
    "    use_batch_normalization3 = x[17]\n",
    "    use_gaussian_noise1 = x[18]\n",
    "    use_gaussian_noise2 = x[19]\n",
    "    use_gaussian_noise3 = x[20]\n",
    "    core_layers_genes = train_model.z\n",
    "    layer_initializer_genes = train_model.m\n",
    "    layer_types = ['LSTM', 'BiLSTM', 'GRU', 'BiGRU', 'SimpleRNN', 'BiSimpleRNN']\n",
    "    layer_initializers = ['he_normal', 'lecun_normal', 'glorot_normal', 'random_normal', 'truncated_normal',\n",
    "                          'he_uniform', 'lecun_uniform', 'random_uniform', 'zeros', 'ones']\n",
    "    smape_scores = []\n",
    "    mse_scores = []\n",
    "    train_mse_scores = []\n",
    "    current_fold = 0\n",
    "    min_regularizer = 0.0\n",
    "    max_regularizer = 0.01\n",
    "    regularizer_chance = 0.1\n",
    "    regularizer_chance_randoms = np.random.rand(9)\n",
    "    l1_l2_randoms = np.random.uniform(low=min_regularizer, high=max_regularizer, size=(9, 2))\n",
    "    model = tf.keras.models.Sequential()\n",
    "    lstm_kwargs = {'units': units1, 'dropout': dropout1, 'recurrent_dropout': recurrent_dropout1,\n",
    "                   'return_sequences': True,\n",
    "                   'implementation': 2,\n",
    "                   }\n",
    "    if regularizer_chance_randoms[0] < regularizer_chance:\n",
    "        lstm_kwargs['activity_regularizer'] = tf.keras.regularizers.l1_l2(\n",
    "            l1_l2_randoms[0, 0], l1_l2_randoms[0, 1])\n",
    "    if regularizer_chance_randoms[1] < regularizer_chance:\n",
    "        lstm_kwargs['bias_regularizer'] = tf.keras.regularizers.l1_l2(\n",
    "            l1_l2_randoms[1, 0], l1_l2_randoms[2, 1])\n",
    "    if regularizer_chance_randoms[2] < regularizer_chance:\n",
    "        lstm_kwargs['kernel_regularizer'] = tf.keras.regularizers.l1_l2(\n",
    "            l1_l2_randoms[2, 0], l1_l2_randoms[0, 1])\n",
    "    lstm_kwargs['kernel_initializer'] = layer_initializers[layer_initializer_genes[0]]  # TODO: layer initializer\n",
    "\n",
    "    n_timesteps,n_features = 2, 64\n",
    "    model.add(tf.keras.layers.Conv1D(\n",
    "        filters=64, kernel_size=3,\n",
    "        activation='relu', input_shape=(n_timesteps,n_features)))\n",
    "#     model.add(tf.keras.layers.Conv1D(filters=64, kernel_size=3, activation='relu'))\n",
    "    model.add(tf.keras.layers.Dropout(0.5))\n",
    "    model.add(tf.keras.layers.MaxPooling1D(pool_size=2))\n",
    "    model.add(tf.keras.layers.Flatten())\n",
    "    \n",
    "    if core_layers_genes[2] == 0:\n",
    "        model.add(tf.keras.layers.LSTM(**lstm_kwargs))\n",
    "    elif core_layers_genes[2] == 1:\n",
    "        model.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(**lstm_kwargs)))\n",
    "    elif core_layers_genes[2] == 2:\n",
    "        model.add(tf.keras.layers.GRU(**lstm_kwargs))\n",
    "    elif core_layers_genes[2] == 3:\n",
    "        model.add(tf.keras.layers.Bidirectional(tf.keras.layers.GRU(**lstm_kwargs)))\n",
    "    elif core_layers_genes[2] == 4:\n",
    "        model.add(tf.keras.layers.SimpleRNN(**lstm_kwargs))\n",
    "    else:\n",
    "        model.add(tf.keras.layers.Bidirectional(tf.keras.layers.SimpleRNN(**lstm_kwargs)))\n",
    "    if use_gaussian_noise1 < 0.5:\n",
    "        model.add(tf.keras.layers.GaussianNoise(noise_stddev1))\n",
    "    if use_batch_normalization1 < 0.5:\n",
    "        model.add(tf.keras.layers.BatchNormalization())\n",
    "\n",
    "    # 2nd base layer\n",
    "    lstm_kwargs['kernel_initializer'] = layer_initializers[layer_initializer_genes[1]]  # TODO: layer initializer\n",
    "    lstm_kwargs['units'] = units2\n",
    "    lstm_kwargs['dropout'] = dropout2\n",
    "    lstm_kwargs['recurrent_dropout'] = recurrent_dropout2\n",
    "    # TODO: Local mutation\n",
    "    if regularizer_chance_randoms[3] < regularizer_chance:\n",
    "        lstm_kwargs['activity_regularizer'] = tf.keras.regularizers.l1_l2(\n",
    "            l1_l2_randoms[3, 0], l1_l2_randoms[3, 1])\n",
    "    if regularizer_chance_randoms[4] < regularizer_chance:\n",
    "        lstm_kwargs['bias_regularizer'] = tf.keras.regularizers.l1_l2(\n",
    "            l1_l2_randoms[4, 0], l1_l2_randoms[4, 1])\n",
    "    if regularizer_chance_randoms[5] < regularizer_chance:\n",
    "        lstm_kwargs['kernel_regularizer'] = tf.keras.regularizers.l1_l2(\n",
    "            l1_l2_randoms[5, 0], l1_l2_randoms[5, 1])\n",
    "    # 2nd base layer\n",
    "    if core_layers_genes[2] == 0:\n",
    "        model.add(tf.keras.layers.LSTM(**lstm_kwargs))\n",
    "    elif core_layers_genes[2] == 1:\n",
    "        model.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(**lstm_kwargs)))\n",
    "    elif core_layers_genes[2] == 2:\n",
    "        model.add(tf.keras.layers.GRU(**lstm_kwargs))\n",
    "    elif core_layers_genes[2] == 3:\n",
    "        model.add(tf.keras.layers.Bidirectional(tf.keras.layers.GRU(**lstm_kwargs)))\n",
    "    elif core_layers_genes[2] == 4:\n",
    "        model.add(tf.keras.layers.SimpleRNN(**lstm_kwargs))\n",
    "    else:\n",
    "        model.add(tf.keras.layers.Bidirectional(tf.keras.layers.SimpleRNN(**lstm_kwargs)))\n",
    "    if use_gaussian_noise2 < 0.5:\n",
    "        model.add(tf.keras.layers.GaussianNoise(noise_stddev2))\n",
    "    if use_batch_normalization2 < 0.5:\n",
    "        model.add(tf.keras.layers.BatchNormalization())\n",
    "\n",
    "    # 3rd base layer\n",
    "    lstm_kwargs['kernel_initializer'] = layer_initializers[layer_initializer_genes[2]]  # TODO: layer initializer\n",
    "    lstm_kwargs['units'] = units3\n",
    "    lstm_kwargs['dropout'] = dropout3\n",
    "    lstm_kwargs['recurrent_dropout'] = recurrent_dropout3\n",
    "    lstm_kwargs['return_sequences'] = False  # Last layer should return sequences\n",
    "    # TODO: Local mutation\n",
    "    if regularizer_chance_randoms[6] < regularizer_chance:\n",
    "        lstm_kwargs['activity_regularizer'] = tf.keras.regularizers.l1_l2(\n",
    "            l1_l2_randoms[6, 0], l1_l2_randoms[6, 1])\n",
    "    if regularizer_chance_randoms[7] < regularizer_chance:\n",
    "        lstm_kwargs['bias_regularizer'] = tf.keras.regularizers.l1_l2(\n",
    "            l1_l2_randoms[7, 0], l1_l2_randoms[7, 1])\n",
    "    if regularizer_chance_randoms[8] < regularizer_chance:\n",
    "        lstm_kwargs['kernel_regularizer'] = tf.keras.regularizers.l1_l2(\n",
    "            l1_l2_randoms[8, 0], l1_l2_randoms[8, 1])\n",
    "    if core_layers_genes[2] == 0:\n",
    "        model.add(tf.keras.layers.LSTM(**lstm_kwargs))\n",
    "    elif core_layers_genes[2] == 1:\n",
    "        model.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(**lstm_kwargs)))\n",
    "    elif core_layers_genes[2] == 2:\n",
    "        model.add(tf.keras.layers.GRU(**lstm_kwargs))\n",
    "    elif core_layers_genes[2] == 3:\n",
    "        model.add(tf.keras.layers.Bidirectional(tf.keras.layers.GRU(**lstm_kwargs)))\n",
    "    elif core_layers_genes[2] == 4:\n",
    "        model.add(tf.keras.layers.SimpleRNN(**lstm_kwargs))\n",
    "    else:\n",
    "        model.add(tf.keras.layers.Bidirectional(tf.keras.layers.SimpleRNN(**lstm_kwargs)))\n",
    "\n",
    "    if use_gaussian_noise3 < 0.5:\n",
    "        model.add(tf.keras.layers.GaussianNoise(noise_stddev3))\n",
    "    if use_batch_normalization3 < 0.5:\n",
    "        model.add(tf.keras.layers.BatchNormalization())\n",
    "\n",
    "    # model.add(tf.keras.layers.Dense(y_data.shape[1], activation=random.choice(\n",
    "    #     [\"tanh\", \"softmax\", \"elu\", \"selu\", \"softplus\", \"relu\", \"softsign\", \"hard_sigmoid\",\n",
    "    #      \"linear\"])))  # TODO: test with 2 extra dense layers\n",
    "    #TODO: test dense\n",
    "    denseCount = 16\n",
    "#     model.add(tf.keras.layers.Dense(y_data.shape[1]))\n",
    "    model.add(tf.keras.layers.Dense(denseCount))\n",
    "#     if multi_gpu:\n",
    "#         model = tf.keras.utils.multi_gpu_model(model, gpus=2)\n",
    "\n",
    "    if optimizer == 'amsgrad':  # Adam variant: amsgrad (boolean), \"On the Convergence of Adam and Beyond\".\n",
    "        model.compile(loss='mean_squared_error', optimizer=tf.keras.optimizers.Adam(amsgrad=True))\n",
    "    else:\n",
    "        model.compile(loss='mean_squared_error', optimizer=optimizer)\n",
    "\n",
    "    current_fold += 1  # TODO: train, trainValidation, validation\n",
    "#     print(\"--- Rank {}: Current Fold: {}/{}\".format(rank, current_fold, totalFolds))\n",
    "\n",
    "    early_stop = [\n",
    "        tf.keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=10, verbose=1, mode='auto'),\n",
    "        tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, mode='auto',\n",
    "                                             cooldown=1, verbose=1),\n",
    "        tf.keras.callbacks.TerminateOnNaN()\n",
    "    ]\n",
    "\n",
    "\n",
    "    print('=', end='')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result2 = None\n",
    "for l in range(1):\n",
    "    for i in range(0, 4):\n",
    "        for j in range(0, 4):\n",
    "            for k in range(0, 4):\n",
    "#                 z = np.array([i, j, k])\n",
    "\n",
    "                x = get_random_model()\n",
    "                args = {}\n",
    "                args[\"modelLabel\"] = \"test\"\n",
    "                train_model.counter = 1\n",
    "                train_model.label = \"test\"\n",
    "                train_model.folds = 1\n",
    "                train_model.data_manipulation = args    \n",
    "#                 train_model.z = np.array([0, 0, 0])\n",
    "                train_model.m = np.array([0, 0, 0])\n",
    "                train_model.z = np.array([i, j, k])\n",
    "\n",
    "#                 result2 = train_model1(x, *args)\n",
    "                result2 = train_model(x, *args)\n",
    "#                 if result2 != 0:\n",
    "#                     print(\"Invalid model\")\n",
    "#                     break\n",
    "print(\"DONE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sequence classification with LSTM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiple Inputs\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import plot_model\n",
    "# from tensorflow.keras.models import Model\n",
    "# from tensorflow.keras.layers import Input, Conv2D, Reshape, Dropout, TimeDistributed\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "# from tensorflow.keras.layers import MaxPooling2D, MaxPooling3D, AveragePooling3D, MaxPooling1D\n",
    "# from tensorflow.keras.layers import BatchNormalization, Bidirectional\n",
    "# from tensorflow.keras.layers import concatenate\n",
    "from tensorflow.keras.layers import LSTM\n",
    "# from tensorflow.keras.layers import ConvLSTM2D\n",
    "# from tensorflow.keras.layers import Conv3D, Conv1D\n",
    "from tensorflow.keras.models import Sequential\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"  # These lines should be called asap, after the os import\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"  # Use CPU only by default\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"  # gtx 970\n",
    "# from keras.models import Sequential\n",
    "# from keras.layers import LSTM, Dense\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 100 samples, validate on 10 samples\n",
      "Epoch 1/5\n",
      "100/100 [==============================] - 7s 71ms/sample - loss: 11.0107 - acc: 0.1000 - val_loss: 10.6301 - val_acc: 0.1000\n",
      "Epoch 2/5\n",
      "100/100 [==============================] - 0s 1ms/sample - loss: 11.0148 - acc: 0.0800 - val_loss: 10.6246 - val_acc: 0.1000\n",
      "Epoch 3/5\n",
      "100/100 [==============================] - 0s 1ms/sample - loss: 11.0268 - acc: 0.0900 - val_loss: 10.6273 - val_acc: 0.1000\n",
      "Epoch 4/5\n",
      "100/100 [==============================] - 0s 1ms/sample - loss: 11.0622 - acc: 0.1200 - val_loss: 10.6712 - val_acc: 0.2000\n",
      "Epoch 5/5\n",
      "100/100 [==============================] - 0s 1ms/sample - loss: 11.1732 - acc: 0.1300 - val_loss: 10.8315 - val_acc: 0.2000\n",
      "[0.24249929547309876, 0.12]\n"
     ]
    }
   ],
   "source": [
    "data_dim = 16\n",
    "timesteps = 8\n",
    "num_classes = 10\n",
    "# expected input data shape: (batch_size, timesteps, data_dim)\n",
    "model = Sequential()\n",
    "model.add(LSTM(16, return_sequences=True))\n",
    "model.add(LSTM(32, return_sequences=True))\n",
    "model.add(LSTM(32))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "\n",
    "x_train = np.random.random((100, timesteps, data_dim))\n",
    "y_train = np.random.random((100, num_classes))\n",
    "x_val = np.random.random((10, timesteps, data_dim))\n",
    "y_val = np.random.random((10, num_classes))\n",
    "\n",
    "model.fit(x_train, y_train, batch_size=64, epochs=5, validation_data=(x_val, y_val))\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM + CONV1d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1000 samples, validate on 100 samples\n",
      "Epoch 1/5\n",
      "1000/1000 [==============================] - 8s 8ms/sample - loss: 0.2438 - acc: 0.0990 - val_loss: 0.2561 - val_acc: 0.1200\n",
      "Epoch 2/5\n",
      "1000/1000 [==============================] - 0s 273us/sample - loss: 0.2437 - acc: 0.1100 - val_loss: 0.2562 - val_acc: 0.1000\n",
      "Epoch 3/5\n",
      "1000/1000 [==============================] - 0s 306us/sample - loss: 0.2437 - acc: 0.1000 - val_loss: 0.2562 - val_acc: 0.0600\n",
      "Epoch 4/5\n",
      "1000/1000 [==============================] - 0s 271us/sample - loss: 0.2437 - acc: 0.1010 - val_loss: 0.2562 - val_acc: 0.0700\n",
      "Epoch 5/5\n",
      "1000/1000 [==============================] - 0s 274us/sample - loss: 0.2436 - acc: 0.0940 - val_loss: 0.2562 - val_acc: 0.0700\n",
      "100/100 [==============================] - 0s 450us/sample - loss: 0.2458 - acc: 0.1500\n",
      "[0.24579177379608155, 0.15]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import MaxPooling1D, Dropout, GlobalAveragePooling1D\n",
    "from tensorflow.keras.layers import Conv1D, Reshape\n",
    "data_dim = 16\n",
    "timesteps = 8 #8\n",
    "num_classes = 10\n",
    "epochs = 5\n",
    "x_train = np.random.random((1000, timesteps, data_dim))\n",
    "y_train = np.random.random((1000, num_classes))\n",
    "x_val = np.random.random((100, timesteps, data_dim))\n",
    "y_val = np.random.random((100, num_classes))\n",
    "x_test = np.random.random((100, timesteps, data_dim))\n",
    "y_test = np.random.random((100, num_classes))\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv1D(64, 3, activation='relu'))))\n",
    "model.add(MaxPooling1D(3))\n",
    "model.add(LSTM(16, return_sequences=True))'))\n",
    "model.add(LSTM(32))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "model.compile(loss='mean_squared_error', optimizer='nadam', metrics=['accuracy'])\n",
    "model.fit(x_train, y_train, verbose=1, batch_size=64, epochs=epochs, validation_data=(x_val, y_val))\n",
    "score = model.evaluate(x_test, y_test, batch_size=16)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1000 samples, validate on 100 samples\n",
      "Epoch 1/5\n",
      "1000/1000 [==============================] - 10s 10ms/sample - loss: 0.2399 - acc: 0.1150 - val_loss: 0.2410 - val_acc: 0.1000\n",
      "Epoch 2/5\n",
      "1000/1000 [==============================] - 0s 250us/sample - loss: 0.2399 - acc: 0.1270 - val_loss: 0.2410 - val_acc: 0.1000\n",
      "Epoch 3/5\n",
      "1000/1000 [==============================] - 0s 239us/sample - loss: 0.2399 - acc: 0.1090 - val_loss: 0.2410 - val_acc: 0.1000\n",
      "Epoch 4/5\n",
      "1000/1000 [==============================] - 0s 249us/sample - loss: 0.2399 - acc: 0.1090 - val_loss: 0.2410 - val_acc: 0.1000\n",
      "Epoch 5/5\n",
      "1000/1000 [==============================] - 0s 249us/sample - loss: 0.2399 - acc: 0.1090 - val_loss: 0.2410 - val_acc: 0.1000\n",
      "100/100 [==============================] - 0s 420us/sample - loss: 0.2394 - acc: 0.0500\n",
      "[0.23940041780471802, 0.05]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import MaxPooling1D, Dropout, GlobalAveragePooling1D\n",
    "from tensorflow.keras.layers import Conv1D, Reshape\n",
    "data_dim = 16\n",
    "timesteps = 8 #8\n",
    "num_classes = 10\n",
    "epochs = 5\n",
    "x_train = np.random.random((1000, timesteps, data_dim))\n",
    "y_train = np.random.random((1000, num_classes))\n",
    "x_val = np.random.random((100, timesteps, data_dim))\n",
    "y_val = np.random.random((100, num_classes))\n",
    "x_test = np.random.random((100, timesteps, data_dim))\n",
    "y_test = np.random.random((100, num_classes))\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv1D(64, 2, activation='relu'))\n",
    "model.add(MaxPooling1D(2))\n",
    "model.add(Conv1D(64, 2, activation='relu'))\n",
    "model.add(MaxPooling1D(2))\n",
    "\n",
    "model.add(LSTM(16, return_sequences=True))\n",
    "\n",
    "# model.add(Conv1D(timesteps, 2, activation='relu'))\n",
    "model.add(LSTM(32))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "model.compile(loss='mean_squared_error', optimizer='nadam', metrics=['accuracy'])\n",
    "model.fit(x_train, y_train, verbose=1, batch_size=64, epochs=epochs, validation_data=(x_val, y_val))\n",
    "score = model.evaluate(x_test, y_test, batch_size=16)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1000 samples, validate on 100 samples\n",
      "Epoch 1/5\n",
      "1000/1000 [==============================] - 11s 11ms/sample - loss: 0.2407 - acc: 0.0890 - val_loss: 0.2385 - val_acc: 0.0700\n",
      "Epoch 2/5\n",
      "1000/1000 [==============================] - 0s 286us/sample - loss: 0.2407 - acc: 0.1060 - val_loss: 0.2385 - val_acc: 0.0800\n",
      "Epoch 3/5\n",
      "1000/1000 [==============================] - 0s 265us/sample - loss: 0.2407 - acc: 0.1100 - val_loss: 0.2385 - val_acc: 0.0900\n",
      "Epoch 4/5\n",
      "1000/1000 [==============================] - 0s 260us/sample - loss: 0.2407 - acc: 0.1070 - val_loss: 0.2385 - val_acc: 0.1000\n",
      "Epoch 5/5\n",
      "1000/1000 [==============================] - 0s 241us/sample - loss: 0.2407 - acc: 0.1100 - val_loss: 0.2385 - val_acc: 0.0700\n",
      "100/100 [==============================] - 0s 400us/sample - loss: 0.2392 - acc: 0.0700\n",
      "[0.23915619552135467, 0.07]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import MaxPooling1D, Dropout, GlobalAveragePooling1D\n",
    "from tensorflow.keras.layers import Conv1D, Reshape\n",
    "data_dim = 16\n",
    "timesteps = 3 #8\n",
    "num_classes = 10\n",
    "epochs = 5\n",
    "x_train = np.random.random((1000, timesteps, data_dim))\n",
    "y_train = np.random.random((1000, num_classes))\n",
    "x_val = np.random.random((100, timesteps, data_dim))\n",
    "y_val = np.random.random((100, num_classes))\n",
    "x_test = np.random.random((100, timesteps, data_dim))\n",
    "y_test = np.random.random((100, num_classes))\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv1D(64, 2, activation='relu'))\n",
    "model.add(Conv1D(64, 2, activation='relu'))\n",
    "model.add(MaxPooling1D(2))\n",
    "\n",
    "model.add(LSTM(16, return_sequences=True))\n",
    "\n",
    "# model.add(Conv1D(timesteps, 2, activation='relu'))\n",
    "model.add(LSTM(32))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "model.compile(loss='mean_squared_error', optimizer='nadam', metrics=['accuracy'])\n",
    "model.fit(x_train, y_train, verbose=1, batch_size=64, epochs=epochs, validation_data=(x_val, y_val))\n",
    "score = model.evaluate(x_test, y_test, batch_size=16)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1000 samples, validate on 100 samples\n",
      "Epoch 1/5\n",
      "1000/1000 [==============================] - 6s 6ms/sample - loss: 0.2447 - acc: 0.0850 - val_loss: 0.2503 - val_acc: 0.1200\n",
      "Epoch 2/5\n",
      "1000/1000 [==============================] - 0s 447us/sample - loss: 0.2447 - acc: 0.0920 - val_loss: 0.2502 - val_acc: 0.0900\n",
      "Epoch 3/5\n",
      "1000/1000 [==============================] - 0s 440us/sample - loss: 0.2447 - acc: 0.1030 - val_loss: 0.2502 - val_acc: 0.0700\n",
      "Epoch 4/5\n",
      "1000/1000 [==============================] - 0s 452us/sample - loss: 0.2446 - acc: 0.0950 - val_loss: 0.2502 - val_acc: 0.0800\n",
      "Epoch 5/5\n",
      "1000/1000 [==============================] - 0s 445us/sample - loss: 0.2446 - acc: 0.1030 - val_loss: 0.2502 - val_acc: 0.0700\n",
      "100/100 [==============================] - 0s 829us/sample - loss: 0.2400 - acc: 0.1100\n",
      "[0.24002591967582704, 0.11]\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(16, return_sequences=True))\n",
    "#                ,\n",
    "#                input_shape=(timesteps, data_dim))\n",
    "#          )  # returns a sequence of vectors of dimension 32\n",
    "model.add(LSTM(32))  # return a single vector of dimension 32\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "model.compile(loss='mse',\n",
    "              optimizer='nadam',\n",
    "              metrics=['accuracy'])\n",
    "model.fit(x_train, y_train, verbose=1,\n",
    "          batch_size=64, epochs=epochs,\n",
    "          validation_data=(x_val, y_val))\n",
    "score = model.evaluate(x_test, y_test, batch_size=16)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing local search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func1(x):\n",
    "    return x[0] **2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "      fun: 2.529512644585829e-17\n",
       " hess_inv: <3x3 LbfgsInvHessProduct with dtype=float64>\n",
       "      jac: array([-5.88521106e-11,  0.00000000e+00,  0.00000000e+00])\n",
       "  message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'\n",
       "     nfev: 16\n",
       "      nit: 2\n",
       "   status: 0\n",
       "  success: True\n",
       "        x: array([-5.02942606e-09,  1.00000000e+00,  2.00000000e+00])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scipy.optimize.minimize(x0 = [4, 2, 5], fun=func1, method=\"L-BFGS-B\", bounds=[(-50, 11), (0, 1), (0, 2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "     fun: 2.447953341786534e-17\n",
       "     jac: array([1.98953592e-08, 0.00000000e+00, 0.00000000e+00])\n",
       " message: 'Converged (|f_n-f_(n-1)| ~= 0)'\n",
       "    nfev: 7\n",
       "     nit: 3\n",
       "  status: 1\n",
       " success: True\n",
       "       x: array([4.9476796e-09, 1.0000000e+00, 2.0000000e+00])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scipy.optimize.minimize(x0 = [4, 2, 5], fun=func1, method=\"TNC\", bounds=[(-50, 11), (0, 1), (0, 2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "     fun: 0.0\n",
       "     jac: array([1.49011612e-08, 0.00000000e+00, 0.00000000e+00])\n",
       " message: 'Optimization terminated successfully.'\n",
       "    nfev: 11\n",
       "     nit: 2\n",
       "    njev: 2\n",
       "  status: 0\n",
       " success: True\n",
       "       x: array([0., 1., 2.])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scipy.optimize.minimize(x0 = [4, 2, 5], fun=func1, method=\"SLSQP\", bounds=[(-50, 11), (0, 1), (0, 2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\temp3rr0r\\Anaconda3\\envs\\tensorflowLast\\lib\\site-packages\\scipy\\optimize\\_hessian_update_strategy.py:187: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  'approximations.', UserWarning)\n",
      "C:\\Users\\temp3rr0r\\Anaconda3\\envs\\tensorflowLast\\lib\\site-packages\\scipy\\optimize\\_hessian_update_strategy.py:187: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  'approximations.', UserWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       " barrier_parameter: 2.048000000000001e-09\n",
       " barrier_tolerance: 2.048000000000001e-09\n",
       "          cg_niter: 59\n",
       "      cg_stop_cond: 4\n",
       "            constr: [array([1.30622094e-10, 5.71539216e-01, 1.09904140e+00])]\n",
       "       constr_nfev: [0]\n",
       "       constr_nhev: [0]\n",
       "       constr_njev: [0]\n",
       "    constr_penalty: 1.0\n",
       "  constr_violation: 0.0\n",
       "    execution_time: 0.18509197235107422\n",
       "               fun: 1.706213144861233e-20\n",
       "              grad: array([1.51624054e-08, 0.00000000e+00, 0.00000000e+00])\n",
       "               jac: [<3x3 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 3 stored elements in Compressed Sparse Row format>]\n",
       "   lagrangian_grad: array([1.51761342e-08, 1.25843779e-10, 1.33893790e-10])\n",
       "           message: '`xtol` termination condition is satisfied.'\n",
       "            method: 'tr_interior_point'\n",
       "              nfev: 308\n",
       "              nhev: 0\n",
       "               nit: 61\n",
       "             niter: 61\n",
       "              njev: 0\n",
       "        optimality: 1.5176134150145295e-08\n",
       "            status: 2\n",
       "           success: True\n",
       "         tr_radius: 1.0000000000000005e-09\n",
       "                 v: [array([1.37287682e-11, 1.25843779e-10, 1.33893790e-10])]\n",
       "                 x: array([1.30622094e-10, 5.71539216e-01, 1.09904140e+00])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scipy.optimize.minimize(x0 = [4, 2, 5], fun=func1, method=\"trust-constr\", bounds=[(-50, 11), (0, 1), (0, 2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_random_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-3d8bfed28c47>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mscipy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mminimize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx0\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0mget_random_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfun\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfunc1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"L-BFGS-B\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbounds\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbounds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'get_random_model' is not defined"
     ]
    }
   ],
   "source": [
    "scipy.optimize.minimize(x0 =get_random_model(), fun=func1, method=\"L-BFGS-B\", bounds=bounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scipy.optimize.minimize(x0 = [4, 2, 5], fun=func1, method=\"TNC\", bounds=[(-50, 11), (0, 1), (0, 2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scipy.optimize.minimize(x0 =get_random_model(), fun=func1, method=\"TNC\", bounds=bounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scipy.optimize.minimize(x0 = [4, 2, 5], fun=func1, method=\"SLSQP\", bounds=[(-50, 11), (0, 1), (0, 2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scipy.optimize.minimize(x0 =get_random_model(), fun=func1, method=\"SLSQP\", bounds=bounds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing weight training stochasticity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0813 23:05:30.996449 17416 deprecation.py:506] From C:\\Users\\temp3rr0r\\Anaconda3\\envs\\tensorflowLast\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "ename": "InternalError",
     "evalue": "2 root error(s) found.\n  (0) Internal: Blas GEMM launch failed : a.shape=(32, 784), b.shape=(784, 512), m=32, n=512, k=784\n\t [[{{node dense/MatMul}}]]\n\t [[metrics/acc/Identity/_61]]\n  (1) Internal: Blas GEMM launch failed : a.shape=(32, 784), b.shape=(784, 512), m=32, n=512, k=784\n\t [[{{node dense/MatMul}}]]\n0 successful operations.\n0 derived errors ignored.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-597b11255308>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m               metrics=['accuracy'])\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflowLast\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    778\u001b[0m           \u001b[0mvalidation_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    779\u001b[0m           \u001b[0mvalidation_freq\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_freq\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 780\u001b[1;33m           steps_name='steps_per_epoch')\n\u001b[0m\u001b[0;32m    781\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    782\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflowLast\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[1;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\u001b[0m\n\u001b[0;32m    361\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    362\u001b[0m         \u001b[1;31m# Get outputs.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 363\u001b[1;33m         \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    364\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    365\u001b[0m           \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflowLast\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   3290\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3291\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[1;32m-> 3292\u001b[1;33m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[0;32m   3293\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3294\u001b[0m     output_structure = nest.pack_sequence_as(\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflowLast\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[0;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1458\u001b[1;33m                                                run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1459\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInternalError\u001b[0m: 2 root error(s) found.\n  (0) Internal: Blas GEMM launch failed : a.shape=(32, 784), b.shape=(784, 512), m=32, n=512, k=784\n\t [[{{node dense/MatMul}}]]\n\t [[metrics/acc/Identity/_61]]\n  (1) Internal: Blas GEMM launch failed : a.shape=(32, 784), b.shape=(784, 512), m=32, n=512, k=784\n\t [[{{node dense/MatMul}}]]\n0 successful operations.\n0 derived errors ignored."
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "mnist = tf.keras.datasets.mnist\n",
    "\n",
    "(x_train, y_train),(x_test, y_test) = mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "  tf.keras.layers.Dense(512, activation=tf.nn.relu),\n",
    "  tf.keras.layers.Dropout(0.2),\n",
    "  tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n",
    "])\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train, epochs=5)\n",
    "model.evaluate(x_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
